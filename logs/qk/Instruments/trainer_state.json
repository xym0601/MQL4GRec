{
  "best_metric": 1.448550820350647,
  "best_model_checkpoint": "./logs/qk/Instruments/checkpoint-15399",
  "epoch": 69.0,
  "eval_steps": 1000,
  "global_step": 18009,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.038314176245210725,
      "grad_norm": 30.88987922668457,
      "learning_rate": 9.578544061302682e-06,
      "loss": 22.5341,
      "step": 10
    },
    {
      "epoch": 0.07662835249042145,
      "grad_norm": 26.224811553955078,
      "learning_rate": 1.9157088122605363e-05,
      "loss": 21.1872,
      "step": 20
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 17.122329711914062,
      "learning_rate": 2.8735632183908045e-05,
      "loss": 19.0807,
      "step": 30
    },
    {
      "epoch": 0.1532567049808429,
      "grad_norm": 10.66740894317627,
      "learning_rate": 3.8314176245210727e-05,
      "loss": 17.6082,
      "step": 40
    },
    {
      "epoch": 0.19157088122605365,
      "grad_norm": 11.392078399658203,
      "learning_rate": 4.789272030651341e-05,
      "loss": 16.07,
      "step": 50
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 7.158722877502441,
      "learning_rate": 5.747126436781609e-05,
      "loss": 14.5761,
      "step": 60
    },
    {
      "epoch": 0.2681992337164751,
      "grad_norm": 6.048799991607666,
      "learning_rate": 6.704980842911877e-05,
      "loss": 13.1837,
      "step": 70
    },
    {
      "epoch": 0.3065134099616858,
      "grad_norm": 5.386917591094971,
      "learning_rate": 7.662835249042145e-05,
      "loss": 11.8457,
      "step": 80
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 3.583099365234375,
      "learning_rate": 8.620689655172414e-05,
      "loss": 10.6869,
      "step": 90
    },
    {
      "epoch": 0.3831417624521073,
      "grad_norm": 2.7615153789520264,
      "learning_rate": 9.578544061302682e-05,
      "loss": 9.6873,
      "step": 100
    },
    {
      "epoch": 0.421455938697318,
      "grad_norm": 1.9077527523040771,
      "learning_rate": 0.0001053639846743295,
      "loss": 9.0036,
      "step": 110
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 1.3991085290908813,
      "learning_rate": 0.00011494252873563218,
      "loss": 8.5827,
      "step": 120
    },
    {
      "epoch": 0.49808429118773945,
      "grad_norm": 1.020061731338501,
      "learning_rate": 0.00012452107279693487,
      "loss": 8.3012,
      "step": 130
    },
    {
      "epoch": 0.5363984674329502,
      "grad_norm": 0.7984686493873596,
      "learning_rate": 0.00013409961685823755,
      "loss": 8.0753,
      "step": 140
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 1.0201852321624756,
      "learning_rate": 0.00014367816091954023,
      "loss": 7.9738,
      "step": 150
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 0.8712598085403442,
      "learning_rate": 0.0001532567049808429,
      "loss": 7.8466,
      "step": 160
    },
    {
      "epoch": 0.6513409961685823,
      "grad_norm": 0.775166928768158,
      "learning_rate": 0.00016283524904214558,
      "loss": 7.7117,
      "step": 170
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 0.9378553628921509,
      "learning_rate": 0.0001724137931034483,
      "loss": 7.6433,
      "step": 180
    },
    {
      "epoch": 0.7279693486590039,
      "grad_norm": 0.9871945381164551,
      "learning_rate": 0.00018199233716475097,
      "loss": 7.5211,
      "step": 190
    },
    {
      "epoch": 0.7662835249042146,
      "grad_norm": 0.8574081659317017,
      "learning_rate": 0.00019157088122605365,
      "loss": 7.4375,
      "step": 200
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 0.7561853528022766,
      "learning_rate": 0.00020114942528735632,
      "loss": 7.3634,
      "step": 210
    },
    {
      "epoch": 0.842911877394636,
      "grad_norm": 0.9123361110687256,
      "learning_rate": 0.000210727969348659,
      "loss": 7.2564,
      "step": 220
    },
    {
      "epoch": 0.8812260536398467,
      "grad_norm": 0.8283889293670654,
      "learning_rate": 0.00022030651340996168,
      "loss": 7.2216,
      "step": 230
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 0.716444194316864,
      "learning_rate": 0.00022988505747126436,
      "loss": 7.1422,
      "step": 240
    },
    {
      "epoch": 0.9578544061302682,
      "grad_norm": 0.8283796310424805,
      "learning_rate": 0.00023946360153256704,
      "loss": 7.0319,
      "step": 250
    },
    {
      "epoch": 0.9961685823754789,
      "grad_norm": 0.9374179244041443,
      "learning_rate": 0.00024904214559386974,
      "loss": 7.0055,
      "step": 260
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.178917646408081,
      "eval_runtime": 3.9771,
      "eval_samples_per_second": 4302.599,
      "eval_steps_per_second": 8.549,
      "step": 261
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 0.8514469861984253,
      "learning_rate": 0.0002586206896551724,
      "loss": 6.9396,
      "step": 270
    },
    {
      "epoch": 1.0727969348659003,
      "grad_norm": 0.9541444778442383,
      "learning_rate": 0.0002681992337164751,
      "loss": 6.8635,
      "step": 280
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.0464967489242554,
      "learning_rate": 0.0002777777777777778,
      "loss": 6.7744,
      "step": 290
    },
    {
      "epoch": 1.1494252873563218,
      "grad_norm": 0.8669950366020203,
      "learning_rate": 0.00028735632183908046,
      "loss": 6.737,
      "step": 300
    },
    {
      "epoch": 1.1877394636015326,
      "grad_norm": 1.1113568544387817,
      "learning_rate": 0.00029693486590038313,
      "loss": 6.6879,
      "step": 310
    },
    {
      "epoch": 1.2260536398467432,
      "grad_norm": 1.0398166179656982,
      "learning_rate": 0.0003065134099616858,
      "loss": 6.6126,
      "step": 320
    },
    {
      "epoch": 1.264367816091954,
      "grad_norm": 1.3377131223678589,
      "learning_rate": 0.0003160919540229885,
      "loss": 6.5252,
      "step": 330
    },
    {
      "epoch": 1.3026819923371646,
      "grad_norm": 0.932213306427002,
      "learning_rate": 0.00032567049808429117,
      "loss": 6.503,
      "step": 340
    },
    {
      "epoch": 1.3409961685823755,
      "grad_norm": 1.1425526142120361,
      "learning_rate": 0.0003352490421455939,
      "loss": 6.4258,
      "step": 350
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 1.0509099960327148,
      "learning_rate": 0.0003448275862068966,
      "loss": 6.3844,
      "step": 360
    },
    {
      "epoch": 1.417624521072797,
      "grad_norm": 1.2039506435394287,
      "learning_rate": 0.00035440613026819926,
      "loss": 6.3289,
      "step": 370
    },
    {
      "epoch": 1.4559386973180077,
      "grad_norm": 1.1064375638961792,
      "learning_rate": 0.00036398467432950194,
      "loss": 6.2944,
      "step": 380
    },
    {
      "epoch": 1.4942528735632183,
      "grad_norm": 1.0198665857315063,
      "learning_rate": 0.0003735632183908046,
      "loss": 6.2617,
      "step": 390
    },
    {
      "epoch": 1.5325670498084292,
      "grad_norm": 1.1268422603607178,
      "learning_rate": 0.0003831417624521073,
      "loss": 6.1965,
      "step": 400
    },
    {
      "epoch": 1.5708812260536398,
      "grad_norm": 1.1571346521377563,
      "learning_rate": 0.00039272030651340997,
      "loss": 6.076,
      "step": 410
    },
    {
      "epoch": 1.6091954022988506,
      "grad_norm": 1.1343713998794556,
      "learning_rate": 0.00040229885057471265,
      "loss": 6.1207,
      "step": 420
    },
    {
      "epoch": 1.6475095785440614,
      "grad_norm": 1.397932767868042,
      "learning_rate": 0.00041187739463601533,
      "loss": 6.018,
      "step": 430
    },
    {
      "epoch": 1.685823754789272,
      "grad_norm": 1.2490894794464111,
      "learning_rate": 0.000421455938697318,
      "loss": 6.0089,
      "step": 440
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 1.06204354763031,
      "learning_rate": 0.0004310344827586207,
      "loss": 5.923,
      "step": 450
    },
    {
      "epoch": 1.7624521072796935,
      "grad_norm": 1.2076648473739624,
      "learning_rate": 0.00044061302681992336,
      "loss": 5.9477,
      "step": 460
    },
    {
      "epoch": 1.8007662835249043,
      "grad_norm": 1.0359759330749512,
      "learning_rate": 0.0004501915708812261,
      "loss": 5.8827,
      "step": 470
    },
    {
      "epoch": 1.839080459770115,
      "grad_norm": 1.0481551885604858,
      "learning_rate": 0.0004597701149425287,
      "loss": 5.8216,
      "step": 480
    },
    {
      "epoch": 1.8773946360153255,
      "grad_norm": 0.950562059879303,
      "learning_rate": 0.00046934865900383145,
      "loss": 5.7784,
      "step": 490
    },
    {
      "epoch": 1.9157088122605364,
      "grad_norm": 1.0427478551864624,
      "learning_rate": 0.0004789272030651341,
      "loss": 5.7589,
      "step": 500
    },
    {
      "epoch": 1.9540229885057472,
      "grad_norm": 1.0115818977355957,
      "learning_rate": 0.0004885057471264368,
      "loss": 5.7036,
      "step": 510
    },
    {
      "epoch": 1.9923371647509578,
      "grad_norm": 1.027329444885254,
      "learning_rate": 0.0004980842911877395,
      "loss": 5.6575,
      "step": 520
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.4926371574401855,
      "eval_runtime": 3.9763,
      "eval_samples_per_second": 4303.518,
      "eval_steps_per_second": 8.551,
      "step": 522
    },
    {
      "epoch": 2.0306513409961684,
      "grad_norm": 1.1853653192520142,
      "learning_rate": 0.0004999999704349697,
      "loss": 5.638,
      "step": 530
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 1.049501895904541,
      "learning_rate": 0.0004999998503270461,
      "loss": 5.5925,
      "step": 540
    },
    {
      "epoch": 2.10727969348659,
      "grad_norm": 1.3412216901779175,
      "learning_rate": 0.000499999637828459,
      "loss": 5.5539,
      "step": 550
    },
    {
      "epoch": 2.1455938697318007,
      "grad_norm": 1.1221755743026733,
      "learning_rate": 0.000499999332939287,
      "loss": 5.4896,
      "step": 560
    },
    {
      "epoch": 2.1839080459770113,
      "grad_norm": 1.1121602058410645,
      "learning_rate": 0.000499998935659643,
      "loss": 5.4466,
      "step": 570
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.0412636995315552,
      "learning_rate": 0.0004999984459896734,
      "loss": 5.4401,
      "step": 580
    },
    {
      "epoch": 2.260536398467433,
      "grad_norm": 0.9592702984809875,
      "learning_rate": 0.0004999978639295597,
      "loss": 5.4308,
      "step": 590
    },
    {
      "epoch": 2.2988505747126435,
      "grad_norm": 1.131767988204956,
      "learning_rate": 0.0004999971894795165,
      "loss": 5.385,
      "step": 600
    },
    {
      "epoch": 2.3371647509578546,
      "grad_norm": 1.0636812448501587,
      "learning_rate": 0.0004999964226397932,
      "loss": 5.3787,
      "step": 610
    },
    {
      "epoch": 2.375478927203065,
      "grad_norm": 0.9739032983779907,
      "learning_rate": 0.0004999955634106734,
      "loss": 5.3422,
      "step": 620
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 1.1357284784317017,
      "learning_rate": 0.0004999946117924746,
      "loss": 5.3537,
      "step": 630
    },
    {
      "epoch": 2.4521072796934864,
      "grad_norm": 1.0881133079528809,
      "learning_rate": 0.0004999935677855482,
      "loss": 5.2844,
      "step": 640
    },
    {
      "epoch": 2.4904214559386975,
      "grad_norm": 1.0036473274230957,
      "learning_rate": 0.0004999924313902802,
      "loss": 5.3027,
      "step": 650
    },
    {
      "epoch": 2.528735632183908,
      "grad_norm": 1.075298547744751,
      "learning_rate": 0.0004999912026070906,
      "loss": 5.2396,
      "step": 660
    },
    {
      "epoch": 2.5670498084291187,
      "grad_norm": 1.0986193418502808,
      "learning_rate": 0.0004999898814364334,
      "loss": 5.2256,
      "step": 670
    },
    {
      "epoch": 2.6053639846743293,
      "grad_norm": 0.9805208444595337,
      "learning_rate": 0.0004999884678787972,
      "loss": 5.1907,
      "step": 680
    },
    {
      "epoch": 2.6436781609195403,
      "grad_norm": 1.235891580581665,
      "learning_rate": 0.0004999869619347039,
      "loss": 5.1861,
      "step": 690
    },
    {
      "epoch": 2.681992337164751,
      "grad_norm": 1.0245851278305054,
      "learning_rate": 0.0004999853636047104,
      "loss": 5.1568,
      "step": 700
    },
    {
      "epoch": 2.7203065134099615,
      "grad_norm": 0.9760609269142151,
      "learning_rate": 0.0004999836728894071,
      "loss": 5.114,
      "step": 710
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 1.1437064409255981,
      "learning_rate": 0.0004999818897894192,
      "loss": 5.1143,
      "step": 720
    },
    {
      "epoch": 2.796934865900383,
      "grad_norm": 0.9444877505302429,
      "learning_rate": 0.0004999800143054053,
      "loss": 5.0534,
      "step": 730
    },
    {
      "epoch": 2.835249042145594,
      "grad_norm": 1.0153377056121826,
      "learning_rate": 0.0004999780464380588,
      "loss": 5.0791,
      "step": 740
    },
    {
      "epoch": 2.873563218390805,
      "grad_norm": 0.9501598477363586,
      "learning_rate": 0.0004999759861881067,
      "loss": 5.0782,
      "step": 750
    },
    {
      "epoch": 2.9118773946360155,
      "grad_norm": 1.1044379472732544,
      "learning_rate": 0.0004999738335563106,
      "loss": 5.0568,
      "step": 760
    },
    {
      "epoch": 2.950191570881226,
      "grad_norm": 1.0665240287780762,
      "learning_rate": 0.0004999715885434659,
      "loss": 4.9733,
      "step": 770
    },
    {
      "epoch": 2.9885057471264367,
      "grad_norm": 1.0126783847808838,
      "learning_rate": 0.0004999692511504023,
      "loss": 5.0137,
      "step": 780
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.1537320613861084,
      "eval_runtime": 4.1626,
      "eval_samples_per_second": 4110.912,
      "eval_steps_per_second": 8.168,
      "step": 783
    },
    {
      "epoch": 3.0268199233716473,
      "grad_norm": 1.00614333152771,
      "learning_rate": 0.0004999668213779837,
      "loss": 4.9895,
      "step": 790
    },
    {
      "epoch": 3.0651340996168583,
      "grad_norm": 0.9600662589073181,
      "learning_rate": 0.0004999642992271078,
      "loss": 4.9107,
      "step": 800
    },
    {
      "epoch": 3.103448275862069,
      "grad_norm": 1.0971075296401978,
      "learning_rate": 0.0004999616846987071,
      "loss": 4.934,
      "step": 810
    },
    {
      "epoch": 3.1417624521072796,
      "grad_norm": 1.0391151905059814,
      "learning_rate": 0.0004999589777937474,
      "loss": 4.8758,
      "step": 820
    },
    {
      "epoch": 3.1800766283524906,
      "grad_norm": 1.0210866928100586,
      "learning_rate": 0.0004999561785132294,
      "loss": 4.8693,
      "step": 830
    },
    {
      "epoch": 3.218390804597701,
      "grad_norm": 1.0829005241394043,
      "learning_rate": 0.0004999532868581875,
      "loss": 4.9009,
      "step": 840
    },
    {
      "epoch": 3.256704980842912,
      "grad_norm": 1.0153053998947144,
      "learning_rate": 0.0004999503028296903,
      "loss": 4.8536,
      "step": 850
    },
    {
      "epoch": 3.2950191570881224,
      "grad_norm": 1.0291601419448853,
      "learning_rate": 0.0004999472264288407,
      "loss": 4.8286,
      "step": 860
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.9675705432891846,
      "learning_rate": 0.0004999440576567755,
      "loss": 4.8353,
      "step": 870
    },
    {
      "epoch": 3.371647509578544,
      "grad_norm": 1.1021126508712769,
      "learning_rate": 0.0004999407965146657,
      "loss": 4.8324,
      "step": 880
    },
    {
      "epoch": 3.4099616858237547,
      "grad_norm": 1.0099339485168457,
      "learning_rate": 0.0004999374430037168,
      "loss": 4.8217,
      "step": 890
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 1.0096772909164429,
      "learning_rate": 0.0004999339971251678,
      "loss": 4.783,
      "step": 900
    },
    {
      "epoch": 3.4865900383141764,
      "grad_norm": 1.1238797903060913,
      "learning_rate": 0.0004999304588802924,
      "loss": 4.7817,
      "step": 910
    },
    {
      "epoch": 3.524904214559387,
      "grad_norm": 0.9670770764350891,
      "learning_rate": 0.000499926828270398,
      "loss": 4.744,
      "step": 920
    },
    {
      "epoch": 3.5632183908045976,
      "grad_norm": 1.0618752241134644,
      "learning_rate": 0.0004999231052968265,
      "loss": 4.7146,
      "step": 930
    },
    {
      "epoch": 3.6015325670498086,
      "grad_norm": 0.9003379940986633,
      "learning_rate": 0.0004999192899609537,
      "loss": 4.7025,
      "step": 940
    },
    {
      "epoch": 3.6398467432950192,
      "grad_norm": 0.9251629114151001,
      "learning_rate": 0.0004999153822641898,
      "loss": 4.6713,
      "step": 950
    },
    {
      "epoch": 3.67816091954023,
      "grad_norm": 0.9088833928108215,
      "learning_rate": 0.0004999113822079785,
      "loss": 4.7187,
      "step": 960
    },
    {
      "epoch": 3.716475095785441,
      "grad_norm": 1.1702476739883423,
      "learning_rate": 0.0004999072897937985,
      "loss": 4.6777,
      "step": 970
    },
    {
      "epoch": 3.7547892720306515,
      "grad_norm": 1.0263993740081787,
      "learning_rate": 0.000499903105023162,
      "loss": 4.6869,
      "step": 980
    },
    {
      "epoch": 3.793103448275862,
      "grad_norm": 0.94952392578125,
      "learning_rate": 0.0004998988278976156,
      "loss": 4.7006,
      "step": 990
    },
    {
      "epoch": 3.8314176245210727,
      "grad_norm": 0.9488111734390259,
      "learning_rate": 0.00049989445841874,
      "loss": 4.7126,
      "step": 1000
    },
    {
      "epoch": 3.8697318007662833,
      "grad_norm": 0.9096628427505493,
      "learning_rate": 0.0004998899965881499,
      "loss": 4.6266,
      "step": 1010
    },
    {
      "epoch": 3.9080459770114944,
      "grad_norm": 0.9763199687004089,
      "learning_rate": 0.0004998854424074943,
      "loss": 4.6322,
      "step": 1020
    },
    {
      "epoch": 3.946360153256705,
      "grad_norm": 1.0964471101760864,
      "learning_rate": 0.0004998807958784561,
      "loss": 4.606,
      "step": 1030
    },
    {
      "epoch": 3.9846743295019156,
      "grad_norm": 0.9502457976341248,
      "learning_rate": 0.0004998760570027528,
      "loss": 4.6587,
      "step": 1040
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.983519434928894,
      "eval_runtime": 3.8188,
      "eval_samples_per_second": 4480.975,
      "eval_steps_per_second": 8.903,
      "step": 1044
    },
    {
      "epoch": 4.022988505747127,
      "grad_norm": 0.9361124634742737,
      "learning_rate": 0.0004998712257821354,
      "loss": 4.5721,
      "step": 1050
    },
    {
      "epoch": 4.061302681992337,
      "grad_norm": 0.9295621514320374,
      "learning_rate": 0.0004998663022183895,
      "loss": 4.559,
      "step": 1060
    },
    {
      "epoch": 4.099616858237548,
      "grad_norm": 0.8899425864219666,
      "learning_rate": 0.0004998612863133347,
      "loss": 4.5606,
      "step": 1070
    },
    {
      "epoch": 4.137931034482759,
      "grad_norm": 0.8657518029212952,
      "learning_rate": 0.0004998561780688246,
      "loss": 4.5485,
      "step": 1080
    },
    {
      "epoch": 4.176245210727969,
      "grad_norm": 0.9733635783195496,
      "learning_rate": 0.0004998509774867469,
      "loss": 4.5555,
      "step": 1090
    },
    {
      "epoch": 4.21455938697318,
      "grad_norm": 0.9403051137924194,
      "learning_rate": 0.0004998456845690238,
      "loss": 4.5154,
      "step": 1100
    },
    {
      "epoch": 4.252873563218391,
      "grad_norm": 0.8961272835731506,
      "learning_rate": 0.0004998402993176113,
      "loss": 4.5308,
      "step": 1110
    },
    {
      "epoch": 4.291187739463601,
      "grad_norm": 0.8912553191184998,
      "learning_rate": 0.0004998348217344995,
      "loss": 4.4959,
      "step": 1120
    },
    {
      "epoch": 4.329501915708812,
      "grad_norm": 1.0585923194885254,
      "learning_rate": 0.0004998292518217126,
      "loss": 4.477,
      "step": 1130
    },
    {
      "epoch": 4.3678160919540225,
      "grad_norm": 0.8648035526275635,
      "learning_rate": 0.0004998235895813094,
      "loss": 4.5155,
      "step": 1140
    },
    {
      "epoch": 4.406130268199234,
      "grad_norm": 0.8524699211120605,
      "learning_rate": 0.0004998178350153821,
      "loss": 4.4759,
      "step": 1150
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.9040095210075378,
      "learning_rate": 0.0004998119881260575,
      "loss": 4.4386,
      "step": 1160
    },
    {
      "epoch": 4.482758620689655,
      "grad_norm": 0.9913222789764404,
      "learning_rate": 0.0004998060489154965,
      "loss": 4.4515,
      "step": 1170
    },
    {
      "epoch": 4.521072796934866,
      "grad_norm": 0.8457707166671753,
      "learning_rate": 0.0004998000173858938,
      "loss": 4.4758,
      "step": 1180
    },
    {
      "epoch": 4.559386973180077,
      "grad_norm": 0.9193850755691528,
      "learning_rate": 0.0004997938935394786,
      "loss": 4.4235,
      "step": 1190
    },
    {
      "epoch": 4.597701149425287,
      "grad_norm": 0.9038034081459045,
      "learning_rate": 0.0004997876773785141,
      "loss": 4.4904,
      "step": 1200
    },
    {
      "epoch": 4.636015325670498,
      "grad_norm": 0.9070680141448975,
      "learning_rate": 0.0004997813689052974,
      "loss": 4.4469,
      "step": 1210
    },
    {
      "epoch": 4.674329501915709,
      "grad_norm": 0.8896865844726562,
      "learning_rate": 0.0004997749681221598,
      "loss": 4.4266,
      "step": 1220
    },
    {
      "epoch": 4.712643678160919,
      "grad_norm": 0.9208410978317261,
      "learning_rate": 0.000499768475031467,
      "loss": 4.4306,
      "step": 1230
    },
    {
      "epoch": 4.75095785440613,
      "grad_norm": 0.8109469413757324,
      "learning_rate": 0.0004997618896356187,
      "loss": 4.4226,
      "step": 1240
    },
    {
      "epoch": 4.789272030651341,
      "grad_norm": 0.9388925433158875,
      "learning_rate": 0.0004997552119370482,
      "loss": 4.412,
      "step": 1250
    },
    {
      "epoch": 4.827586206896552,
      "grad_norm": 0.856442391872406,
      "learning_rate": 0.0004997484419382237,
      "loss": 4.3628,
      "step": 1260
    },
    {
      "epoch": 4.865900383141763,
      "grad_norm": 0.8417755365371704,
      "learning_rate": 0.0004997415796416469,
      "loss": 4.3909,
      "step": 1270
    },
    {
      "epoch": 4.904214559386973,
      "grad_norm": 0.961651086807251,
      "learning_rate": 0.0004997346250498542,
      "loss": 4.4044,
      "step": 1280
    },
    {
      "epoch": 4.942528735632184,
      "grad_norm": 0.8042181730270386,
      "learning_rate": 0.0004997275781654152,
      "loss": 4.3248,
      "step": 1290
    },
    {
      "epoch": 4.980842911877395,
      "grad_norm": 0.8480373024940491,
      "learning_rate": 0.0004997204389909348,
      "loss": 4.357,
      "step": 1300
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.875197410583496,
      "eval_runtime": 3.9288,
      "eval_samples_per_second": 4355.547,
      "eval_steps_per_second": 8.654,
      "step": 1305
    },
    {
      "epoch": 5.019157088122605,
      "grad_norm": 0.8969938158988953,
      "learning_rate": 0.000499713207529051,
      "loss": 4.3858,
      "step": 1310
    },
    {
      "epoch": 5.057471264367816,
      "grad_norm": 0.8591309785842896,
      "learning_rate": 0.0004997058837824362,
      "loss": 4.3522,
      "step": 1320
    },
    {
      "epoch": 5.095785440613027,
      "grad_norm": 0.8585525751113892,
      "learning_rate": 0.0004996984677537972,
      "loss": 4.3033,
      "step": 1330
    },
    {
      "epoch": 5.134099616858237,
      "grad_norm": 0.8059447407722473,
      "learning_rate": 0.0004996909594458746,
      "loss": 4.3344,
      "step": 1340
    },
    {
      "epoch": 5.172413793103448,
      "grad_norm": 0.7970752716064453,
      "learning_rate": 0.0004996833588614433,
      "loss": 4.2755,
      "step": 1350
    },
    {
      "epoch": 5.210727969348659,
      "grad_norm": 0.8756186366081238,
      "learning_rate": 0.000499675666003312,
      "loss": 4.3179,
      "step": 1360
    },
    {
      "epoch": 5.24904214559387,
      "grad_norm": 0.9466236233711243,
      "learning_rate": 0.0004996678808743238,
      "loss": 4.2756,
      "step": 1370
    },
    {
      "epoch": 5.287356321839081,
      "grad_norm": 0.7973470091819763,
      "learning_rate": 0.0004996600034773559,
      "loss": 4.2863,
      "step": 1380
    },
    {
      "epoch": 5.325670498084291,
      "grad_norm": 0.8267777562141418,
      "learning_rate": 0.0004996520338153193,
      "loss": 4.2834,
      "step": 1390
    },
    {
      "epoch": 5.363984674329502,
      "grad_norm": 0.7997210621833801,
      "learning_rate": 0.0004996439718911594,
      "loss": 4.2545,
      "step": 1400
    },
    {
      "epoch": 5.402298850574713,
      "grad_norm": 0.8277457356452942,
      "learning_rate": 0.0004996358177078556,
      "loss": 4.2493,
      "step": 1410
    },
    {
      "epoch": 5.440613026819923,
      "grad_norm": 0.834790825843811,
      "learning_rate": 0.0004996275712684213,
      "loss": 4.2597,
      "step": 1420
    },
    {
      "epoch": 5.478927203065134,
      "grad_norm": 0.831848680973053,
      "learning_rate": 0.0004996192325759041,
      "loss": 4.2783,
      "step": 1430
    },
    {
      "epoch": 5.517241379310345,
      "grad_norm": 0.8734632134437561,
      "learning_rate": 0.0004996108016333858,
      "loss": 4.284,
      "step": 1440
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 0.8464562296867371,
      "learning_rate": 0.000499602278443982,
      "loss": 4.2707,
      "step": 1450
    },
    {
      "epoch": 5.593869731800766,
      "grad_norm": 0.8547065258026123,
      "learning_rate": 0.0004995936630108426,
      "loss": 4.2481,
      "step": 1460
    },
    {
      "epoch": 5.6321839080459775,
      "grad_norm": 0.7858909964561462,
      "learning_rate": 0.0004995849553371517,
      "loss": 4.2155,
      "step": 1470
    },
    {
      "epoch": 5.670498084291188,
      "grad_norm": 0.8566884398460388,
      "learning_rate": 0.000499576155426127,
      "loss": 4.1851,
      "step": 1480
    },
    {
      "epoch": 5.708812260536399,
      "grad_norm": 0.8048356771469116,
      "learning_rate": 0.0004995672632810208,
      "loss": 4.2138,
      "step": 1490
    },
    {
      "epoch": 5.747126436781609,
      "grad_norm": 0.8528866767883301,
      "learning_rate": 0.0004995582789051195,
      "loss": 4.2173,
      "step": 1500
    },
    {
      "epoch": 5.78544061302682,
      "grad_norm": 0.8140727281570435,
      "learning_rate": 0.0004995492023017431,
      "loss": 4.2075,
      "step": 1510
    },
    {
      "epoch": 5.823754789272031,
      "grad_norm": 0.7669476270675659,
      "learning_rate": 0.0004995400334742462,
      "loss": 4.2071,
      "step": 1520
    },
    {
      "epoch": 5.862068965517241,
      "grad_norm": 0.8113999962806702,
      "learning_rate": 0.0004995307724260171,
      "loss": 4.1904,
      "step": 1530
    },
    {
      "epoch": 5.900383141762452,
      "grad_norm": 0.9056473970413208,
      "learning_rate": 0.0004995214191604783,
      "loss": 4.1733,
      "step": 1540
    },
    {
      "epoch": 5.938697318007663,
      "grad_norm": 0.7893140316009521,
      "learning_rate": 0.0004995119736810866,
      "loss": 4.2203,
      "step": 1550
    },
    {
      "epoch": 5.977011494252873,
      "grad_norm": 0.7556474804878235,
      "learning_rate": 0.0004995024359913325,
      "loss": 4.1801,
      "step": 1560
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.8003709316253662,
      "eval_runtime": 3.9815,
      "eval_samples_per_second": 4297.841,
      "eval_steps_per_second": 8.539,
      "step": 1566
    },
    {
      "epoch": 6.015325670498084,
      "grad_norm": 0.7959371209144592,
      "learning_rate": 0.000499492806094741,
      "loss": 4.1367,
      "step": 1570
    },
    {
      "epoch": 6.053639846743295,
      "grad_norm": 0.8078412413597107,
      "learning_rate": 0.0004994830839948707,
      "loss": 4.1035,
      "step": 1580
    },
    {
      "epoch": 6.091954022988506,
      "grad_norm": 0.7857649922370911,
      "learning_rate": 0.0004994732696953149,
      "loss": 4.184,
      "step": 1590
    },
    {
      "epoch": 6.130268199233717,
      "grad_norm": 0.7777785658836365,
      "learning_rate": 0.0004994633631997001,
      "loss": 4.1625,
      "step": 1600
    },
    {
      "epoch": 6.168582375478927,
      "grad_norm": 0.7848942875862122,
      "learning_rate": 0.0004994533645116877,
      "loss": 4.1565,
      "step": 1610
    },
    {
      "epoch": 6.206896551724138,
      "grad_norm": 0.7648436427116394,
      "learning_rate": 0.0004994432736349729,
      "loss": 4.1374,
      "step": 1620
    },
    {
      "epoch": 6.245210727969349,
      "grad_norm": 0.7799566388130188,
      "learning_rate": 0.0004994330905732846,
      "loss": 4.1393,
      "step": 1630
    },
    {
      "epoch": 6.283524904214559,
      "grad_norm": 0.8278527855873108,
      "learning_rate": 0.0004994228153303864,
      "loss": 4.1316,
      "step": 1640
    },
    {
      "epoch": 6.32183908045977,
      "grad_norm": 0.8166772723197937,
      "learning_rate": 0.0004994124479100755,
      "loss": 4.1495,
      "step": 1650
    },
    {
      "epoch": 6.360153256704981,
      "grad_norm": 0.7996745705604553,
      "learning_rate": 0.0004994019883161834,
      "loss": 4.0951,
      "step": 1660
    },
    {
      "epoch": 6.398467432950191,
      "grad_norm": 0.8413161039352417,
      "learning_rate": 0.0004993914365525754,
      "loss": 4.0991,
      "step": 1670
    },
    {
      "epoch": 6.436781609195402,
      "grad_norm": 0.7392215132713318,
      "learning_rate": 0.0004993807926231513,
      "loss": 4.0921,
      "step": 1680
    },
    {
      "epoch": 6.4750957854406135,
      "grad_norm": 0.9255962371826172,
      "learning_rate": 0.0004993700565318444,
      "loss": 4.0721,
      "step": 1690
    },
    {
      "epoch": 6.513409961685824,
      "grad_norm": 0.7810244560241699,
      "learning_rate": 0.0004993592282826226,
      "loss": 4.1192,
      "step": 1700
    },
    {
      "epoch": 6.551724137931035,
      "grad_norm": 0.8077940940856934,
      "learning_rate": 0.0004993483078794875,
      "loss": 4.0568,
      "step": 1710
    },
    {
      "epoch": 6.590038314176245,
      "grad_norm": 0.7667796611785889,
      "learning_rate": 0.0004993372953264748,
      "loss": 4.1023,
      "step": 1720
    },
    {
      "epoch": 6.628352490421456,
      "grad_norm": 0.7624425292015076,
      "learning_rate": 0.0004993261906276546,
      "loss": 4.0669,
      "step": 1730
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.6932268738746643,
      "learning_rate": 0.0004993149937871307,
      "loss": 4.0771,
      "step": 1740
    },
    {
      "epoch": 6.704980842911877,
      "grad_norm": 0.7632744908332825,
      "learning_rate": 0.0004993037048090408,
      "loss": 4.0758,
      "step": 1750
    },
    {
      "epoch": 6.743295019157088,
      "grad_norm": 0.7944172620773315,
      "learning_rate": 0.0004992923236975571,
      "loss": 4.0695,
      "step": 1760
    },
    {
      "epoch": 6.781609195402299,
      "grad_norm": 0.7804440259933472,
      "learning_rate": 0.0004992808504568855,
      "loss": 4.0413,
      "step": 1770
    },
    {
      "epoch": 6.819923371647509,
      "grad_norm": 0.7080180644989014,
      "learning_rate": 0.0004992692850912662,
      "loss": 4.0467,
      "step": 1780
    },
    {
      "epoch": 6.85823754789272,
      "grad_norm": 0.7493563890457153,
      "learning_rate": 0.0004992576276049734,
      "loss": 4.0435,
      "step": 1790
    },
    {
      "epoch": 6.896551724137931,
      "grad_norm": 0.7455304861068726,
      "learning_rate": 0.0004992458780023151,
      "loss": 4.039,
      "step": 1800
    },
    {
      "epoch": 6.934865900383142,
      "grad_norm": 0.7703078389167786,
      "learning_rate": 0.0004992340362876336,
      "loss": 4.0346,
      "step": 1810
    },
    {
      "epoch": 6.973180076628353,
      "grad_norm": 0.7787305116653442,
      "learning_rate": 0.0004992221024653051,
      "loss": 4.0565,
      "step": 1820
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.743353247642517,
      "eval_runtime": 4.0628,
      "eval_samples_per_second": 4211.911,
      "eval_steps_per_second": 8.369,
      "step": 1827
    },
    {
      "epoch": 7.011494252873563,
      "grad_norm": 0.7410679459571838,
      "learning_rate": 0.00049921007653974,
      "loss": 4.0158,
      "step": 1830
    },
    {
      "epoch": 7.049808429118774,
      "grad_norm": 0.7399406433105469,
      "learning_rate": 0.0004991979585153826,
      "loss": 3.9962,
      "step": 1840
    },
    {
      "epoch": 7.088122605363985,
      "grad_norm": 0.7906485199928284,
      "learning_rate": 0.0004991857483967111,
      "loss": 4.0057,
      "step": 1850
    },
    {
      "epoch": 7.126436781609195,
      "grad_norm": 0.6971807479858398,
      "learning_rate": 0.0004991734461882383,
      "loss": 3.9906,
      "step": 1860
    },
    {
      "epoch": 7.164750957854406,
      "grad_norm": 0.7519943714141846,
      "learning_rate": 0.0004991610518945102,
      "loss": 3.9983,
      "step": 1870
    },
    {
      "epoch": 7.203065134099617,
      "grad_norm": 0.7601094245910645,
      "learning_rate": 0.0004991485655201076,
      "loss": 3.984,
      "step": 1880
    },
    {
      "epoch": 7.241379310344827,
      "grad_norm": 0.7854604125022888,
      "learning_rate": 0.0004991359870696448,
      "loss": 3.9655,
      "step": 1890
    },
    {
      "epoch": 7.2796934865900385,
      "grad_norm": 0.7095401883125305,
      "learning_rate": 0.0004991233165477704,
      "loss": 4.0159,
      "step": 1900
    },
    {
      "epoch": 7.3180076628352495,
      "grad_norm": 0.7054269313812256,
      "learning_rate": 0.0004991105539591669,
      "loss": 4.0143,
      "step": 1910
    },
    {
      "epoch": 7.35632183908046,
      "grad_norm": 0.8055443167686462,
      "learning_rate": 0.0004990976993085511,
      "loss": 3.9934,
      "step": 1920
    },
    {
      "epoch": 7.394636015325671,
      "grad_norm": 0.7779070734977722,
      "learning_rate": 0.0004990847526006733,
      "loss": 3.9617,
      "step": 1930
    },
    {
      "epoch": 7.432950191570881,
      "grad_norm": 0.788658857345581,
      "learning_rate": 0.0004990717138403183,
      "loss": 3.981,
      "step": 1940
    },
    {
      "epoch": 7.471264367816092,
      "grad_norm": 0.6968464255332947,
      "learning_rate": 0.0004990585830323047,
      "loss": 3.9913,
      "step": 1950
    },
    {
      "epoch": 7.509578544061303,
      "grad_norm": 0.7055931687355042,
      "learning_rate": 0.0004990453601814852,
      "loss": 4.0106,
      "step": 1960
    },
    {
      "epoch": 7.547892720306513,
      "grad_norm": 0.7659137845039368,
      "learning_rate": 0.0004990320452927463,
      "loss": 3.9685,
      "step": 1970
    },
    {
      "epoch": 7.586206896551724,
      "grad_norm": 0.6982773542404175,
      "learning_rate": 0.0004990186383710089,
      "loss": 3.9674,
      "step": 1980
    },
    {
      "epoch": 7.624521072796935,
      "grad_norm": 0.7306683659553528,
      "learning_rate": 0.0004990051394212276,
      "loss": 3.9231,
      "step": 1990
    },
    {
      "epoch": 7.662835249042145,
      "grad_norm": 0.7333664298057556,
      "learning_rate": 0.0004989915484483911,
      "loss": 3.9424,
      "step": 2000
    },
    {
      "epoch": 7.7011494252873565,
      "grad_norm": 0.7344537973403931,
      "learning_rate": 0.0004989778654575222,
      "loss": 3.9383,
      "step": 2010
    },
    {
      "epoch": 7.739463601532567,
      "grad_norm": 0.7676939368247986,
      "learning_rate": 0.0004989640904536776,
      "loss": 3.9589,
      "step": 2020
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 0.7169104814529419,
      "learning_rate": 0.000498950223441948,
      "loss": 3.97,
      "step": 2030
    },
    {
      "epoch": 7.816091954022989,
      "grad_norm": 0.6932138800621033,
      "learning_rate": 0.0004989362644274581,
      "loss": 3.9393,
      "step": 2040
    },
    {
      "epoch": 7.854406130268199,
      "grad_norm": 0.7830069661140442,
      "learning_rate": 0.0004989222134153666,
      "loss": 3.9415,
      "step": 2050
    },
    {
      "epoch": 7.89272030651341,
      "grad_norm": 0.7187108397483826,
      "learning_rate": 0.0004989080704108664,
      "loss": 3.9627,
      "step": 2060
    },
    {
      "epoch": 7.931034482758621,
      "grad_norm": 0.6865217089653015,
      "learning_rate": 0.0004988938354191842,
      "loss": 3.9267,
      "step": 2070
    },
    {
      "epoch": 7.969348659003831,
      "grad_norm": 0.7760374546051025,
      "learning_rate": 0.0004988795084455805,
      "loss": 3.9272,
      "step": 2080
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6982272863388062,
      "eval_runtime": 4.2729,
      "eval_samples_per_second": 4004.79,
      "eval_steps_per_second": 7.957,
      "step": 2088
    },
    {
      "epoch": 8.007662835249041,
      "grad_norm": 0.7384939789772034,
      "learning_rate": 0.0004988650894953503,
      "loss": 3.9186,
      "step": 2090
    },
    {
      "epoch": 8.045977011494253,
      "grad_norm": 0.7795880436897278,
      "learning_rate": 0.0004988505785738221,
      "loss": 3.8925,
      "step": 2100
    },
    {
      "epoch": 8.084291187739463,
      "grad_norm": 0.695905864238739,
      "learning_rate": 0.0004988359756863587,
      "loss": 3.8962,
      "step": 2110
    },
    {
      "epoch": 8.122605363984674,
      "grad_norm": 0.7470498085021973,
      "learning_rate": 0.0004988212808383568,
      "loss": 3.8603,
      "step": 2120
    },
    {
      "epoch": 8.160919540229886,
      "grad_norm": 0.7380682229995728,
      "learning_rate": 0.000498806494035247,
      "loss": 3.9161,
      "step": 2130
    },
    {
      "epoch": 8.199233716475096,
      "grad_norm": 0.7676171660423279,
      "learning_rate": 0.0004987916152824941,
      "loss": 3.9092,
      "step": 2140
    },
    {
      "epoch": 8.237547892720306,
      "grad_norm": 0.6959284543991089,
      "learning_rate": 0.0004987766445855964,
      "loss": 3.8971,
      "step": 2150
    },
    {
      "epoch": 8.275862068965518,
      "grad_norm": 0.7141432762145996,
      "learning_rate": 0.000498761581950087,
      "loss": 3.8782,
      "step": 2160
    },
    {
      "epoch": 8.314176245210728,
      "grad_norm": 0.7141118049621582,
      "learning_rate": 0.0004987464273815321,
      "loss": 3.8738,
      "step": 2170
    },
    {
      "epoch": 8.352490421455938,
      "grad_norm": 0.6916083693504333,
      "learning_rate": 0.0004987311808855323,
      "loss": 3.8845,
      "step": 2180
    },
    {
      "epoch": 8.39080459770115,
      "grad_norm": 0.7570017576217651,
      "learning_rate": 0.0004987158424677224,
      "loss": 3.8719,
      "step": 2190
    },
    {
      "epoch": 8.42911877394636,
      "grad_norm": 0.704089879989624,
      "learning_rate": 0.0004987004121337707,
      "loss": 3.9053,
      "step": 2200
    },
    {
      "epoch": 8.46743295019157,
      "grad_norm": 0.7462996244430542,
      "learning_rate": 0.0004986848898893797,
      "loss": 3.8723,
      "step": 2210
    },
    {
      "epoch": 8.505747126436782,
      "grad_norm": 0.6822379231452942,
      "learning_rate": 0.0004986692757402858,
      "loss": 3.8765,
      "step": 2220
    },
    {
      "epoch": 8.544061302681992,
      "grad_norm": 0.698583722114563,
      "learning_rate": 0.0004986535696922597,
      "loss": 3.8949,
      "step": 2230
    },
    {
      "epoch": 8.582375478927203,
      "grad_norm": 0.7130672931671143,
      "learning_rate": 0.0004986377717511053,
      "loss": 3.8723,
      "step": 2240
    },
    {
      "epoch": 8.620689655172415,
      "grad_norm": 0.6823482513427734,
      "learning_rate": 0.0004986218819226613,
      "loss": 3.8577,
      "step": 2250
    },
    {
      "epoch": 8.659003831417625,
      "grad_norm": 0.6568982005119324,
      "learning_rate": 0.0004986059002128,
      "loss": 3.8456,
      "step": 2260
    },
    {
      "epoch": 8.697318007662835,
      "grad_norm": 0.6923357248306274,
      "learning_rate": 0.0004985898266274273,
      "loss": 3.8452,
      "step": 2270
    },
    {
      "epoch": 8.735632183908045,
      "grad_norm": 0.722771942615509,
      "learning_rate": 0.0004985736611724837,
      "loss": 3.8751,
      "step": 2280
    },
    {
      "epoch": 8.773946360153257,
      "grad_norm": 0.7146545052528381,
      "learning_rate": 0.0004985574038539433,
      "loss": 3.8501,
      "step": 2290
    },
    {
      "epoch": 8.812260536398467,
      "grad_norm": 0.6744874715805054,
      "learning_rate": 0.0004985410546778142,
      "loss": 3.8322,
      "step": 2300
    },
    {
      "epoch": 8.850574712643677,
      "grad_norm": 0.685998260974884,
      "learning_rate": 0.0004985246136501384,
      "loss": 3.8309,
      "step": 2310
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.6939442157745361,
      "learning_rate": 0.0004985080807769918,
      "loss": 3.8247,
      "step": 2320
    },
    {
      "epoch": 8.9272030651341,
      "grad_norm": 0.7450827360153198,
      "learning_rate": 0.0004984914560644846,
      "loss": 3.8557,
      "step": 2330
    },
    {
      "epoch": 8.96551724137931,
      "grad_norm": 0.6923622488975525,
      "learning_rate": 0.0004984747395187604,
      "loss": 3.812,
      "step": 2340
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.6655819416046143,
      "eval_runtime": 4.1125,
      "eval_samples_per_second": 4160.982,
      "eval_steps_per_second": 8.267,
      "step": 2349
    },
    {
      "epoch": 9.003831417624522,
      "grad_norm": 0.6690950989723206,
      "learning_rate": 0.0004984579311459972,
      "loss": 3.817,
      "step": 2350
    },
    {
      "epoch": 9.042145593869732,
      "grad_norm": 0.6563674211502075,
      "learning_rate": 0.0004984410309524067,
      "loss": 3.8205,
      "step": 2360
    },
    {
      "epoch": 9.080459770114942,
      "grad_norm": 0.6803968548774719,
      "learning_rate": 0.0004984240389442346,
      "loss": 3.8271,
      "step": 2370
    },
    {
      "epoch": 9.118773946360154,
      "grad_norm": 0.6811447143554688,
      "learning_rate": 0.0004984069551277605,
      "loss": 3.7958,
      "step": 2380
    },
    {
      "epoch": 9.157088122605364,
      "grad_norm": 0.7081525325775146,
      "learning_rate": 0.0004983897795092979,
      "loss": 3.7925,
      "step": 2390
    },
    {
      "epoch": 9.195402298850574,
      "grad_norm": 0.654763400554657,
      "learning_rate": 0.0004983725120951944,
      "loss": 3.7963,
      "step": 2400
    },
    {
      "epoch": 9.233716475095786,
      "grad_norm": 0.7055858373641968,
      "learning_rate": 0.0004983551528918312,
      "loss": 3.7926,
      "step": 2410
    },
    {
      "epoch": 9.272030651340996,
      "grad_norm": 0.7234364151954651,
      "learning_rate": 0.0004983377019056237,
      "loss": 3.8123,
      "step": 2420
    },
    {
      "epoch": 9.310344827586206,
      "grad_norm": 0.6975898742675781,
      "learning_rate": 0.0004983201591430214,
      "loss": 3.8146,
      "step": 2430
    },
    {
      "epoch": 9.348659003831418,
      "grad_norm": 0.6553497910499573,
      "learning_rate": 0.0004983025246105069,
      "loss": 3.8205,
      "step": 2440
    },
    {
      "epoch": 9.386973180076629,
      "grad_norm": 0.6896116733551025,
      "learning_rate": 0.0004982847983145977,
      "loss": 3.8179,
      "step": 2450
    },
    {
      "epoch": 9.425287356321839,
      "grad_norm": 0.6674038767814636,
      "learning_rate": 0.0004982669802618447,
      "loss": 3.7581,
      "step": 2460
    },
    {
      "epoch": 9.46360153256705,
      "grad_norm": 0.6921708583831787,
      "learning_rate": 0.0004982490704588328,
      "loss": 3.8122,
      "step": 2470
    },
    {
      "epoch": 9.50191570881226,
      "grad_norm": 0.6761804223060608,
      "learning_rate": 0.0004982310689121807,
      "loss": 3.7687,
      "step": 2480
    },
    {
      "epoch": 9.540229885057471,
      "grad_norm": 0.6646056771278381,
      "learning_rate": 0.000498212975628541,
      "loss": 3.7938,
      "step": 2490
    },
    {
      "epoch": 9.578544061302683,
      "grad_norm": 0.654965341091156,
      "learning_rate": 0.0004981947906146006,
      "loss": 3.7659,
      "step": 2500
    },
    {
      "epoch": 9.616858237547893,
      "grad_norm": 0.6705137491226196,
      "learning_rate": 0.0004981765138770798,
      "loss": 3.7666,
      "step": 2510
    },
    {
      "epoch": 9.655172413793103,
      "grad_norm": 0.6608172059059143,
      "learning_rate": 0.0004981581454227332,
      "loss": 3.7821,
      "step": 2520
    },
    {
      "epoch": 9.693486590038313,
      "grad_norm": 0.6657755970954895,
      "learning_rate": 0.0004981396852583489,
      "loss": 3.7679,
      "step": 2530
    },
    {
      "epoch": 9.731800766283525,
      "grad_norm": 0.6787702441215515,
      "learning_rate": 0.0004981211333907491,
      "loss": 3.8076,
      "step": 2540
    },
    {
      "epoch": 9.770114942528735,
      "grad_norm": 0.6671040654182434,
      "learning_rate": 0.00049810248982679,
      "loss": 3.7537,
      "step": 2550
    },
    {
      "epoch": 9.808429118773946,
      "grad_norm": 0.631904661655426,
      "learning_rate": 0.0004980837545733614,
      "loss": 3.7652,
      "step": 2560
    },
    {
      "epoch": 9.846743295019158,
      "grad_norm": 0.6959129571914673,
      "learning_rate": 0.0004980649276373875,
      "loss": 3.7683,
      "step": 2570
    },
    {
      "epoch": 9.885057471264368,
      "grad_norm": 0.6428097486495972,
      "learning_rate": 0.0004980460090258256,
      "loss": 3.749,
      "step": 2580
    },
    {
      "epoch": 9.923371647509578,
      "grad_norm": 0.6492882370948792,
      "learning_rate": 0.0004980269987456676,
      "loss": 3.7864,
      "step": 2590
    },
    {
      "epoch": 9.96168582375479,
      "grad_norm": 0.6356145739555359,
      "learning_rate": 0.0004980078968039389,
      "loss": 3.7068,
      "step": 2600
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.6479933857917786,
      "learning_rate": 0.0004979887032076989,
      "loss": 3.7452,
      "step": 2610
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.6372003555297852,
      "eval_runtime": 4.063,
      "eval_samples_per_second": 4211.703,
      "eval_steps_per_second": 8.368,
      "step": 2610
    },
    {
      "epoch": 10.03831417624521,
      "grad_norm": 0.6662806868553162,
      "learning_rate": 0.0004979694179640407,
      "loss": 3.7134,
      "step": 2620
    },
    {
      "epoch": 10.076628352490422,
      "grad_norm": 0.6315167546272278,
      "learning_rate": 0.0004979500410800916,
      "loss": 3.6998,
      "step": 2630
    },
    {
      "epoch": 10.114942528735632,
      "grad_norm": 0.6474336981773376,
      "learning_rate": 0.0004979305725630126,
      "loss": 3.7587,
      "step": 2640
    },
    {
      "epoch": 10.153256704980842,
      "grad_norm": 0.6956920027732849,
      "learning_rate": 0.0004979110124199984,
      "loss": 3.7555,
      "step": 2650
    },
    {
      "epoch": 10.191570881226054,
      "grad_norm": 0.670793354511261,
      "learning_rate": 0.0004978913606582776,
      "loss": 3.7249,
      "step": 2660
    },
    {
      "epoch": 10.229885057471265,
      "grad_norm": 0.7228084802627563,
      "learning_rate": 0.0004978716172851131,
      "loss": 3.7354,
      "step": 2670
    },
    {
      "epoch": 10.268199233716475,
      "grad_norm": 0.6676074862480164,
      "learning_rate": 0.0004978517823078011,
      "loss": 3.7183,
      "step": 2680
    },
    {
      "epoch": 10.306513409961687,
      "grad_norm": 0.6998555064201355,
      "learning_rate": 0.0004978318557336719,
      "loss": 3.7119,
      "step": 2690
    },
    {
      "epoch": 10.344827586206897,
      "grad_norm": 0.636440098285675,
      "learning_rate": 0.0004978118375700895,
      "loss": 3.6954,
      "step": 2700
    },
    {
      "epoch": 10.383141762452107,
      "grad_norm": 0.6611690521240234,
      "learning_rate": 0.0004977917278244521,
      "loss": 3.7006,
      "step": 2710
    },
    {
      "epoch": 10.421455938697317,
      "grad_norm": 0.6470128893852234,
      "learning_rate": 0.0004977715265041914,
      "loss": 3.7391,
      "step": 2720
    },
    {
      "epoch": 10.459770114942529,
      "grad_norm": 0.6391475796699524,
      "learning_rate": 0.0004977512336167731,
      "loss": 3.7342,
      "step": 2730
    },
    {
      "epoch": 10.49808429118774,
      "grad_norm": 0.6594648361206055,
      "learning_rate": 0.0004977308491696966,
      "loss": 3.7039,
      "step": 2740
    },
    {
      "epoch": 10.53639846743295,
      "grad_norm": 0.6597473621368408,
      "learning_rate": 0.0004977103731704954,
      "loss": 3.7243,
      "step": 2750
    },
    {
      "epoch": 10.574712643678161,
      "grad_norm": 0.6507874727249146,
      "learning_rate": 0.0004976898056267365,
      "loss": 3.7094,
      "step": 2760
    },
    {
      "epoch": 10.613026819923371,
      "grad_norm": 0.6481805443763733,
      "learning_rate": 0.0004976691465460209,
      "loss": 3.7217,
      "step": 2770
    },
    {
      "epoch": 10.651340996168582,
      "grad_norm": 0.6640089154243469,
      "learning_rate": 0.0004976483959359835,
      "loss": 3.6772,
      "step": 2780
    },
    {
      "epoch": 10.689655172413794,
      "grad_norm": 0.6376044154167175,
      "learning_rate": 0.0004976275538042932,
      "loss": 3.6711,
      "step": 2790
    },
    {
      "epoch": 10.727969348659004,
      "grad_norm": 0.6599556803703308,
      "learning_rate": 0.000497606620158652,
      "loss": 3.7064,
      "step": 2800
    },
    {
      "epoch": 10.766283524904214,
      "grad_norm": 0.638835072517395,
      "learning_rate": 0.0004975855950067965,
      "loss": 3.7043,
      "step": 2810
    },
    {
      "epoch": 10.804597701149426,
      "grad_norm": 0.629170835018158,
      "learning_rate": 0.0004975644783564967,
      "loss": 3.6848,
      "step": 2820
    },
    {
      "epoch": 10.842911877394636,
      "grad_norm": 0.6389849781990051,
      "learning_rate": 0.0004975432702155566,
      "loss": 3.718,
      "step": 2830
    },
    {
      "epoch": 10.881226053639846,
      "grad_norm": 0.7269439697265625,
      "learning_rate": 0.000497521970591814,
      "loss": 3.7129,
      "step": 2840
    },
    {
      "epoch": 10.919540229885058,
      "grad_norm": 0.6559399366378784,
      "learning_rate": 0.0004975005794931402,
      "loss": 3.6963,
      "step": 2850
    },
    {
      "epoch": 10.957854406130268,
      "grad_norm": 0.6149498224258423,
      "learning_rate": 0.0004974790969274408,
      "loss": 3.6689,
      "step": 2860
    },
    {
      "epoch": 10.996168582375478,
      "grad_norm": 0.6337221264839172,
      "learning_rate": 0.0004974575229026549,
      "loss": 3.665,
      "step": 2870
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.6114493608474731,
      "eval_runtime": 4.3604,
      "eval_samples_per_second": 3924.392,
      "eval_steps_per_second": 7.797,
      "step": 2871
    },
    {
      "epoch": 11.03448275862069,
      "grad_norm": 0.6466646194458008,
      "learning_rate": 0.0004974358574267554,
      "loss": 3.6604,
      "step": 2880
    },
    {
      "epoch": 11.0727969348659,
      "grad_norm": 0.6386730670928955,
      "learning_rate": 0.0004974141005077492,
      "loss": 3.6784,
      "step": 2890
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 0.6334689855575562,
      "learning_rate": 0.0004973922521536767,
      "loss": 3.6651,
      "step": 2900
    },
    {
      "epoch": 11.149425287356323,
      "grad_norm": 0.627766489982605,
      "learning_rate": 0.0004973703123726122,
      "loss": 3.6686,
      "step": 2910
    },
    {
      "epoch": 11.187739463601533,
      "grad_norm": 0.6335583925247192,
      "learning_rate": 0.0004973482811726639,
      "loss": 3.6484,
      "step": 2920
    },
    {
      "epoch": 11.226053639846743,
      "grad_norm": 0.6360116600990295,
      "learning_rate": 0.0004973261585619739,
      "loss": 3.6715,
      "step": 2930
    },
    {
      "epoch": 11.264367816091955,
      "grad_norm": 0.6359638571739197,
      "learning_rate": 0.0004973039445487176,
      "loss": 3.6454,
      "step": 2940
    },
    {
      "epoch": 11.302681992337165,
      "grad_norm": 0.6224679350852966,
      "learning_rate": 0.0004972816391411045,
      "loss": 3.6476,
      "step": 2950
    },
    {
      "epoch": 11.340996168582375,
      "grad_norm": 0.6481630206108093,
      "learning_rate": 0.0004972592423473782,
      "loss": 3.6454,
      "step": 2960
    },
    {
      "epoch": 11.379310344827585,
      "grad_norm": 0.658258855342865,
      "learning_rate": 0.0004972367541758154,
      "loss": 3.672,
      "step": 2970
    },
    {
      "epoch": 11.417624521072797,
      "grad_norm": 0.6454787254333496,
      "learning_rate": 0.0004972141746347269,
      "loss": 3.6266,
      "step": 2980
    },
    {
      "epoch": 11.455938697318008,
      "grad_norm": 0.6363733410835266,
      "learning_rate": 0.0004971915037324573,
      "loss": 3.652,
      "step": 2990
    },
    {
      "epoch": 11.494252873563218,
      "grad_norm": 0.624674916267395,
      "learning_rate": 0.000497168741477385,
      "loss": 3.6369,
      "step": 3000
    },
    {
      "epoch": 11.53256704980843,
      "grad_norm": 0.6455360054969788,
      "learning_rate": 0.0004971458878779221,
      "loss": 3.6398,
      "step": 3010
    },
    {
      "epoch": 11.57088122605364,
      "grad_norm": 0.6804043054580688,
      "learning_rate": 0.0004971229429425142,
      "loss": 3.6441,
      "step": 3020
    },
    {
      "epoch": 11.60919540229885,
      "grad_norm": 0.6627867817878723,
      "learning_rate": 0.0004970999066796412,
      "loss": 3.6551,
      "step": 3030
    },
    {
      "epoch": 11.647509578544062,
      "grad_norm": 0.6206763386726379,
      "learning_rate": 0.0004970767790978163,
      "loss": 3.6444,
      "step": 3040
    },
    {
      "epoch": 11.685823754789272,
      "grad_norm": 0.6336628198623657,
      "learning_rate": 0.0004970535602055866,
      "loss": 3.6545,
      "step": 3050
    },
    {
      "epoch": 11.724137931034482,
      "grad_norm": 0.6221806406974792,
      "learning_rate": 0.000497030250011533,
      "loss": 3.6138,
      "step": 3060
    },
    {
      "epoch": 11.762452107279694,
      "grad_norm": 0.6700534224510193,
      "learning_rate": 0.00049700684852427,
      "loss": 3.6425,
      "step": 3070
    },
    {
      "epoch": 11.800766283524904,
      "grad_norm": 0.6359976530075073,
      "learning_rate": 0.0004969833557524459,
      "loss": 3.6344,
      "step": 3080
    },
    {
      "epoch": 11.839080459770114,
      "grad_norm": 0.6400163769721985,
      "learning_rate": 0.0004969597717047429,
      "loss": 3.6521,
      "step": 3090
    },
    {
      "epoch": 11.877394636015326,
      "grad_norm": 0.6107277274131775,
      "learning_rate": 0.0004969360963898767,
      "loss": 3.6581,
      "step": 3100
    },
    {
      "epoch": 11.915708812260537,
      "grad_norm": 0.6305680871009827,
      "learning_rate": 0.0004969123298165968,
      "loss": 3.6541,
      "step": 3110
    },
    {
      "epoch": 11.954022988505747,
      "grad_norm": 0.6141008734703064,
      "learning_rate": 0.0004968884719936864,
      "loss": 3.6278,
      "step": 3120
    },
    {
      "epoch": 11.992337164750959,
      "grad_norm": 0.6873835921287537,
      "learning_rate": 0.0004968645229299628,
      "loss": 3.6501,
      "step": 3130
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.5938262939453125,
      "eval_runtime": 3.9821,
      "eval_samples_per_second": 4297.234,
      "eval_steps_per_second": 8.538,
      "step": 3132
    },
    {
      "epoch": 12.030651340996169,
      "grad_norm": 0.6542406678199768,
      "learning_rate": 0.0004968404826342761,
      "loss": 3.5983,
      "step": 3140
    },
    {
      "epoch": 12.068965517241379,
      "grad_norm": 0.6370678544044495,
      "learning_rate": 0.0004968163511155112,
      "loss": 3.6311,
      "step": 3150
    },
    {
      "epoch": 12.10727969348659,
      "grad_norm": 0.6255068778991699,
      "learning_rate": 0.0004967921283825859,
      "loss": 3.6035,
      "step": 3160
    },
    {
      "epoch": 12.145593869731801,
      "grad_norm": 0.657779335975647,
      "learning_rate": 0.0004967678144444523,
      "loss": 3.6061,
      "step": 3170
    },
    {
      "epoch": 12.183908045977011,
      "grad_norm": 0.6794222593307495,
      "learning_rate": 0.0004967434093100957,
      "loss": 3.6089,
      "step": 3180
    },
    {
      "epoch": 12.222222222222221,
      "grad_norm": 0.6309499740600586,
      "learning_rate": 0.0004967189129885353,
      "loss": 3.6037,
      "step": 3190
    },
    {
      "epoch": 12.260536398467433,
      "grad_norm": 0.6015575528144836,
      "learning_rate": 0.0004966943254888244,
      "loss": 3.5858,
      "step": 3200
    },
    {
      "epoch": 12.298850574712644,
      "grad_norm": 0.596037745475769,
      "learning_rate": 0.0004966696468200492,
      "loss": 3.6009,
      "step": 3210
    },
    {
      "epoch": 12.337164750957854,
      "grad_norm": 0.6237718462944031,
      "learning_rate": 0.0004966448769913301,
      "loss": 3.6116,
      "step": 3220
    },
    {
      "epoch": 12.375478927203066,
      "grad_norm": 0.632522463798523,
      "learning_rate": 0.0004966200160118213,
      "loss": 3.5809,
      "step": 3230
    },
    {
      "epoch": 12.413793103448276,
      "grad_norm": 0.6544691324234009,
      "learning_rate": 0.0004965950638907105,
      "loss": 3.6034,
      "step": 3240
    },
    {
      "epoch": 12.452107279693486,
      "grad_norm": 0.6605625748634338,
      "learning_rate": 0.0004965700206372189,
      "loss": 3.6125,
      "step": 3250
    },
    {
      "epoch": 12.490421455938698,
      "grad_norm": 0.6683950424194336,
      "learning_rate": 0.0004965448862606016,
      "loss": 3.6236,
      "step": 3260
    },
    {
      "epoch": 12.528735632183908,
      "grad_norm": 0.6161261200904846,
      "learning_rate": 0.0004965196607701473,
      "loss": 3.5951,
      "step": 3270
    },
    {
      "epoch": 12.567049808429118,
      "grad_norm": 0.7578253149986267,
      "learning_rate": 0.0004964943441751786,
      "loss": 3.6088,
      "step": 3280
    },
    {
      "epoch": 12.60536398467433,
      "grad_norm": 0.6299041509628296,
      "learning_rate": 0.0004964689364850514,
      "loss": 3.596,
      "step": 3290
    },
    {
      "epoch": 12.64367816091954,
      "grad_norm": 0.6177541017532349,
      "learning_rate": 0.0004964434377091554,
      "loss": 3.5985,
      "step": 3300
    },
    {
      "epoch": 12.68199233716475,
      "grad_norm": 0.6118341684341431,
      "learning_rate": 0.0004964178478569142,
      "loss": 3.5866,
      "step": 3310
    },
    {
      "epoch": 12.720306513409962,
      "grad_norm": 0.6369765996932983,
      "learning_rate": 0.0004963921669377847,
      "loss": 3.5831,
      "step": 3320
    },
    {
      "epoch": 12.758620689655173,
      "grad_norm": 0.5959796905517578,
      "learning_rate": 0.0004963663949612575,
      "loss": 3.5558,
      "step": 3330
    },
    {
      "epoch": 12.796934865900383,
      "grad_norm": 0.6119282841682434,
      "learning_rate": 0.0004963405319368574,
      "loss": 3.5914,
      "step": 3340
    },
    {
      "epoch": 12.835249042145595,
      "grad_norm": 0.6936191916465759,
      "learning_rate": 0.000496314577874142,
      "loss": 3.5901,
      "step": 3350
    },
    {
      "epoch": 12.873563218390805,
      "grad_norm": 0.5867137908935547,
      "learning_rate": 0.000496288532782703,
      "loss": 3.538,
      "step": 3360
    },
    {
      "epoch": 12.911877394636015,
      "grad_norm": 0.6485968828201294,
      "learning_rate": 0.0004962623966721658,
      "loss": 3.5778,
      "step": 3370
    },
    {
      "epoch": 12.950191570881227,
      "grad_norm": 0.6793025732040405,
      "learning_rate": 0.0004962361695521895,
      "loss": 3.5698,
      "step": 3380
    },
    {
      "epoch": 12.988505747126437,
      "grad_norm": 0.6182209253311157,
      "learning_rate": 0.0004962098514324664,
      "loss": 3.605,
      "step": 3390
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.579775333404541,
      "eval_runtime": 3.9978,
      "eval_samples_per_second": 4280.405,
      "eval_steps_per_second": 8.505,
      "step": 3393
    },
    {
      "epoch": 13.026819923371647,
      "grad_norm": 0.6456747651100159,
      "learning_rate": 0.0004961834423227228,
      "loss": 3.5531,
      "step": 3400
    },
    {
      "epoch": 13.065134099616857,
      "grad_norm": 0.6387558579444885,
      "learning_rate": 0.0004961569422327185,
      "loss": 3.5754,
      "step": 3410
    },
    {
      "epoch": 13.10344827586207,
      "grad_norm": 0.6014226675033569,
      "learning_rate": 0.0004961303511722469,
      "loss": 3.5377,
      "step": 3420
    },
    {
      "epoch": 13.14176245210728,
      "grad_norm": 0.6261295676231384,
      "learning_rate": 0.0004961036691511352,
      "loss": 3.556,
      "step": 3430
    },
    {
      "epoch": 13.18007662835249,
      "grad_norm": 0.6289821267127991,
      "learning_rate": 0.0004960768961792441,
      "loss": 3.547,
      "step": 3440
    },
    {
      "epoch": 13.218390804597702,
      "grad_norm": 0.6045966744422913,
      "learning_rate": 0.0004960500322664679,
      "loss": 3.5653,
      "step": 3450
    },
    {
      "epoch": 13.256704980842912,
      "grad_norm": 0.6155398488044739,
      "learning_rate": 0.0004960230774227342,
      "loss": 3.5572,
      "step": 3460
    },
    {
      "epoch": 13.295019157088122,
      "grad_norm": 0.61188143491745,
      "learning_rate": 0.0004959960316580048,
      "loss": 3.5361,
      "step": 3470
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 0.6042096018791199,
      "learning_rate": 0.0004959688949822749,
      "loss": 3.5472,
      "step": 3480
    },
    {
      "epoch": 13.371647509578544,
      "grad_norm": 0.6145840287208557,
      "learning_rate": 0.0004959416674055728,
      "loss": 3.5459,
      "step": 3490
    },
    {
      "epoch": 13.409961685823754,
      "grad_norm": 0.623555064201355,
      "learning_rate": 0.0004959143489379612,
      "loss": 3.5499,
      "step": 3500
    },
    {
      "epoch": 13.448275862068966,
      "grad_norm": 0.7440997362136841,
      "learning_rate": 0.0004958869395895359,
      "loss": 3.5293,
      "step": 3510
    },
    {
      "epoch": 13.486590038314176,
      "grad_norm": 0.6354436278343201,
      "learning_rate": 0.0004958594393704262,
      "loss": 3.5532,
      "step": 3520
    },
    {
      "epoch": 13.524904214559387,
      "grad_norm": 0.5999276638031006,
      "learning_rate": 0.0004958318482907953,
      "loss": 3.5378,
      "step": 3530
    },
    {
      "epoch": 13.563218390804598,
      "grad_norm": 0.5864585638046265,
      "learning_rate": 0.0004958041663608399,
      "loss": 3.544,
      "step": 3540
    },
    {
      "epoch": 13.601532567049809,
      "grad_norm": 0.615580141544342,
      "learning_rate": 0.0004957763935907902,
      "loss": 3.5513,
      "step": 3550
    },
    {
      "epoch": 13.639846743295019,
      "grad_norm": 0.5930789113044739,
      "learning_rate": 0.0004957485299909098,
      "loss": 3.5183,
      "step": 3560
    },
    {
      "epoch": 13.678160919540229,
      "grad_norm": 0.6266682744026184,
      "learning_rate": 0.0004957205755714962,
      "loss": 3.5709,
      "step": 3570
    },
    {
      "epoch": 13.71647509578544,
      "grad_norm": 0.599166989326477,
      "learning_rate": 0.0004956925303428803,
      "loss": 3.5476,
      "step": 3580
    },
    {
      "epoch": 13.754789272030651,
      "grad_norm": 0.5901556015014648,
      "learning_rate": 0.0004956643943154265,
      "loss": 3.5553,
      "step": 3590
    },
    {
      "epoch": 13.793103448275861,
      "grad_norm": 0.6083163619041443,
      "learning_rate": 0.0004956361674995329,
      "loss": 3.5444,
      "step": 3600
    },
    {
      "epoch": 13.831417624521073,
      "grad_norm": 0.5994539856910706,
      "learning_rate": 0.0004956078499056312,
      "loss": 3.5545,
      "step": 3610
    },
    {
      "epoch": 13.869731800766283,
      "grad_norm": 0.6586935520172119,
      "learning_rate": 0.0004955794415441864,
      "loss": 3.5304,
      "step": 3620
    },
    {
      "epoch": 13.908045977011493,
      "grad_norm": 0.5836783051490784,
      "learning_rate": 0.0004955509424256971,
      "loss": 3.5092,
      "step": 3630
    },
    {
      "epoch": 13.946360153256705,
      "grad_norm": 0.6092826724052429,
      "learning_rate": 0.0004955223525606957,
      "loss": 3.5469,
      "step": 3640
    },
    {
      "epoch": 13.984674329501916,
      "grad_norm": 0.609157145023346,
      "learning_rate": 0.0004954936719597477,
      "loss": 3.5444,
      "step": 3650
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.5644727945327759,
      "eval_runtime": 3.9917,
      "eval_samples_per_second": 4286.947,
      "eval_steps_per_second": 8.518,
      "step": 3654
    },
    {
      "epoch": 14.022988505747126,
      "grad_norm": 0.6261510848999023,
      "learning_rate": 0.0004954649006334527,
      "loss": 3.506,
      "step": 3660
    },
    {
      "epoch": 14.061302681992338,
      "grad_norm": 0.6221017837524414,
      "learning_rate": 0.0004954360385924433,
      "loss": 3.5022,
      "step": 3670
    },
    {
      "epoch": 14.099616858237548,
      "grad_norm": 0.590529203414917,
      "learning_rate": 0.0004954070858473858,
      "loss": 3.5186,
      "step": 3680
    },
    {
      "epoch": 14.137931034482758,
      "grad_norm": 0.628675103187561,
      "learning_rate": 0.0004953780424089803,
      "loss": 3.4883,
      "step": 3690
    },
    {
      "epoch": 14.17624521072797,
      "grad_norm": 0.6147618293762207,
      "learning_rate": 0.00049534890828796,
      "loss": 3.5219,
      "step": 3700
    },
    {
      "epoch": 14.21455938697318,
      "grad_norm": 0.6034520864486694,
      "learning_rate": 0.0004953196834950917,
      "loss": 3.5403,
      "step": 3710
    },
    {
      "epoch": 14.25287356321839,
      "grad_norm": 0.6068375706672668,
      "learning_rate": 0.000495290368041176,
      "loss": 3.514,
      "step": 3720
    },
    {
      "epoch": 14.291187739463602,
      "grad_norm": 0.6250054836273193,
      "learning_rate": 0.0004952609619370467,
      "loss": 3.5006,
      "step": 3730
    },
    {
      "epoch": 14.329501915708812,
      "grad_norm": 0.6065638065338135,
      "learning_rate": 0.0004952314651935713,
      "loss": 3.4967,
      "step": 3740
    },
    {
      "epoch": 14.367816091954023,
      "grad_norm": 0.5926713347434998,
      "learning_rate": 0.0004952018778216506,
      "loss": 3.4987,
      "step": 3750
    },
    {
      "epoch": 14.406130268199234,
      "grad_norm": 0.6091626286506653,
      "learning_rate": 0.0004951721998322189,
      "loss": 3.5427,
      "step": 3760
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 0.5842257738113403,
      "learning_rate": 0.0004951424312362444,
      "loss": 3.5257,
      "step": 3770
    },
    {
      "epoch": 14.482758620689655,
      "grad_norm": 0.5846444368362427,
      "learning_rate": 0.0004951125720447281,
      "loss": 3.5034,
      "step": 3780
    },
    {
      "epoch": 14.521072796934867,
      "grad_norm": 0.6353232860565186,
      "learning_rate": 0.0004950826222687052,
      "loss": 3.504,
      "step": 3790
    },
    {
      "epoch": 14.559386973180077,
      "grad_norm": 0.6377238631248474,
      "learning_rate": 0.0004950525819192436,
      "loss": 3.5076,
      "step": 3800
    },
    {
      "epoch": 14.597701149425287,
      "grad_norm": 0.6142462491989136,
      "learning_rate": 0.0004950224510074456,
      "loss": 3.5183,
      "step": 3810
    },
    {
      "epoch": 14.636015325670499,
      "grad_norm": 0.5732647776603699,
      "learning_rate": 0.0004949922295444461,
      "loss": 3.4986,
      "step": 3820
    },
    {
      "epoch": 14.67432950191571,
      "grad_norm": 0.5872717499732971,
      "learning_rate": 0.000494961917541414,
      "loss": 3.5002,
      "step": 3830
    },
    {
      "epoch": 14.71264367816092,
      "grad_norm": 0.6527543067932129,
      "learning_rate": 0.0004949315150095513,
      "loss": 3.4986,
      "step": 3840
    },
    {
      "epoch": 14.75095785440613,
      "grad_norm": 0.6125829815864563,
      "learning_rate": 0.0004949010219600939,
      "loss": 3.5183,
      "step": 3850
    },
    {
      "epoch": 14.789272030651341,
      "grad_norm": 0.5928277969360352,
      "learning_rate": 0.0004948704384043107,
      "loss": 3.475,
      "step": 3860
    },
    {
      "epoch": 14.827586206896552,
      "grad_norm": 0.5882531404495239,
      "learning_rate": 0.0004948397643535043,
      "loss": 3.4985,
      "step": 3870
    },
    {
      "epoch": 14.865900383141762,
      "grad_norm": 0.5904949307441711,
      "learning_rate": 0.0004948089998190108,
      "loss": 3.5242,
      "step": 3880
    },
    {
      "epoch": 14.904214559386974,
      "grad_norm": 0.5797762870788574,
      "learning_rate": 0.0004947781448121995,
      "loss": 3.4756,
      "step": 3890
    },
    {
      "epoch": 14.942528735632184,
      "grad_norm": 0.5856850743293762,
      "learning_rate": 0.0004947471993444734,
      "loss": 3.5101,
      "step": 3900
    },
    {
      "epoch": 14.980842911877394,
      "grad_norm": 0.6089755296707153,
      "learning_rate": 0.0004947161634272685,
      "loss": 3.4896,
      "step": 3910
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.552215576171875,
      "eval_runtime": 4.085,
      "eval_samples_per_second": 4188.991,
      "eval_steps_per_second": 8.323,
      "step": 3915
    },
    {
      "epoch": 15.019157088122606,
      "grad_norm": 0.5776030421257019,
      "learning_rate": 0.0004946850370720549,
      "loss": 3.491,
      "step": 3920
    },
    {
      "epoch": 15.057471264367816,
      "grad_norm": 0.6274594068527222,
      "learning_rate": 0.0004946538202903356,
      "loss": 3.469,
      "step": 3930
    },
    {
      "epoch": 15.095785440613026,
      "grad_norm": 0.6077725887298584,
      "learning_rate": 0.000494622513093647,
      "loss": 3.4617,
      "step": 3940
    },
    {
      "epoch": 15.134099616858238,
      "grad_norm": 0.6349509358406067,
      "learning_rate": 0.0004945911154935593,
      "loss": 3.4686,
      "step": 3950
    },
    {
      "epoch": 15.172413793103448,
      "grad_norm": 0.5963027477264404,
      "learning_rate": 0.0004945596275016758,
      "loss": 3.4596,
      "step": 3960
    },
    {
      "epoch": 15.210727969348659,
      "grad_norm": 0.6175850033760071,
      "learning_rate": 0.0004945280491296334,
      "loss": 3.4852,
      "step": 3970
    },
    {
      "epoch": 15.24904214559387,
      "grad_norm": 0.6020484566688538,
      "learning_rate": 0.000494496380389102,
      "loss": 3.4465,
      "step": 3980
    },
    {
      "epoch": 15.28735632183908,
      "grad_norm": 0.6136419773101807,
      "learning_rate": 0.0004944646212917855,
      "loss": 3.4536,
      "step": 3990
    },
    {
      "epoch": 15.32567049808429,
      "grad_norm": 0.6239376068115234,
      "learning_rate": 0.0004944327718494207,
      "loss": 3.4793,
      "step": 4000
    },
    {
      "epoch": 15.363984674329503,
      "grad_norm": 0.5846113562583923,
      "learning_rate": 0.000494400832073778,
      "loss": 3.5057,
      "step": 4010
    },
    {
      "epoch": 15.402298850574713,
      "grad_norm": 0.6431229710578918,
      "learning_rate": 0.0004943688019766613,
      "loss": 3.5023,
      "step": 4020
    },
    {
      "epoch": 15.440613026819923,
      "grad_norm": 0.6059973835945129,
      "learning_rate": 0.0004943366815699076,
      "loss": 3.4696,
      "step": 4030
    },
    {
      "epoch": 15.478927203065133,
      "grad_norm": 0.6162024736404419,
      "learning_rate": 0.0004943044708653874,
      "loss": 3.4779,
      "step": 4040
    },
    {
      "epoch": 15.517241379310345,
      "grad_norm": 0.57511305809021,
      "learning_rate": 0.0004942721698750046,
      "loss": 3.4541,
      "step": 4050
    },
    {
      "epoch": 15.555555555555555,
      "grad_norm": 0.6060734987258911,
      "learning_rate": 0.0004942397786106965,
      "loss": 3.4655,
      "step": 4060
    },
    {
      "epoch": 15.593869731800766,
      "grad_norm": 0.5709812641143799,
      "learning_rate": 0.0004942072970844336,
      "loss": 3.4619,
      "step": 4070
    },
    {
      "epoch": 15.632183908045977,
      "grad_norm": 0.5900779366493225,
      "learning_rate": 0.0004941747253082201,
      "loss": 3.454,
      "step": 4080
    },
    {
      "epoch": 15.670498084291188,
      "grad_norm": 0.6157929301261902,
      "learning_rate": 0.0004941420632940931,
      "loss": 3.4891,
      "step": 4090
    },
    {
      "epoch": 15.708812260536398,
      "grad_norm": 0.6067290902137756,
      "learning_rate": 0.0004941093110541232,
      "loss": 3.4647,
      "step": 4100
    },
    {
      "epoch": 15.74712643678161,
      "grad_norm": 0.5800323486328125,
      "learning_rate": 0.0004940764686004146,
      "loss": 3.4629,
      "step": 4110
    },
    {
      "epoch": 15.78544061302682,
      "grad_norm": 0.5963418483734131,
      "learning_rate": 0.0004940435359451047,
      "loss": 3.4659,
      "step": 4120
    },
    {
      "epoch": 15.82375478927203,
      "grad_norm": 0.5721551179885864,
      "learning_rate": 0.000494010513100364,
      "loss": 3.4623,
      "step": 4130
    },
    {
      "epoch": 15.862068965517242,
      "grad_norm": 0.5918585062026978,
      "learning_rate": 0.0004939774000783966,
      "loss": 3.4718,
      "step": 4140
    },
    {
      "epoch": 15.900383141762452,
      "grad_norm": 0.5655192732810974,
      "learning_rate": 0.0004939441968914397,
      "loss": 3.4414,
      "step": 4150
    },
    {
      "epoch": 15.938697318007662,
      "grad_norm": 0.6295875906944275,
      "learning_rate": 0.0004939109035517643,
      "loss": 3.4674,
      "step": 4160
    },
    {
      "epoch": 15.977011494252874,
      "grad_norm": 0.5755696892738342,
      "learning_rate": 0.000493877520071674,
      "loss": 3.4479,
      "step": 4170
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.5427868366241455,
      "eval_runtime": 3.9201,
      "eval_samples_per_second": 4365.176,
      "eval_steps_per_second": 8.673,
      "step": 4176
    },
    {
      "epoch": 16.015325670498083,
      "grad_norm": 0.5739682912826538,
      "learning_rate": 0.0004938440464635064,
      "loss": 3.4756,
      "step": 4180
    },
    {
      "epoch": 16.053639846743295,
      "grad_norm": 0.6144437789916992,
      "learning_rate": 0.0004938104827396319,
      "loss": 3.425,
      "step": 4190
    },
    {
      "epoch": 16.091954022988507,
      "grad_norm": 0.5796476006507874,
      "learning_rate": 0.0004937768289124547,
      "loss": 3.4186,
      "step": 4200
    },
    {
      "epoch": 16.130268199233715,
      "grad_norm": 0.6385205388069153,
      "learning_rate": 0.0004937430849944115,
      "loss": 3.4519,
      "step": 4210
    },
    {
      "epoch": 16.168582375478927,
      "grad_norm": 0.5885963439941406,
      "learning_rate": 0.0004937092509979732,
      "loss": 3.4408,
      "step": 4220
    },
    {
      "epoch": 16.20689655172414,
      "grad_norm": 0.5665003657341003,
      "learning_rate": 0.0004936753269356435,
      "loss": 3.4217,
      "step": 4230
    },
    {
      "epoch": 16.245210727969347,
      "grad_norm": 0.5870096683502197,
      "learning_rate": 0.0004936413128199596,
      "loss": 3.4439,
      "step": 4240
    },
    {
      "epoch": 16.28352490421456,
      "grad_norm": 0.5918912887573242,
      "learning_rate": 0.0004936072086634915,
      "loss": 3.4468,
      "step": 4250
    },
    {
      "epoch": 16.32183908045977,
      "grad_norm": 0.7005227208137512,
      "learning_rate": 0.0004935730144788432,
      "loss": 3.4589,
      "step": 4260
    },
    {
      "epoch": 16.36015325670498,
      "grad_norm": 0.612886369228363,
      "learning_rate": 0.0004935387302786513,
      "loss": 3.4509,
      "step": 4270
    },
    {
      "epoch": 16.39846743295019,
      "grad_norm": 0.5867318511009216,
      "learning_rate": 0.0004935043560755862,
      "loss": 3.4352,
      "step": 4280
    },
    {
      "epoch": 16.436781609195403,
      "grad_norm": 0.6199052333831787,
      "learning_rate": 0.0004934698918823512,
      "loss": 3.4382,
      "step": 4290
    },
    {
      "epoch": 16.47509578544061,
      "grad_norm": 0.5863763093948364,
      "learning_rate": 0.000493435337711683,
      "loss": 3.4624,
      "step": 4300
    },
    {
      "epoch": 16.513409961685824,
      "grad_norm": 0.5877918004989624,
      "learning_rate": 0.0004934006935763517,
      "loss": 3.4488,
      "step": 4310
    },
    {
      "epoch": 16.551724137931036,
      "grad_norm": 0.6022346615791321,
      "learning_rate": 0.0004933659594891602,
      "loss": 3.4607,
      "step": 4320
    },
    {
      "epoch": 16.590038314176244,
      "grad_norm": 0.5835731029510498,
      "learning_rate": 0.0004933311354629451,
      "loss": 3.4437,
      "step": 4330
    },
    {
      "epoch": 16.628352490421456,
      "grad_norm": 0.5727782249450684,
      "learning_rate": 0.0004932962215105761,
      "loss": 3.4434,
      "step": 4340
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 0.5751965641975403,
      "learning_rate": 0.0004932612176449559,
      "loss": 3.4301,
      "step": 4350
    },
    {
      "epoch": 16.704980842911876,
      "grad_norm": 0.5792434811592102,
      "learning_rate": 0.000493226123879021,
      "loss": 3.4141,
      "step": 4360
    },
    {
      "epoch": 16.743295019157088,
      "grad_norm": 0.6179944276809692,
      "learning_rate": 0.0004931909402257404,
      "loss": 3.4435,
      "step": 4370
    },
    {
      "epoch": 16.7816091954023,
      "grad_norm": 0.5763678550720215,
      "learning_rate": 0.0004931556666981167,
      "loss": 3.4316,
      "step": 4380
    },
    {
      "epoch": 16.81992337164751,
      "grad_norm": 0.5741292834281921,
      "learning_rate": 0.000493120303309186,
      "loss": 3.4357,
      "step": 4390
    },
    {
      "epoch": 16.85823754789272,
      "grad_norm": 0.6093788743019104,
      "learning_rate": 0.000493084850072017,
      "loss": 3.4053,
      "step": 4400
    },
    {
      "epoch": 16.896551724137932,
      "grad_norm": 0.5925942063331604,
      "learning_rate": 0.000493049306999712,
      "loss": 3.4017,
      "step": 4410
    },
    {
      "epoch": 16.93486590038314,
      "grad_norm": 0.5886186361312866,
      "learning_rate": 0.0004930136741054063,
      "loss": 3.4044,
      "step": 4420
    },
    {
      "epoch": 16.973180076628353,
      "grad_norm": 0.5832160115242004,
      "learning_rate": 0.0004929779514022686,
      "loss": 3.427,
      "step": 4430
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.5328837633132935,
      "eval_runtime": 4.1921,
      "eval_samples_per_second": 4081.921,
      "eval_steps_per_second": 8.11,
      "step": 4437
    },
    {
      "epoch": 17.011494252873565,
      "grad_norm": 0.5784831047058105,
      "learning_rate": 0.0004929421389035007,
      "loss": 3.3924,
      "step": 4440
    },
    {
      "epoch": 17.049808429118773,
      "grad_norm": 0.5868982672691345,
      "learning_rate": 0.0004929062366223376,
      "loss": 3.4206,
      "step": 4450
    },
    {
      "epoch": 17.088122605363985,
      "grad_norm": 0.5768109560012817,
      "learning_rate": 0.0004928702445720473,
      "loss": 3.3911,
      "step": 4460
    },
    {
      "epoch": 17.126436781609197,
      "grad_norm": 0.6058052778244019,
      "learning_rate": 0.0004928341627659312,
      "loss": 3.4059,
      "step": 4470
    },
    {
      "epoch": 17.164750957854405,
      "grad_norm": 0.6622015833854675,
      "learning_rate": 0.0004927979912173238,
      "loss": 3.4192,
      "step": 4480
    },
    {
      "epoch": 17.203065134099617,
      "grad_norm": 0.603036105632782,
      "learning_rate": 0.0004927617299395928,
      "loss": 3.4043,
      "step": 4490
    },
    {
      "epoch": 17.24137931034483,
      "grad_norm": 0.5781179070472717,
      "learning_rate": 0.000492725378946139,
      "loss": 3.4003,
      "step": 4500
    },
    {
      "epoch": 17.279693486590038,
      "grad_norm": 0.5581566095352173,
      "learning_rate": 0.0004926889382503962,
      "loss": 3.385,
      "step": 4510
    },
    {
      "epoch": 17.31800766283525,
      "grad_norm": 0.5786114931106567,
      "learning_rate": 0.0004926524078658319,
      "loss": 3.3858,
      "step": 4520
    },
    {
      "epoch": 17.35632183908046,
      "grad_norm": 0.6024898886680603,
      "learning_rate": 0.0004926157878059461,
      "loss": 3.4152,
      "step": 4530
    },
    {
      "epoch": 17.39463601532567,
      "grad_norm": 0.6098840236663818,
      "learning_rate": 0.0004925790780842723,
      "loss": 3.3786,
      "step": 4540
    },
    {
      "epoch": 17.43295019157088,
      "grad_norm": 0.5488906502723694,
      "learning_rate": 0.000492542278714377,
      "loss": 3.4304,
      "step": 4550
    },
    {
      "epoch": 17.47126436781609,
      "grad_norm": 0.5668551921844482,
      "learning_rate": 0.0004925053897098599,
      "loss": 3.407,
      "step": 4560
    },
    {
      "epoch": 17.509578544061302,
      "grad_norm": 0.5923009514808655,
      "learning_rate": 0.0004924684110843539,
      "loss": 3.3901,
      "step": 4570
    },
    {
      "epoch": 17.547892720306514,
      "grad_norm": 0.568081796169281,
      "learning_rate": 0.0004924313428515248,
      "loss": 3.418,
      "step": 4580
    },
    {
      "epoch": 17.586206896551722,
      "grad_norm": 0.549176037311554,
      "learning_rate": 0.0004923941850250717,
      "loss": 3.4107,
      "step": 4590
    },
    {
      "epoch": 17.624521072796934,
      "grad_norm": 0.57121342420578,
      "learning_rate": 0.0004923569376187267,
      "loss": 3.4137,
      "step": 4600
    },
    {
      "epoch": 17.662835249042146,
      "grad_norm": 0.5939942002296448,
      "learning_rate": 0.0004923196006462551,
      "loss": 3.4101,
      "step": 4610
    },
    {
      "epoch": 17.701149425287355,
      "grad_norm": 0.5547963976860046,
      "learning_rate": 0.0004922821741214552,
      "loss": 3.4016,
      "step": 4620
    },
    {
      "epoch": 17.739463601532567,
      "grad_norm": 0.5830602645874023,
      "learning_rate": 0.0004922446580581586,
      "loss": 3.4066,
      "step": 4630
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 0.5918421745300293,
      "learning_rate": 0.0004922070524702298,
      "loss": 3.3993,
      "step": 4640
    },
    {
      "epoch": 17.816091954022987,
      "grad_norm": 0.5681458115577698,
      "learning_rate": 0.0004921693573715665,
      "loss": 3.4001,
      "step": 4650
    },
    {
      "epoch": 17.8544061302682,
      "grad_norm": 0.5650302767753601,
      "learning_rate": 0.000492131572776099,
      "loss": 3.3814,
      "step": 4660
    },
    {
      "epoch": 17.89272030651341,
      "grad_norm": 0.5626837611198425,
      "learning_rate": 0.0004920936986977916,
      "loss": 3.3928,
      "step": 4670
    },
    {
      "epoch": 17.93103448275862,
      "grad_norm": 0.5667978525161743,
      "learning_rate": 0.0004920557351506408,
      "loss": 3.439,
      "step": 4680
    },
    {
      "epoch": 17.96934865900383,
      "grad_norm": 0.5718722939491272,
      "learning_rate": 0.0004920176821486767,
      "loss": 3.4262,
      "step": 4690
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.524874210357666,
      "eval_runtime": 4.2148,
      "eval_samples_per_second": 4059.996,
      "eval_steps_per_second": 8.067,
      "step": 4698
    },
    {
      "epoch": 18.007662835249043,
      "grad_norm": 0.568489670753479,
      "learning_rate": 0.0004919795397059623,
      "loss": 3.3928,
      "step": 4700
    },
    {
      "epoch": 18.04597701149425,
      "grad_norm": 0.5768930315971375,
      "learning_rate": 0.0004919413078365935,
      "loss": 3.3916,
      "step": 4710
    },
    {
      "epoch": 18.084291187739463,
      "grad_norm": 0.5554691553115845,
      "learning_rate": 0.0004919029865546995,
      "loss": 3.3895,
      "step": 4720
    },
    {
      "epoch": 18.122605363984675,
      "grad_norm": 0.5997123122215271,
      "learning_rate": 0.0004918645758744423,
      "loss": 3.3794,
      "step": 4730
    },
    {
      "epoch": 18.160919540229884,
      "grad_norm": 0.5530970692634583,
      "learning_rate": 0.000491826075810017,
      "loss": 3.3863,
      "step": 4740
    },
    {
      "epoch": 18.199233716475096,
      "grad_norm": 0.5918648838996887,
      "learning_rate": 0.000491787486375652,
      "loss": 3.3957,
      "step": 4750
    },
    {
      "epoch": 18.237547892720308,
      "grad_norm": 0.585006058216095,
      "learning_rate": 0.0004917488075856086,
      "loss": 3.3639,
      "step": 4760
    },
    {
      "epoch": 18.275862068965516,
      "grad_norm": 0.562411367893219,
      "learning_rate": 0.0004917100394541807,
      "loss": 3.3834,
      "step": 4770
    },
    {
      "epoch": 18.314176245210728,
      "grad_norm": 0.53377765417099,
      "learning_rate": 0.0004916711819956958,
      "loss": 3.3576,
      "step": 4780
    },
    {
      "epoch": 18.35249042145594,
      "grad_norm": 0.5774298906326294,
      "learning_rate": 0.0004916322352245142,
      "loss": 3.3839,
      "step": 4790
    },
    {
      "epoch": 18.39080459770115,
      "grad_norm": 0.5725610256195068,
      "learning_rate": 0.000491593199155029,
      "loss": 3.3989,
      "step": 4800
    },
    {
      "epoch": 18.42911877394636,
      "grad_norm": 0.5778921842575073,
      "learning_rate": 0.0004915540738016667,
      "loss": 3.3665,
      "step": 4810
    },
    {
      "epoch": 18.467432950191572,
      "grad_norm": 0.5554885268211365,
      "learning_rate": 0.0004915148591788863,
      "loss": 3.3854,
      "step": 4820
    },
    {
      "epoch": 18.50574712643678,
      "grad_norm": 0.5747888088226318,
      "learning_rate": 0.0004914755553011804,
      "loss": 3.3674,
      "step": 4830
    },
    {
      "epoch": 18.544061302681992,
      "grad_norm": 0.6440844535827637,
      "learning_rate": 0.0004914361621830739,
      "loss": 3.3755,
      "step": 4840
    },
    {
      "epoch": 18.582375478927204,
      "grad_norm": 0.5391805171966553,
      "learning_rate": 0.0004913966798391254,
      "loss": 3.3772,
      "step": 4850
    },
    {
      "epoch": 18.620689655172413,
      "grad_norm": 0.5667957663536072,
      "learning_rate": 0.0004913571082839258,
      "loss": 3.3854,
      "step": 4860
    },
    {
      "epoch": 18.659003831417625,
      "grad_norm": 0.5932502150535583,
      "learning_rate": 0.0004913174475320994,
      "loss": 3.3718,
      "step": 4870
    },
    {
      "epoch": 18.697318007662837,
      "grad_norm": 0.5853309631347656,
      "learning_rate": 0.0004912776975983035,
      "loss": 3.3747,
      "step": 4880
    },
    {
      "epoch": 18.735632183908045,
      "grad_norm": 0.5694151520729065,
      "learning_rate": 0.0004912378584972279,
      "loss": 3.3857,
      "step": 4890
    },
    {
      "epoch": 18.773946360153257,
      "grad_norm": 0.5657628774642944,
      "learning_rate": 0.0004911979302435959,
      "loss": 3.3751,
      "step": 4900
    },
    {
      "epoch": 18.81226053639847,
      "grad_norm": 0.5612602829933167,
      "learning_rate": 0.0004911579128521632,
      "loss": 3.3852,
      "step": 4910
    },
    {
      "epoch": 18.850574712643677,
      "grad_norm": 0.5646210312843323,
      "learning_rate": 0.000491117806337719,
      "loss": 3.3616,
      "step": 4920
    },
    {
      "epoch": 18.88888888888889,
      "grad_norm": 0.6017677783966064,
      "learning_rate": 0.0004910776107150853,
      "loss": 3.3765,
      "step": 4930
    },
    {
      "epoch": 18.9272030651341,
      "grad_norm": 0.5754407644271851,
      "learning_rate": 0.0004910373259991166,
      "loss": 3.3876,
      "step": 4940
    },
    {
      "epoch": 18.96551724137931,
      "grad_norm": 0.5733324289321899,
      "learning_rate": 0.0004909969522047008,
      "loss": 3.3702,
      "step": 4950
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.5202478170394897,
      "eval_runtime": 3.9955,
      "eval_samples_per_second": 4282.769,
      "eval_steps_per_second": 8.509,
      "step": 4959
    },
    {
      "epoch": 19.00383141762452,
      "grad_norm": 0.5623795986175537,
      "learning_rate": 0.0004909564893467585,
      "loss": 3.3696,
      "step": 4960
    },
    {
      "epoch": 19.042145593869733,
      "grad_norm": 0.54261714220047,
      "learning_rate": 0.0004909159374402433,
      "loss": 3.3434,
      "step": 4970
    },
    {
      "epoch": 19.080459770114942,
      "grad_norm": 0.5678743720054626,
      "learning_rate": 0.0004908752965001417,
      "loss": 3.3662,
      "step": 4980
    },
    {
      "epoch": 19.118773946360154,
      "grad_norm": 0.5395678877830505,
      "learning_rate": 0.000490834566541473,
      "loss": 3.3478,
      "step": 4990
    },
    {
      "epoch": 19.157088122605366,
      "grad_norm": 0.6159077286720276,
      "learning_rate": 0.0004907937475792896,
      "loss": 3.3601,
      "step": 5000
    },
    {
      "epoch": 19.195402298850574,
      "grad_norm": 0.5553202033042908,
      "learning_rate": 0.0004907528396286766,
      "loss": 3.3408,
      "step": 5010
    },
    {
      "epoch": 19.233716475095786,
      "grad_norm": 0.5734204053878784,
      "learning_rate": 0.000490711842704752,
      "loss": 3.3409,
      "step": 5020
    },
    {
      "epoch": 19.272030651340994,
      "grad_norm": 0.5904489159584045,
      "learning_rate": 0.0004906707568226668,
      "loss": 3.3618,
      "step": 5030
    },
    {
      "epoch": 19.310344827586206,
      "grad_norm": 0.5772026181221008,
      "learning_rate": 0.0004906295819976049,
      "loss": 3.3449,
      "step": 5040
    },
    {
      "epoch": 19.34865900383142,
      "grad_norm": 0.5819700956344604,
      "learning_rate": 0.0004905883182447829,
      "loss": 3.3572,
      "step": 5050
    },
    {
      "epoch": 19.386973180076627,
      "grad_norm": 0.5614970326423645,
      "learning_rate": 0.0004905469655794503,
      "loss": 3.3616,
      "step": 5060
    },
    {
      "epoch": 19.42528735632184,
      "grad_norm": 0.572476863861084,
      "learning_rate": 0.0004905055240168897,
      "loss": 3.3401,
      "step": 5070
    },
    {
      "epoch": 19.46360153256705,
      "grad_norm": 0.5639654994010925,
      "learning_rate": 0.0004904639935724161,
      "loss": 3.3591,
      "step": 5080
    },
    {
      "epoch": 19.50191570881226,
      "grad_norm": 0.5499991774559021,
      "learning_rate": 0.0004904223742613777,
      "loss": 3.3479,
      "step": 5090
    },
    {
      "epoch": 19.54022988505747,
      "grad_norm": 0.5522267818450928,
      "learning_rate": 0.0004903806660991557,
      "loss": 3.3354,
      "step": 5100
    },
    {
      "epoch": 19.578544061302683,
      "grad_norm": 0.5377187132835388,
      "learning_rate": 0.0004903388691011635,
      "loss": 3.3385,
      "step": 5110
    },
    {
      "epoch": 19.61685823754789,
      "grad_norm": 0.5623925924301147,
      "learning_rate": 0.000490296983282848,
      "loss": 3.3495,
      "step": 5120
    },
    {
      "epoch": 19.655172413793103,
      "grad_norm": 0.5529911518096924,
      "learning_rate": 0.0004902550086596885,
      "loss": 3.3635,
      "step": 5130
    },
    {
      "epoch": 19.693486590038315,
      "grad_norm": 0.5456171035766602,
      "learning_rate": 0.0004902129452471973,
      "loss": 3.3591,
      "step": 5140
    },
    {
      "epoch": 19.731800766283524,
      "grad_norm": 0.5597068667411804,
      "learning_rate": 0.0004901707930609195,
      "loss": 3.3663,
      "step": 5150
    },
    {
      "epoch": 19.770114942528735,
      "grad_norm": 0.5559124946594238,
      "learning_rate": 0.000490128552116433,
      "loss": 3.3529,
      "step": 5160
    },
    {
      "epoch": 19.808429118773947,
      "grad_norm": 0.5607943534851074,
      "learning_rate": 0.0004900862224293485,
      "loss": 3.3542,
      "step": 5170
    },
    {
      "epoch": 19.846743295019156,
      "grad_norm": 0.5425288677215576,
      "learning_rate": 0.0004900438040153093,
      "loss": 3.3515,
      "step": 5180
    },
    {
      "epoch": 19.885057471264368,
      "grad_norm": 0.5757127404212952,
      "learning_rate": 0.000490001296889992,
      "loss": 3.3671,
      "step": 5190
    },
    {
      "epoch": 19.92337164750958,
      "grad_norm": 0.5556961297988892,
      "learning_rate": 0.0004899587010691053,
      "loss": 3.3648,
      "step": 5200
    },
    {
      "epoch": 19.961685823754788,
      "grad_norm": 0.5682632327079773,
      "learning_rate": 0.0004899160165683913,
      "loss": 3.3548,
      "step": 5210
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.5771201848983765,
      "learning_rate": 0.0004898732434036243,
      "loss": 3.3421,
      "step": 5220
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.5123207569122314,
      "eval_runtime": 3.8648,
      "eval_samples_per_second": 4427.63,
      "eval_steps_per_second": 8.797,
      "step": 5220
    },
    {
      "epoch": 20.038314176245212,
      "grad_norm": 0.5587488412857056,
      "learning_rate": 0.0004898303815906121,
      "loss": 3.3258,
      "step": 5230
    },
    {
      "epoch": 20.07662835249042,
      "grad_norm": 0.5678009390830994,
      "learning_rate": 0.0004897874311451945,
      "loss": 3.3166,
      "step": 5240
    },
    {
      "epoch": 20.114942528735632,
      "grad_norm": 0.5507732033729553,
      "learning_rate": 0.0004897443920832445,
      "loss": 3.3076,
      "step": 5250
    },
    {
      "epoch": 20.153256704980844,
      "grad_norm": 0.548609733581543,
      "learning_rate": 0.0004897012644206678,
      "loss": 3.323,
      "step": 5260
    },
    {
      "epoch": 20.191570881226053,
      "grad_norm": 0.5481412410736084,
      "learning_rate": 0.0004896580481734027,
      "loss": 3.3201,
      "step": 5270
    },
    {
      "epoch": 20.229885057471265,
      "grad_norm": 0.5605740547180176,
      "learning_rate": 0.0004896147433574203,
      "loss": 3.3266,
      "step": 5280
    },
    {
      "epoch": 20.268199233716476,
      "grad_norm": 0.5723811984062195,
      "learning_rate": 0.0004895713499887246,
      "loss": 3.318,
      "step": 5290
    },
    {
      "epoch": 20.306513409961685,
      "grad_norm": 0.5302518606185913,
      "learning_rate": 0.000489527868083352,
      "loss": 3.3516,
      "step": 5300
    },
    {
      "epoch": 20.344827586206897,
      "grad_norm": 0.548180878162384,
      "learning_rate": 0.0004894842976573718,
      "loss": 3.3498,
      "step": 5310
    },
    {
      "epoch": 20.38314176245211,
      "grad_norm": 0.5416290163993835,
      "learning_rate": 0.0004894406387268862,
      "loss": 3.3364,
      "step": 5320
    },
    {
      "epoch": 20.421455938697317,
      "grad_norm": 0.5620133876800537,
      "learning_rate": 0.0004893968913080298,
      "loss": 3.3285,
      "step": 5330
    },
    {
      "epoch": 20.45977011494253,
      "grad_norm": 0.5361514091491699,
      "learning_rate": 0.0004893530554169699,
      "loss": 3.3284,
      "step": 5340
    },
    {
      "epoch": 20.49808429118774,
      "grad_norm": 0.5613905191421509,
      "learning_rate": 0.0004893091310699069,
      "loss": 3.3342,
      "step": 5350
    },
    {
      "epoch": 20.53639846743295,
      "grad_norm": 0.5491517782211304,
      "learning_rate": 0.0004892651182830735,
      "loss": 3.3348,
      "step": 5360
    },
    {
      "epoch": 20.57471264367816,
      "grad_norm": 0.5757691860198975,
      "learning_rate": 0.0004892210170727349,
      "loss": 3.3575,
      "step": 5370
    },
    {
      "epoch": 20.613026819923373,
      "grad_norm": 0.5590764880180359,
      "learning_rate": 0.0004891768274551897,
      "loss": 3.3143,
      "step": 5380
    },
    {
      "epoch": 20.65134099616858,
      "grad_norm": 0.580438494682312,
      "learning_rate": 0.0004891325494467684,
      "loss": 3.3488,
      "step": 5390
    },
    {
      "epoch": 20.689655172413794,
      "grad_norm": 0.5582266449928284,
      "learning_rate": 0.0004890881830638349,
      "loss": 3.3408,
      "step": 5400
    },
    {
      "epoch": 20.727969348659006,
      "grad_norm": 0.5473594069480896,
      "learning_rate": 0.0004890437283227849,
      "loss": 3.3439,
      "step": 5410
    },
    {
      "epoch": 20.766283524904214,
      "grad_norm": 0.5324251055717468,
      "learning_rate": 0.0004889991852400475,
      "loss": 3.3131,
      "step": 5420
    },
    {
      "epoch": 20.804597701149426,
      "grad_norm": 0.5840139985084534,
      "learning_rate": 0.0004889545538320842,
      "loss": 3.3235,
      "step": 5430
    },
    {
      "epoch": 20.842911877394634,
      "grad_norm": 0.560940682888031,
      "learning_rate": 0.000488909834115389,
      "loss": 3.3307,
      "step": 5440
    },
    {
      "epoch": 20.881226053639846,
      "grad_norm": 0.5546018481254578,
      "learning_rate": 0.0004888650261064887,
      "loss": 3.327,
      "step": 5450
    },
    {
      "epoch": 20.919540229885058,
      "grad_norm": 0.554355800151825,
      "learning_rate": 0.0004888201298219426,
      "loss": 3.3351,
      "step": 5460
    },
    {
      "epoch": 20.957854406130267,
      "grad_norm": 0.5220299363136292,
      "learning_rate": 0.0004887751452783428,
      "loss": 3.2921,
      "step": 5470
    },
    {
      "epoch": 20.99616858237548,
      "grad_norm": 0.5459181666374207,
      "learning_rate": 0.000488730072492314,
      "loss": 3.332,
      "step": 5480
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.5077308416366577,
      "eval_runtime": 4.147,
      "eval_samples_per_second": 4126.354,
      "eval_steps_per_second": 8.199,
      "step": 5481
    },
    {
      "epoch": 21.03448275862069,
      "grad_norm": 0.5316929817199707,
      "learning_rate": 0.0004886849114805131,
      "loss": 3.3001,
      "step": 5490
    },
    {
      "epoch": 21.0727969348659,
      "grad_norm": 0.5707224607467651,
      "learning_rate": 0.0004886396622596303,
      "loss": 3.2989,
      "step": 5500
    },
    {
      "epoch": 21.11111111111111,
      "grad_norm": 0.5366214513778687,
      "learning_rate": 0.0004885943248463878,
      "loss": 3.2934,
      "step": 5510
    },
    {
      "epoch": 21.149425287356323,
      "grad_norm": 0.557733952999115,
      "learning_rate": 0.0004885488992575406,
      "loss": 3.3242,
      "step": 5520
    },
    {
      "epoch": 21.18773946360153,
      "grad_norm": 0.5476649403572083,
      "learning_rate": 0.0004885033855098765,
      "loss": 3.3133,
      "step": 5530
    },
    {
      "epoch": 21.226053639846743,
      "grad_norm": 0.5566468238830566,
      "learning_rate": 0.0004884577836202157,
      "loss": 3.3036,
      "step": 5540
    },
    {
      "epoch": 21.264367816091955,
      "grad_norm": 0.5394571423530579,
      "learning_rate": 0.0004884120936054107,
      "loss": 3.3098,
      "step": 5550
    },
    {
      "epoch": 21.302681992337163,
      "grad_norm": 0.5440812110900879,
      "learning_rate": 0.000488366315482347,
      "loss": 3.3149,
      "step": 5560
    },
    {
      "epoch": 21.340996168582375,
      "grad_norm": 0.5292820930480957,
      "learning_rate": 0.0004883204492679426,
      "loss": 3.2966,
      "step": 5570
    },
    {
      "epoch": 21.379310344827587,
      "grad_norm": 0.5526414513587952,
      "learning_rate": 0.00048827449497914775,
      "loss": 3.3061,
      "step": 5580
    },
    {
      "epoch": 21.417624521072796,
      "grad_norm": 0.5536166429519653,
      "learning_rate": 0.00048822845263294556,
      "loss": 3.3181,
      "step": 5590
    },
    {
      "epoch": 21.455938697318008,
      "grad_norm": 0.552525520324707,
      "learning_rate": 0.00048818232224635155,
      "loss": 3.3088,
      "step": 5600
    },
    {
      "epoch": 21.49425287356322,
      "grad_norm": 0.5397613048553467,
      "learning_rate": 0.0004881361038364138,
      "loss": 3.3171,
      "step": 5610
    },
    {
      "epoch": 21.532567049808428,
      "grad_norm": 0.5476444363594055,
      "learning_rate": 0.00048808979742021296,
      "loss": 3.3147,
      "step": 5620
    },
    {
      "epoch": 21.57088122605364,
      "grad_norm": 0.5621108412742615,
      "learning_rate": 0.00048804340301486206,
      "loss": 3.3115,
      "step": 5630
    },
    {
      "epoch": 21.60919540229885,
      "grad_norm": 0.5504531264305115,
      "learning_rate": 0.0004879969206375069,
      "loss": 3.3053,
      "step": 5640
    },
    {
      "epoch": 21.64750957854406,
      "grad_norm": 0.5328163504600525,
      "learning_rate": 0.0004879503503053254,
      "loss": 3.3145,
      "step": 5650
    },
    {
      "epoch": 21.685823754789272,
      "grad_norm": 0.5754535794258118,
      "learning_rate": 0.0004879036920355285,
      "loss": 3.2749,
      "step": 5660
    },
    {
      "epoch": 21.724137931034484,
      "grad_norm": 0.5776776671409607,
      "learning_rate": 0.00048785694584535923,
      "loss": 3.2929,
      "step": 5670
    },
    {
      "epoch": 21.762452107279692,
      "grad_norm": 0.5484170913696289,
      "learning_rate": 0.00048781011175209324,
      "loss": 3.3077,
      "step": 5680
    },
    {
      "epoch": 21.800766283524904,
      "grad_norm": 0.5466414093971252,
      "learning_rate": 0.0004877631897730387,
      "loss": 3.3,
      "step": 5690
    },
    {
      "epoch": 21.839080459770116,
      "grad_norm": 0.5587424039840698,
      "learning_rate": 0.0004877161799255362,
      "loss": 3.3294,
      "step": 5700
    },
    {
      "epoch": 21.877394636015325,
      "grad_norm": 0.569959819316864,
      "learning_rate": 0.00048766908222695884,
      "loss": 3.3142,
      "step": 5710
    },
    {
      "epoch": 21.915708812260537,
      "grad_norm": 0.5469854474067688,
      "learning_rate": 0.00048762189669471223,
      "loss": 3.3224,
      "step": 5720
    },
    {
      "epoch": 21.95402298850575,
      "grad_norm": 0.5445138812065125,
      "learning_rate": 0.0004875746233462344,
      "loss": 3.3163,
      "step": 5730
    },
    {
      "epoch": 21.992337164750957,
      "grad_norm": 0.5507252216339111,
      "learning_rate": 0.00048752726219899567,
      "loss": 3.2869,
      "step": 5740
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.5008785724639893,
      "eval_runtime": 3.9723,
      "eval_samples_per_second": 4307.881,
      "eval_steps_per_second": 8.559,
      "step": 5742
    },
    {
      "epoch": 22.03065134099617,
      "grad_norm": 0.5689812302589417,
      "learning_rate": 0.00048747981327049915,
      "loss": 3.2873,
      "step": 5750
    },
    {
      "epoch": 22.06896551724138,
      "grad_norm": 0.5391882061958313,
      "learning_rate": 0.0004874322765782802,
      "loss": 3.2742,
      "step": 5760
    },
    {
      "epoch": 22.10727969348659,
      "grad_norm": 0.568391740322113,
      "learning_rate": 0.00048738465213990636,
      "loss": 3.311,
      "step": 5770
    },
    {
      "epoch": 22.1455938697318,
      "grad_norm": 0.5487828850746155,
      "learning_rate": 0.00048733693997297814,
      "loss": 3.2729,
      "step": 5780
    },
    {
      "epoch": 22.183908045977013,
      "grad_norm": 0.5412724018096924,
      "learning_rate": 0.0004872891400951281,
      "loss": 3.2777,
      "step": 5790
    },
    {
      "epoch": 22.22222222222222,
      "grad_norm": 0.5470783114433289,
      "learning_rate": 0.00048724125252402124,
      "loss": 3.2835,
      "step": 5800
    },
    {
      "epoch": 22.260536398467433,
      "grad_norm": 0.5703319311141968,
      "learning_rate": 0.00048719327727735507,
      "loss": 3.285,
      "step": 5810
    },
    {
      "epoch": 22.298850574712645,
      "grad_norm": 0.5605497360229492,
      "learning_rate": 0.00048714521437285955,
      "loss": 3.2818,
      "step": 5820
    },
    {
      "epoch": 22.337164750957854,
      "grad_norm": 0.5794405937194824,
      "learning_rate": 0.0004870970638282968,
      "loss": 3.3161,
      "step": 5830
    },
    {
      "epoch": 22.375478927203066,
      "grad_norm": 0.5764412879943848,
      "learning_rate": 0.00048704882566146147,
      "loss": 3.2978,
      "step": 5840
    },
    {
      "epoch": 22.413793103448278,
      "grad_norm": 0.57643723487854,
      "learning_rate": 0.0004870004998901807,
      "loss": 3.2791,
      "step": 5850
    },
    {
      "epoch": 22.452107279693486,
      "grad_norm": 0.5469898581504822,
      "learning_rate": 0.0004869520865323139,
      "loss": 3.2838,
      "step": 5860
    },
    {
      "epoch": 22.490421455938698,
      "grad_norm": 0.5335580706596375,
      "learning_rate": 0.00048690358560575276,
      "loss": 3.3057,
      "step": 5870
    },
    {
      "epoch": 22.52873563218391,
      "grad_norm": 0.5790258049964905,
      "learning_rate": 0.00048685499712842144,
      "loss": 3.2989,
      "step": 5880
    },
    {
      "epoch": 22.56704980842912,
      "grad_norm": 0.5362939238548279,
      "learning_rate": 0.0004868063211182765,
      "loss": 3.2825,
      "step": 5890
    },
    {
      "epoch": 22.60536398467433,
      "grad_norm": 0.5291183590888977,
      "learning_rate": 0.00048675755759330675,
      "loss": 3.2742,
      "step": 5900
    },
    {
      "epoch": 22.64367816091954,
      "grad_norm": 0.5370786786079407,
      "learning_rate": 0.0004867087065715334,
      "loss": 3.3001,
      "step": 5910
    },
    {
      "epoch": 22.68199233716475,
      "grad_norm": 0.5311012864112854,
      "learning_rate": 0.00048665976807100997,
      "loss": 3.2935,
      "step": 5920
    },
    {
      "epoch": 22.720306513409962,
      "grad_norm": 0.5581839680671692,
      "learning_rate": 0.0004866107421098223,
      "loss": 3.2926,
      "step": 5930
    },
    {
      "epoch": 22.75862068965517,
      "grad_norm": 0.5504910945892334,
      "learning_rate": 0.0004865616287060885,
      "loss": 3.3012,
      "step": 5940
    },
    {
      "epoch": 22.796934865900383,
      "grad_norm": 0.5357918739318848,
      "learning_rate": 0.00048651242787795917,
      "loss": 3.2898,
      "step": 5950
    },
    {
      "epoch": 22.835249042145595,
      "grad_norm": 0.5479482412338257,
      "learning_rate": 0.00048646313964361706,
      "loss": 3.3078,
      "step": 5960
    },
    {
      "epoch": 22.873563218390803,
      "grad_norm": 0.5403474569320679,
      "learning_rate": 0.0004864137640212772,
      "loss": 3.2893,
      "step": 5970
    },
    {
      "epoch": 22.911877394636015,
      "grad_norm": 0.5414004325866699,
      "learning_rate": 0.0004863643010291872,
      "loss": 3.2922,
      "step": 5980
    },
    {
      "epoch": 22.950191570881227,
      "grad_norm": 0.668015718460083,
      "learning_rate": 0.0004863147506856265,
      "loss": 3.2861,
      "step": 5990
    },
    {
      "epoch": 22.988505747126435,
      "grad_norm": 0.5517936944961548,
      "learning_rate": 0.00048626511300890724,
      "loss": 3.301,
      "step": 6000
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.497353196144104,
      "eval_runtime": 4.0922,
      "eval_samples_per_second": 4181.645,
      "eval_steps_per_second": 8.309,
      "step": 6003
    },
    {
      "epoch": 23.026819923371647,
      "grad_norm": 0.5403496026992798,
      "learning_rate": 0.0004862153880173735,
      "loss": 3.2565,
      "step": 6010
    },
    {
      "epoch": 23.06513409961686,
      "grad_norm": 0.5385605692863464,
      "learning_rate": 0.000486165575729402,
      "loss": 3.2771,
      "step": 6020
    },
    {
      "epoch": 23.103448275862068,
      "grad_norm": 0.5544288754463196,
      "learning_rate": 0.00048611567616340133,
      "loss": 3.2667,
      "step": 6030
    },
    {
      "epoch": 23.14176245210728,
      "grad_norm": 0.561278223991394,
      "learning_rate": 0.0004860656893378126,
      "loss": 3.2726,
      "step": 6040
    },
    {
      "epoch": 23.18007662835249,
      "grad_norm": 0.527280330657959,
      "learning_rate": 0.00048601561527110914,
      "loss": 3.2671,
      "step": 6050
    },
    {
      "epoch": 23.2183908045977,
      "grad_norm": 0.5324869751930237,
      "learning_rate": 0.00048596545398179637,
      "loss": 3.2694,
      "step": 6060
    },
    {
      "epoch": 23.256704980842912,
      "grad_norm": 0.5346227288246155,
      "learning_rate": 0.000485915205488412,
      "loss": 3.2611,
      "step": 6070
    },
    {
      "epoch": 23.295019157088124,
      "grad_norm": 0.5460575222969055,
      "learning_rate": 0.00048586486980952613,
      "loss": 3.274,
      "step": 6080
    },
    {
      "epoch": 23.333333333333332,
      "grad_norm": 0.5197834372520447,
      "learning_rate": 0.00048581444696374086,
      "loss": 3.2503,
      "step": 6090
    },
    {
      "epoch": 23.371647509578544,
      "grad_norm": 0.5317137241363525,
      "learning_rate": 0.00048576393696969075,
      "loss": 3.2841,
      "step": 6100
    },
    {
      "epoch": 23.409961685823756,
      "grad_norm": 0.5391823649406433,
      "learning_rate": 0.00048571333984604223,
      "loss": 3.2755,
      "step": 6110
    },
    {
      "epoch": 23.448275862068964,
      "grad_norm": 0.5278846025466919,
      "learning_rate": 0.00048566265561149417,
      "loss": 3.2699,
      "step": 6120
    },
    {
      "epoch": 23.486590038314176,
      "grad_norm": 0.5496212840080261,
      "learning_rate": 0.0004856118842847777,
      "loss": 3.2788,
      "step": 6130
    },
    {
      "epoch": 23.52490421455939,
      "grad_norm": 0.5339133739471436,
      "learning_rate": 0.0004855610258846559,
      "loss": 3.2678,
      "step": 6140
    },
    {
      "epoch": 23.563218390804597,
      "grad_norm": 0.5487635731697083,
      "learning_rate": 0.00048551008042992415,
      "loss": 3.2754,
      "step": 6150
    },
    {
      "epoch": 23.60153256704981,
      "grad_norm": 0.5527021288871765,
      "learning_rate": 0.0004854590479394101,
      "loss": 3.2437,
      "step": 6160
    },
    {
      "epoch": 23.63984674329502,
      "grad_norm": 0.5281745195388794,
      "learning_rate": 0.0004854079284319733,
      "loss": 3.2759,
      "step": 6170
    },
    {
      "epoch": 23.67816091954023,
      "grad_norm": 0.5218383073806763,
      "learning_rate": 0.0004853567219265058,
      "loss": 3.2677,
      "step": 6180
    },
    {
      "epoch": 23.71647509578544,
      "grad_norm": 0.5548424124717712,
      "learning_rate": 0.00048530542844193146,
      "loss": 3.2571,
      "step": 6190
    },
    {
      "epoch": 23.754789272030653,
      "grad_norm": 0.5533859729766846,
      "learning_rate": 0.0004852540479972066,
      "loss": 3.2936,
      "step": 6200
    },
    {
      "epoch": 23.79310344827586,
      "grad_norm": 0.534477710723877,
      "learning_rate": 0.0004852025806113194,
      "loss": 3.2352,
      "step": 6210
    },
    {
      "epoch": 23.831417624521073,
      "grad_norm": 0.5345239639282227,
      "learning_rate": 0.00048515102630329035,
      "loss": 3.2903,
      "step": 6220
    },
    {
      "epoch": 23.869731800766285,
      "grad_norm": 0.5470053553581238,
      "learning_rate": 0.000485099385092172,
      "loss": 3.2789,
      "step": 6230
    },
    {
      "epoch": 23.908045977011493,
      "grad_norm": 0.5402540564537048,
      "learning_rate": 0.000485047656997049,
      "loss": 3.2872,
      "step": 6240
    },
    {
      "epoch": 23.946360153256705,
      "grad_norm": 0.5300097465515137,
      "learning_rate": 0.00048499584203703824,
      "loss": 3.2666,
      "step": 6250
    },
    {
      "epoch": 23.984674329501917,
      "grad_norm": 0.5294559001922607,
      "learning_rate": 0.00048494394023128844,
      "loss": 3.2639,
      "step": 6260
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.493457317352295,
      "eval_runtime": 4.0441,
      "eval_samples_per_second": 4231.374,
      "eval_steps_per_second": 8.407,
      "step": 6264
    },
    {
      "epoch": 24.022988505747126,
      "grad_norm": 0.5269438028335571,
      "learning_rate": 0.0004848919515989807,
      "loss": 3.2529,
      "step": 6270
    },
    {
      "epoch": 24.061302681992338,
      "grad_norm": 0.5509729981422424,
      "learning_rate": 0.0004848398761593281,
      "loss": 3.2353,
      "step": 6280
    },
    {
      "epoch": 24.09961685823755,
      "grad_norm": 0.5189269781112671,
      "learning_rate": 0.00048478771393157573,
      "loss": 3.2349,
      "step": 6290
    },
    {
      "epoch": 24.137931034482758,
      "grad_norm": 0.5617401599884033,
      "learning_rate": 0.00048473546493500074,
      "loss": 3.248,
      "step": 6300
    },
    {
      "epoch": 24.17624521072797,
      "grad_norm": 0.538352370262146,
      "learning_rate": 0.00048468312918891263,
      "loss": 3.2615,
      "step": 6310
    },
    {
      "epoch": 24.21455938697318,
      "grad_norm": 0.5542235374450684,
      "learning_rate": 0.00048463070671265254,
      "loss": 3.255,
      "step": 6320
    },
    {
      "epoch": 24.25287356321839,
      "grad_norm": 0.5097853541374207,
      "learning_rate": 0.00048457819752559397,
      "loss": 3.2471,
      "step": 6330
    },
    {
      "epoch": 24.291187739463602,
      "grad_norm": 0.5222174525260925,
      "learning_rate": 0.0004845256016471424,
      "loss": 3.2562,
      "step": 6340
    },
    {
      "epoch": 24.32950191570881,
      "grad_norm": 0.5298014283180237,
      "learning_rate": 0.0004844729190967352,
      "loss": 3.2465,
      "step": 6350
    },
    {
      "epoch": 24.367816091954023,
      "grad_norm": 0.5537000894546509,
      "learning_rate": 0.00048442014989384197,
      "loss": 3.2599,
      "step": 6360
    },
    {
      "epoch": 24.406130268199234,
      "grad_norm": 0.5291643738746643,
      "learning_rate": 0.0004843672940579643,
      "loss": 3.2471,
      "step": 6370
    },
    {
      "epoch": 24.444444444444443,
      "grad_norm": 0.5338597893714905,
      "learning_rate": 0.00048431435160863556,
      "loss": 3.2246,
      "step": 6380
    },
    {
      "epoch": 24.482758620689655,
      "grad_norm": 0.5319778919219971,
      "learning_rate": 0.0004842613225654215,
      "loss": 3.2434,
      "step": 6390
    },
    {
      "epoch": 24.521072796934867,
      "grad_norm": 0.5403582453727722,
      "learning_rate": 0.00048420820694791965,
      "loss": 3.2625,
      "step": 6400
    },
    {
      "epoch": 24.559386973180075,
      "grad_norm": 0.5276902318000793,
      "learning_rate": 0.0004841550047757595,
      "loss": 3.2689,
      "step": 6410
    },
    {
      "epoch": 24.597701149425287,
      "grad_norm": 0.5309352278709412,
      "learning_rate": 0.0004841017160686026,
      "loss": 3.2752,
      "step": 6420
    },
    {
      "epoch": 24.6360153256705,
      "grad_norm": 0.524386465549469,
      "learning_rate": 0.00048404834084614254,
      "loss": 3.2566,
      "step": 6430
    },
    {
      "epoch": 24.674329501915707,
      "grad_norm": 0.543977677822113,
      "learning_rate": 0.00048399487912810475,
      "loss": 3.2642,
      "step": 6440
    },
    {
      "epoch": 24.71264367816092,
      "grad_norm": 0.5500335097312927,
      "learning_rate": 0.00048394133093424674,
      "loss": 3.2593,
      "step": 6450
    },
    {
      "epoch": 24.75095785440613,
      "grad_norm": 0.5585136413574219,
      "learning_rate": 0.00048388769628435803,
      "loss": 3.2874,
      "step": 6460
    },
    {
      "epoch": 24.78927203065134,
      "grad_norm": 0.5156406760215759,
      "learning_rate": 0.0004838339751982598,
      "loss": 3.2665,
      "step": 6470
    },
    {
      "epoch": 24.82758620689655,
      "grad_norm": 0.52229905128479,
      "learning_rate": 0.00048378016769580544,
      "loss": 3.2423,
      "step": 6480
    },
    {
      "epoch": 24.865900383141764,
      "grad_norm": 0.5717985033988953,
      "learning_rate": 0.00048372627379688027,
      "loss": 3.2631,
      "step": 6490
    },
    {
      "epoch": 24.904214559386972,
      "grad_norm": 0.5540456175804138,
      "learning_rate": 0.0004836722935214015,
      "loss": 3.2681,
      "step": 6500
    },
    {
      "epoch": 24.942528735632184,
      "grad_norm": 0.5317384004592896,
      "learning_rate": 0.0004836182268893181,
      "loss": 3.2429,
      "step": 6510
    },
    {
      "epoch": 24.980842911877396,
      "grad_norm": 0.5278621912002563,
      "learning_rate": 0.0004835640739206112,
      "loss": 3.2484,
      "step": 6520
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.4906078577041626,
      "eval_runtime": 3.853,
      "eval_samples_per_second": 4441.175,
      "eval_steps_per_second": 8.824,
      "step": 6525
    },
    {
      "epoch": 25.019157088122604,
      "grad_norm": 0.5534076690673828,
      "learning_rate": 0.0004835098346352936,
      "loss": 3.2509,
      "step": 6530
    },
    {
      "epoch": 25.057471264367816,
      "grad_norm": 0.5419762134552002,
      "learning_rate": 0.0004834555090534103,
      "loss": 3.211,
      "step": 6540
    },
    {
      "epoch": 25.095785440613028,
      "grad_norm": 0.520470380783081,
      "learning_rate": 0.00048340109719503794,
      "loss": 3.2314,
      "step": 6550
    },
    {
      "epoch": 25.134099616858236,
      "grad_norm": 0.5568398237228394,
      "learning_rate": 0.0004833465990802851,
      "loss": 3.2638,
      "step": 6560
    },
    {
      "epoch": 25.17241379310345,
      "grad_norm": 0.5610601902008057,
      "learning_rate": 0.00048329201472929224,
      "loss": 3.223,
      "step": 6570
    },
    {
      "epoch": 25.21072796934866,
      "grad_norm": 0.5398918986320496,
      "learning_rate": 0.0004832373441622318,
      "loss": 3.2163,
      "step": 6580
    },
    {
      "epoch": 25.24904214559387,
      "grad_norm": 0.5323063135147095,
      "learning_rate": 0.0004831825873993079,
      "loss": 3.2282,
      "step": 6590
    },
    {
      "epoch": 25.28735632183908,
      "grad_norm": 0.5389547944068909,
      "learning_rate": 0.0004831277444607566,
      "loss": 3.2451,
      "step": 6600
    },
    {
      "epoch": 25.325670498084293,
      "grad_norm": 0.5356437563896179,
      "learning_rate": 0.0004830728153668459,
      "loss": 3.2377,
      "step": 6610
    },
    {
      "epoch": 25.3639846743295,
      "grad_norm": 0.5403830409049988,
      "learning_rate": 0.0004830178001378755,
      "loss": 3.2298,
      "step": 6620
    },
    {
      "epoch": 25.402298850574713,
      "grad_norm": 0.5360434651374817,
      "learning_rate": 0.000482962698794177,
      "loss": 3.2338,
      "step": 6630
    },
    {
      "epoch": 25.440613026819925,
      "grad_norm": 0.5309386849403381,
      "learning_rate": 0.0004829075113561139,
      "loss": 3.2362,
      "step": 6640
    },
    {
      "epoch": 25.478927203065133,
      "grad_norm": 0.5418775677680969,
      "learning_rate": 0.0004828522378440812,
      "loss": 3.2508,
      "step": 6650
    },
    {
      "epoch": 25.517241379310345,
      "grad_norm": 0.5286353230476379,
      "learning_rate": 0.00048279687827850615,
      "loss": 3.224,
      "step": 6660
    },
    {
      "epoch": 25.555555555555557,
      "grad_norm": 0.5210367441177368,
      "learning_rate": 0.0004827414326798475,
      "loss": 3.215,
      "step": 6670
    },
    {
      "epoch": 25.593869731800766,
      "grad_norm": 0.516306459903717,
      "learning_rate": 0.0004826859010685959,
      "loss": 3.2571,
      "step": 6680
    },
    {
      "epoch": 25.632183908045977,
      "grad_norm": 0.5234557390213013,
      "learning_rate": 0.0004826302834652737,
      "loss": 3.2576,
      "step": 6690
    },
    {
      "epoch": 25.67049808429119,
      "grad_norm": 0.5333683490753174,
      "learning_rate": 0.0004825745798904353,
      "loss": 3.2538,
      "step": 6700
    },
    {
      "epoch": 25.708812260536398,
      "grad_norm": 0.5199098587036133,
      "learning_rate": 0.0004825187903646665,
      "loss": 3.2158,
      "step": 6710
    },
    {
      "epoch": 25.74712643678161,
      "grad_norm": 0.5830223560333252,
      "learning_rate": 0.00048246291490858505,
      "loss": 3.259,
      "step": 6720
    },
    {
      "epoch": 25.78544061302682,
      "grad_norm": 0.5319412350654602,
      "learning_rate": 0.0004824069535428405,
      "loss": 3.2561,
      "step": 6730
    },
    {
      "epoch": 25.82375478927203,
      "grad_norm": 0.5225410461425781,
      "learning_rate": 0.00048235090628811404,
      "loss": 3.2203,
      "step": 6740
    },
    {
      "epoch": 25.862068965517242,
      "grad_norm": 0.5198761820793152,
      "learning_rate": 0.0004822947731651187,
      "loss": 3.2611,
      "step": 6750
    },
    {
      "epoch": 25.900383141762454,
      "grad_norm": 0.519392192363739,
      "learning_rate": 0.00048223855419459924,
      "loss": 3.2339,
      "step": 6760
    },
    {
      "epoch": 25.938697318007662,
      "grad_norm": 0.5322317481040955,
      "learning_rate": 0.000482182249397332,
      "loss": 3.2305,
      "step": 6770
    },
    {
      "epoch": 25.977011494252874,
      "grad_norm": 0.5608459115028381,
      "learning_rate": 0.0004821258587941252,
      "loss": 3.2558,
      "step": 6780
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.4851415157318115,
      "eval_runtime": 4.0312,
      "eval_samples_per_second": 4244.861,
      "eval_steps_per_second": 8.434,
      "step": 6786
    },
    {
      "epoch": 26.015325670498083,
      "grad_norm": 0.5190927386283875,
      "learning_rate": 0.00048206938240581866,
      "loss": 3.2397,
      "step": 6790
    },
    {
      "epoch": 26.053639846743295,
      "grad_norm": 0.543671727180481,
      "learning_rate": 0.00048201282025328406,
      "loss": 3.222,
      "step": 6800
    },
    {
      "epoch": 26.091954022988507,
      "grad_norm": 0.5413016080856323,
      "learning_rate": 0.00048195617235742463,
      "loss": 3.228,
      "step": 6810
    },
    {
      "epoch": 26.130268199233715,
      "grad_norm": 0.5219035744667053,
      "learning_rate": 0.0004818994387391752,
      "loss": 3.2274,
      "step": 6820
    },
    {
      "epoch": 26.168582375478927,
      "grad_norm": 0.5412670373916626,
      "learning_rate": 0.0004818426194195026,
      "loss": 3.1906,
      "step": 6830
    },
    {
      "epoch": 26.20689655172414,
      "grad_norm": 0.501931369304657,
      "learning_rate": 0.00048178571441940513,
      "loss": 3.1899,
      "step": 6840
    },
    {
      "epoch": 26.245210727969347,
      "grad_norm": 0.5493077039718628,
      "learning_rate": 0.00048172872375991263,
      "loss": 3.2294,
      "step": 6850
    },
    {
      "epoch": 26.28352490421456,
      "grad_norm": 0.526509165763855,
      "learning_rate": 0.00048167164746208674,
      "loss": 3.2132,
      "step": 6860
    },
    {
      "epoch": 26.32183908045977,
      "grad_norm": 0.5187579393386841,
      "learning_rate": 0.00048161448554702083,
      "loss": 3.2381,
      "step": 6870
    },
    {
      "epoch": 26.36015325670498,
      "grad_norm": 0.5418973565101624,
      "learning_rate": 0.0004815572380358398,
      "loss": 3.2508,
      "step": 6880
    },
    {
      "epoch": 26.39846743295019,
      "grad_norm": 0.5267759561538696,
      "learning_rate": 0.0004814999049497003,
      "loss": 3.2196,
      "step": 6890
    },
    {
      "epoch": 26.436781609195403,
      "grad_norm": 0.5252622365951538,
      "learning_rate": 0.00048144248630979024,
      "loss": 3.232,
      "step": 6900
    },
    {
      "epoch": 26.47509578544061,
      "grad_norm": 0.5239047408103943,
      "learning_rate": 0.0004813849821373297,
      "loss": 3.2325,
      "step": 6910
    },
    {
      "epoch": 26.513409961685824,
      "grad_norm": 0.5458512902259827,
      "learning_rate": 0.00048132739245356995,
      "loss": 3.2208,
      "step": 6920
    },
    {
      "epoch": 26.551724137931036,
      "grad_norm": 0.5099655389785767,
      "learning_rate": 0.000481269717279794,
      "loss": 3.2222,
      "step": 6930
    },
    {
      "epoch": 26.590038314176244,
      "grad_norm": 0.5337017178535461,
      "learning_rate": 0.00048121195663731645,
      "loss": 3.2211,
      "step": 6940
    },
    {
      "epoch": 26.628352490421456,
      "grad_norm": 0.5530619621276855,
      "learning_rate": 0.00048115411054748357,
      "loss": 3.2212,
      "step": 6950
    },
    {
      "epoch": 26.666666666666668,
      "grad_norm": 0.5198140144348145,
      "learning_rate": 0.00048109617903167303,
      "loss": 3.2136,
      "step": 6960
    },
    {
      "epoch": 26.704980842911876,
      "grad_norm": 0.5388604998588562,
      "learning_rate": 0.0004810381621112942,
      "loss": 3.2144,
      "step": 6970
    },
    {
      "epoch": 26.743295019157088,
      "grad_norm": 0.5390354990959167,
      "learning_rate": 0.00048098005980778805,
      "loss": 3.2181,
      "step": 6980
    },
    {
      "epoch": 26.7816091954023,
      "grad_norm": 0.5200040340423584,
      "learning_rate": 0.0004809218721426269,
      "loss": 3.2312,
      "step": 6990
    },
    {
      "epoch": 26.81992337164751,
      "grad_norm": 0.526793897151947,
      "learning_rate": 0.0004808635991373148,
      "loss": 3.2346,
      "step": 7000
    },
    {
      "epoch": 26.85823754789272,
      "grad_norm": 0.5303137898445129,
      "learning_rate": 0.0004808052408133874,
      "loss": 3.2236,
      "step": 7010
    },
    {
      "epoch": 26.896551724137932,
      "grad_norm": 0.5116649270057678,
      "learning_rate": 0.00048074679719241163,
      "loss": 3.216,
      "step": 7020
    },
    {
      "epoch": 26.93486590038314,
      "grad_norm": 0.5136651396751404,
      "learning_rate": 0.0004806882682959862,
      "loss": 3.2153,
      "step": 7030
    },
    {
      "epoch": 26.973180076628353,
      "grad_norm": 0.5410695672035217,
      "learning_rate": 0.0004806296541457411,
      "loss": 3.2456,
      "step": 7040
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.4805923700332642,
      "eval_runtime": 4.139,
      "eval_samples_per_second": 4134.303,
      "eval_steps_per_second": 8.214,
      "step": 7047
    },
    {
      "epoch": 27.011494252873565,
      "grad_norm": 0.5176010131835938,
      "learning_rate": 0.000480570954763338,
      "loss": 3.2033,
      "step": 7050
    },
    {
      "epoch": 27.049808429118773,
      "grad_norm": 0.49581506848335266,
      "learning_rate": 0.00048051217017047,
      "loss": 3.1986,
      "step": 7060
    },
    {
      "epoch": 27.088122605363985,
      "grad_norm": 0.5152580142021179,
      "learning_rate": 0.0004804533003888617,
      "loss": 3.2154,
      "step": 7070
    },
    {
      "epoch": 27.126436781609197,
      "grad_norm": 0.5344078540802002,
      "learning_rate": 0.00048039434544026914,
      "loss": 3.2074,
      "step": 7080
    },
    {
      "epoch": 27.164750957854405,
      "grad_norm": 0.5207248330116272,
      "learning_rate": 0.00048033530534648006,
      "loss": 3.202,
      "step": 7090
    },
    {
      "epoch": 27.203065134099617,
      "grad_norm": 0.5264805555343628,
      "learning_rate": 0.0004802761801293133,
      "loss": 3.2229,
      "step": 7100
    },
    {
      "epoch": 27.24137931034483,
      "grad_norm": 0.5143694877624512,
      "learning_rate": 0.00048021696981061945,
      "loss": 3.2001,
      "step": 7110
    },
    {
      "epoch": 27.279693486590038,
      "grad_norm": 0.5061075687408447,
      "learning_rate": 0.0004801576744122804,
      "loss": 3.2035,
      "step": 7120
    },
    {
      "epoch": 27.31800766283525,
      "grad_norm": 0.5458139777183533,
      "learning_rate": 0.0004800982939562095,
      "loss": 3.2122,
      "step": 7130
    },
    {
      "epoch": 27.35632183908046,
      "grad_norm": 0.5068297982215881,
      "learning_rate": 0.0004800388284643516,
      "loss": 3.2076,
      "step": 7140
    },
    {
      "epoch": 27.39463601532567,
      "grad_norm": 0.5204288363456726,
      "learning_rate": 0.0004799792779586829,
      "loss": 3.2106,
      "step": 7150
    },
    {
      "epoch": 27.43295019157088,
      "grad_norm": 0.6580526232719421,
      "learning_rate": 0.00047991964246121117,
      "loss": 3.2166,
      "step": 7160
    },
    {
      "epoch": 27.47126436781609,
      "grad_norm": 0.570205569267273,
      "learning_rate": 0.0004798599219939753,
      "loss": 3.2127,
      "step": 7170
    },
    {
      "epoch": 27.509578544061302,
      "grad_norm": 0.5210220217704773,
      "learning_rate": 0.000479800116579046,
      "loss": 3.1883,
      "step": 7180
    },
    {
      "epoch": 27.547892720306514,
      "grad_norm": 0.5193472504615784,
      "learning_rate": 0.00047974022623852486,
      "loss": 3.2545,
      "step": 7190
    },
    {
      "epoch": 27.586206896551722,
      "grad_norm": 0.5145193934440613,
      "learning_rate": 0.00047968025099454526,
      "loss": 3.2073,
      "step": 7200
    },
    {
      "epoch": 27.624521072796934,
      "grad_norm": 0.5206037163734436,
      "learning_rate": 0.0004796201908692718,
      "loss": 3.2182,
      "step": 7210
    },
    {
      "epoch": 27.662835249042146,
      "grad_norm": 0.5342309474945068,
      "learning_rate": 0.00047956004588490056,
      "loss": 3.2311,
      "step": 7220
    },
    {
      "epoch": 27.701149425287355,
      "grad_norm": 0.5314507484436035,
      "learning_rate": 0.0004794998160636588,
      "loss": 3.1923,
      "step": 7230
    },
    {
      "epoch": 27.739463601532567,
      "grad_norm": 0.5119596719741821,
      "learning_rate": 0.00047943950142780516,
      "loss": 3.2123,
      "step": 7240
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 0.5290297269821167,
      "learning_rate": 0.00047937910199962987,
      "loss": 3.2088,
      "step": 7250
    },
    {
      "epoch": 27.816091954022987,
      "grad_norm": 0.5260677337646484,
      "learning_rate": 0.0004793186178014542,
      "loss": 3.2144,
      "step": 7260
    },
    {
      "epoch": 27.8544061302682,
      "grad_norm": 0.5312447547912598,
      "learning_rate": 0.00047925804885563084,
      "loss": 3.1991,
      "step": 7270
    },
    {
      "epoch": 27.89272030651341,
      "grad_norm": 0.49729830026626587,
      "learning_rate": 0.00047919739518454394,
      "loss": 3.208,
      "step": 7280
    },
    {
      "epoch": 27.93103448275862,
      "grad_norm": 0.5191022753715515,
      "learning_rate": 0.0004791366568106087,
      "loss": 3.2,
      "step": 7290
    },
    {
      "epoch": 27.96934865900383,
      "grad_norm": 0.5009288191795349,
      "learning_rate": 0.00047907583375627195,
      "loss": 3.1952,
      "step": 7300
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.4794906377792358,
      "eval_runtime": 4.1678,
      "eval_samples_per_second": 4105.769,
      "eval_steps_per_second": 8.158,
      "step": 7308
    },
    {
      "epoch": 28.007662835249043,
      "grad_norm": 0.5378714799880981,
      "learning_rate": 0.00047901492604401143,
      "loss": 3.1971,
      "step": 7310
    },
    {
      "epoch": 28.04597701149425,
      "grad_norm": 0.5402839779853821,
      "learning_rate": 0.00047895393369633645,
      "loss": 3.1716,
      "step": 7320
    },
    {
      "epoch": 28.084291187739463,
      "grad_norm": 0.5406979918479919,
      "learning_rate": 0.00047889285673578764,
      "loss": 3.1986,
      "step": 7330
    },
    {
      "epoch": 28.122605363984675,
      "grad_norm": 0.5310227274894714,
      "learning_rate": 0.00047883169518493656,
      "loss": 3.1908,
      "step": 7340
    },
    {
      "epoch": 28.160919540229884,
      "grad_norm": 0.5220707654953003,
      "learning_rate": 0.00047877044906638645,
      "loss": 3.1866,
      "step": 7350
    },
    {
      "epoch": 28.199233716475096,
      "grad_norm": 0.5400604009628296,
      "learning_rate": 0.00047870911840277143,
      "loss": 3.1943,
      "step": 7360
    },
    {
      "epoch": 28.237547892720308,
      "grad_norm": 0.5185056924819946,
      "learning_rate": 0.0004786477032167571,
      "loss": 3.1929,
      "step": 7370
    },
    {
      "epoch": 28.275862068965516,
      "grad_norm": 0.5338171124458313,
      "learning_rate": 0.0004785862035310403,
      "loss": 3.1667,
      "step": 7380
    },
    {
      "epoch": 28.314176245210728,
      "grad_norm": 0.5221313238143921,
      "learning_rate": 0.000478524619368349,
      "loss": 3.1942,
      "step": 7390
    },
    {
      "epoch": 28.35249042145594,
      "grad_norm": 0.5417658686637878,
      "learning_rate": 0.0004784629507514424,
      "loss": 3.2023,
      "step": 7400
    },
    {
      "epoch": 28.39080459770115,
      "grad_norm": 0.5253968238830566,
      "learning_rate": 0.0004784011977031109,
      "loss": 3.1939,
      "step": 7410
    },
    {
      "epoch": 28.42911877394636,
      "grad_norm": 0.4885517358779907,
      "learning_rate": 0.0004783393602461762,
      "loss": 3.1728,
      "step": 7420
    },
    {
      "epoch": 28.467432950191572,
      "grad_norm": 0.5228508114814758,
      "learning_rate": 0.000478277438403491,
      "loss": 3.1935,
      "step": 7430
    },
    {
      "epoch": 28.50574712643678,
      "grad_norm": 0.5331270694732666,
      "learning_rate": 0.0004782154321979395,
      "loss": 3.1855,
      "step": 7440
    },
    {
      "epoch": 28.544061302681992,
      "grad_norm": 0.5204445123672485,
      "learning_rate": 0.00047815334165243674,
      "loss": 3.1903,
      "step": 7450
    },
    {
      "epoch": 28.582375478927204,
      "grad_norm": 0.526971161365509,
      "learning_rate": 0.0004780911667899292,
      "loss": 3.2098,
      "step": 7460
    },
    {
      "epoch": 28.620689655172413,
      "grad_norm": 0.5098035931587219,
      "learning_rate": 0.00047802890763339436,
      "loss": 3.1985,
      "step": 7470
    },
    {
      "epoch": 28.659003831417625,
      "grad_norm": 0.530841052532196,
      "learning_rate": 0.00047796656420584076,
      "loss": 3.1874,
      "step": 7480
    },
    {
      "epoch": 28.697318007662837,
      "grad_norm": 0.5077205300331116,
      "learning_rate": 0.0004779041365303084,
      "loss": 3.2008,
      "step": 7490
    },
    {
      "epoch": 28.735632183908045,
      "grad_norm": 0.503864049911499,
      "learning_rate": 0.0004778416246298682,
      "loss": 3.2001,
      "step": 7500
    },
    {
      "epoch": 28.773946360153257,
      "grad_norm": 0.5147016644477844,
      "learning_rate": 0.0004777790285276221,
      "loss": 3.1831,
      "step": 7510
    },
    {
      "epoch": 28.81226053639847,
      "grad_norm": 0.5220921635627747,
      "learning_rate": 0.0004777163482467035,
      "loss": 3.2154,
      "step": 7520
    },
    {
      "epoch": 28.850574712643677,
      "grad_norm": 0.5371365547180176,
      "learning_rate": 0.0004776535838102765,
      "loss": 3.186,
      "step": 7530
    },
    {
      "epoch": 28.88888888888889,
      "grad_norm": 0.5162036418914795,
      "learning_rate": 0.00047759073524153667,
      "loss": 3.2174,
      "step": 7540
    },
    {
      "epoch": 28.9272030651341,
      "grad_norm": 0.5278014540672302,
      "learning_rate": 0.0004775278025637104,
      "loss": 3.2002,
      "step": 7550
    },
    {
      "epoch": 28.96551724137931,
      "grad_norm": 0.5047954320907593,
      "learning_rate": 0.0004774647858000554,
      "loss": 3.209,
      "step": 7560
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.4758591651916504,
      "eval_runtime": 4.1905,
      "eval_samples_per_second": 4083.514,
      "eval_steps_per_second": 8.114,
      "step": 7569
    },
    {
      "epoch": 29.00383141762452,
      "grad_norm": 0.49368640780448914,
      "learning_rate": 0.00047740168497386015,
      "loss": 3.1972,
      "step": 7570
    },
    {
      "epoch": 29.042145593869733,
      "grad_norm": 0.5030303597450256,
      "learning_rate": 0.00047733850010844446,
      "loss": 3.1783,
      "step": 7580
    },
    {
      "epoch": 29.080459770114942,
      "grad_norm": 0.5109004974365234,
      "learning_rate": 0.00047727523122715915,
      "loss": 3.1941,
      "step": 7590
    },
    {
      "epoch": 29.118773946360154,
      "grad_norm": 0.5168946981430054,
      "learning_rate": 0.000477211878353386,
      "loss": 3.1714,
      "step": 7600
    },
    {
      "epoch": 29.157088122605366,
      "grad_norm": 0.5253221988677979,
      "learning_rate": 0.0004771484415105379,
      "loss": 3.1969,
      "step": 7610
    },
    {
      "epoch": 29.195402298850574,
      "grad_norm": 0.5210526585578918,
      "learning_rate": 0.0004770849207220588,
      "loss": 3.1937,
      "step": 7620
    },
    {
      "epoch": 29.233716475095786,
      "grad_norm": 0.49623051285743713,
      "learning_rate": 0.00047702131601142346,
      "loss": 3.1799,
      "step": 7630
    },
    {
      "epoch": 29.272030651340994,
      "grad_norm": 0.5319982767105103,
      "learning_rate": 0.00047695762740213797,
      "loss": 3.187,
      "step": 7640
    },
    {
      "epoch": 29.310344827586206,
      "grad_norm": 0.52511066198349,
      "learning_rate": 0.0004768938549177393,
      "loss": 3.1774,
      "step": 7650
    },
    {
      "epoch": 29.34865900383142,
      "grad_norm": 0.5296831130981445,
      "learning_rate": 0.0004768299985817952,
      "loss": 3.1767,
      "step": 7660
    },
    {
      "epoch": 29.386973180076627,
      "grad_norm": 0.5244765877723694,
      "learning_rate": 0.0004767660584179049,
      "loss": 3.1745,
      "step": 7670
    },
    {
      "epoch": 29.42528735632184,
      "grad_norm": 0.5396891832351685,
      "learning_rate": 0.000476702034449698,
      "loss": 3.16,
      "step": 7680
    },
    {
      "epoch": 29.46360153256705,
      "grad_norm": 0.5057008266448975,
      "learning_rate": 0.00047663792670083556,
      "loss": 3.1871,
      "step": 7690
    },
    {
      "epoch": 29.50191570881226,
      "grad_norm": 0.5413962602615356,
      "learning_rate": 0.0004765737351950094,
      "loss": 3.1696,
      "step": 7700
    },
    {
      "epoch": 29.54022988505747,
      "grad_norm": 0.5313303470611572,
      "learning_rate": 0.0004765094599559423,
      "loss": 3.1893,
      "step": 7710
    },
    {
      "epoch": 29.578544061302683,
      "grad_norm": 0.5149130821228027,
      "learning_rate": 0.00047644510100738804,
      "loss": 3.185,
      "step": 7720
    },
    {
      "epoch": 29.61685823754789,
      "grad_norm": 0.5326334834098816,
      "learning_rate": 0.0004763806583731312,
      "loss": 3.1773,
      "step": 7730
    },
    {
      "epoch": 29.655172413793103,
      "grad_norm": 0.5165226459503174,
      "learning_rate": 0.00047631613207698743,
      "loss": 3.1894,
      "step": 7740
    },
    {
      "epoch": 29.693486590038315,
      "grad_norm": 0.5421746969223022,
      "learning_rate": 0.0004762515221428033,
      "loss": 3.1951,
      "step": 7750
    },
    {
      "epoch": 29.731800766283524,
      "grad_norm": 0.5132374167442322,
      "learning_rate": 0.00047618682859445626,
      "loss": 3.1888,
      "step": 7760
    },
    {
      "epoch": 29.770114942528735,
      "grad_norm": 0.4993569850921631,
      "learning_rate": 0.0004761220514558546,
      "loss": 3.1786,
      "step": 7770
    },
    {
      "epoch": 29.808429118773947,
      "grad_norm": 0.5173161625862122,
      "learning_rate": 0.00047605719075093745,
      "loss": 3.196,
      "step": 7780
    },
    {
      "epoch": 29.846743295019156,
      "grad_norm": 0.5213356614112854,
      "learning_rate": 0.0004759922465036751,
      "loss": 3.1757,
      "step": 7790
    },
    {
      "epoch": 29.885057471264368,
      "grad_norm": 0.5143013596534729,
      "learning_rate": 0.0004759272187380684,
      "loss": 3.1843,
      "step": 7800
    },
    {
      "epoch": 29.92337164750958,
      "grad_norm": 0.511995792388916,
      "learning_rate": 0.0004758621074781493,
      "loss": 3.1955,
      "step": 7810
    },
    {
      "epoch": 29.961685823754788,
      "grad_norm": 0.49714022874832153,
      "learning_rate": 0.00047579691274798045,
      "loss": 3.1851,
      "step": 7820
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.5172138214111328,
      "learning_rate": 0.0004757316345716554,
      "loss": 3.1703,
      "step": 7830
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.475649356842041,
      "eval_runtime": 4.1426,
      "eval_samples_per_second": 4130.746,
      "eval_steps_per_second": 8.207,
      "step": 7830
    },
    {
      "epoch": 30.038314176245212,
      "grad_norm": 0.4870019555091858,
      "learning_rate": 0.00047566627297329845,
      "loss": 3.1725,
      "step": 7840
    },
    {
      "epoch": 30.07662835249042,
      "grad_norm": 0.5276582837104797,
      "learning_rate": 0.00047560082797706496,
      "loss": 3.1773,
      "step": 7850
    },
    {
      "epoch": 30.114942528735632,
      "grad_norm": 0.5222703814506531,
      "learning_rate": 0.00047553529960714093,
      "loss": 3.1684,
      "step": 7860
    },
    {
      "epoch": 30.153256704980844,
      "grad_norm": 0.5179457068443298,
      "learning_rate": 0.00047546968788774314,
      "loss": 3.1594,
      "step": 7870
    },
    {
      "epoch": 30.191570881226053,
      "grad_norm": 0.5307521224021912,
      "learning_rate": 0.0004754039928431194,
      "loss": 3.17,
      "step": 7880
    },
    {
      "epoch": 30.229885057471265,
      "grad_norm": 0.5119329690933228,
      "learning_rate": 0.000475338214497548,
      "loss": 3.1562,
      "step": 7890
    },
    {
      "epoch": 30.268199233716476,
      "grad_norm": 0.5488606095314026,
      "learning_rate": 0.0004752723528753383,
      "loss": 3.1869,
      "step": 7900
    },
    {
      "epoch": 30.306513409961685,
      "grad_norm": 0.5030123591423035,
      "learning_rate": 0.00047520640800083027,
      "loss": 3.1827,
      "step": 7910
    },
    {
      "epoch": 30.344827586206897,
      "grad_norm": 0.5135530233383179,
      "learning_rate": 0.00047514037989839457,
      "loss": 3.1721,
      "step": 7920
    },
    {
      "epoch": 30.38314176245211,
      "grad_norm": 0.49910083413124084,
      "learning_rate": 0.000475074268592433,
      "loss": 3.1475,
      "step": 7930
    },
    {
      "epoch": 30.421455938697317,
      "grad_norm": 0.5248881578445435,
      "learning_rate": 0.0004750080741073776,
      "loss": 3.1793,
      "step": 7940
    },
    {
      "epoch": 30.45977011494253,
      "grad_norm": 0.4999752938747406,
      "learning_rate": 0.0004749417964676914,
      "loss": 3.1659,
      "step": 7950
    },
    {
      "epoch": 30.49808429118774,
      "grad_norm": 0.5214218497276306,
      "learning_rate": 0.0004748754356978683,
      "loss": 3.1579,
      "step": 7960
    },
    {
      "epoch": 30.53639846743295,
      "grad_norm": 0.5082606077194214,
      "learning_rate": 0.0004748089918224326,
      "loss": 3.1661,
      "step": 7970
    },
    {
      "epoch": 30.57471264367816,
      "grad_norm": 0.5153483152389526,
      "learning_rate": 0.00047474246486593974,
      "loss": 3.157,
      "step": 7980
    },
    {
      "epoch": 30.613026819923373,
      "grad_norm": 0.5237894058227539,
      "learning_rate": 0.0004746758548529755,
      "loss": 3.1593,
      "step": 7990
    },
    {
      "epoch": 30.65134099616858,
      "grad_norm": 0.5181130766868591,
      "learning_rate": 0.0004746091618081563,
      "loss": 3.182,
      "step": 8000
    },
    {
      "epoch": 30.689655172413794,
      "grad_norm": 0.5001683235168457,
      "learning_rate": 0.0004745423857561296,
      "loss": 3.1786,
      "step": 8010
    },
    {
      "epoch": 30.727969348659006,
      "grad_norm": 0.4988953173160553,
      "learning_rate": 0.00047447552672157334,
      "loss": 3.1646,
      "step": 8020
    },
    {
      "epoch": 30.766283524904214,
      "grad_norm": 0.49841174483299255,
      "learning_rate": 0.0004744085847291961,
      "loss": 3.1609,
      "step": 8030
    },
    {
      "epoch": 30.804597701149426,
      "grad_norm": 0.5067945122718811,
      "learning_rate": 0.00047434155980373714,
      "loss": 3.1824,
      "step": 8040
    },
    {
      "epoch": 30.842911877394634,
      "grad_norm": 0.5255600810050964,
      "learning_rate": 0.0004742744519699664,
      "loss": 3.1696,
      "step": 8050
    },
    {
      "epoch": 30.881226053639846,
      "grad_norm": 0.5180799961090088,
      "learning_rate": 0.00047420726125268445,
      "loss": 3.1921,
      "step": 8060
    },
    {
      "epoch": 30.919540229885058,
      "grad_norm": 0.5216328501701355,
      "learning_rate": 0.0004741399876767224,
      "loss": 3.1711,
      "step": 8070
    },
    {
      "epoch": 30.957854406130267,
      "grad_norm": 0.5350872874259949,
      "learning_rate": 0.0004740726312669422,
      "loss": 3.1855,
      "step": 8080
    },
    {
      "epoch": 30.99616858237548,
      "grad_norm": 0.5197725892066956,
      "learning_rate": 0.0004740051920482362,
      "loss": 3.1718,
      "step": 8090
    },
    {
      "epoch": 31.0,
      "eval_loss": 1.4724711179733276,
      "eval_runtime": 4.0719,
      "eval_samples_per_second": 4202.507,
      "eval_steps_per_second": 8.35,
      "step": 8091
    },
    {
      "epoch": 31.03448275862069,
      "grad_norm": 0.5103307366371155,
      "learning_rate": 0.0004739376700455275,
      "loss": 3.1458,
      "step": 8100
    },
    {
      "epoch": 31.0727969348659,
      "grad_norm": 0.5337116122245789,
      "learning_rate": 0.00047387006528376964,
      "loss": 3.147,
      "step": 8110
    },
    {
      "epoch": 31.11111111111111,
      "grad_norm": 0.5221734642982483,
      "learning_rate": 0.00047380237778794687,
      "loss": 3.18,
      "step": 8120
    },
    {
      "epoch": 31.149425287356323,
      "grad_norm": 0.5300010442733765,
      "learning_rate": 0.000473734607583074,
      "loss": 3.1528,
      "step": 8130
    },
    {
      "epoch": 31.18773946360153,
      "grad_norm": 0.5292860269546509,
      "learning_rate": 0.0004736667546941963,
      "loss": 3.1407,
      "step": 8140
    },
    {
      "epoch": 31.226053639846743,
      "grad_norm": 0.5154520273208618,
      "learning_rate": 0.0004735988191463898,
      "loss": 3.1633,
      "step": 8150
    },
    {
      "epoch": 31.264367816091955,
      "grad_norm": 0.49560993909835815,
      "learning_rate": 0.0004735308009647609,
      "loss": 3.1633,
      "step": 8160
    },
    {
      "epoch": 31.302681992337163,
      "grad_norm": 0.49736639857292175,
      "learning_rate": 0.0004734627001744465,
      "loss": 3.178,
      "step": 8170
    },
    {
      "epoch": 31.340996168582375,
      "grad_norm": 0.50339275598526,
      "learning_rate": 0.0004733945168006143,
      "loss": 3.1446,
      "step": 8180
    },
    {
      "epoch": 31.379310344827587,
      "grad_norm": 0.5166463851928711,
      "learning_rate": 0.00047332625086846217,
      "loss": 3.1731,
      "step": 8190
    },
    {
      "epoch": 31.417624521072796,
      "grad_norm": 0.500294029712677,
      "learning_rate": 0.00047325790240321876,
      "loss": 3.1611,
      "step": 8200
    },
    {
      "epoch": 31.455938697318008,
      "grad_norm": 0.5015785098075867,
      "learning_rate": 0.00047318947143014313,
      "loss": 3.1784,
      "step": 8210
    },
    {
      "epoch": 31.49425287356322,
      "grad_norm": 0.5177054405212402,
      "learning_rate": 0.0004731209579745248,
      "loss": 3.1618,
      "step": 8220
    },
    {
      "epoch": 31.532567049808428,
      "grad_norm": 0.49578773975372314,
      "learning_rate": 0.0004730523620616839,
      "loss": 3.1608,
      "step": 8230
    },
    {
      "epoch": 31.57088122605364,
      "grad_norm": 0.5001543760299683,
      "learning_rate": 0.00047298368371697074,
      "loss": 3.1325,
      "step": 8240
    },
    {
      "epoch": 31.60919540229885,
      "grad_norm": 0.542543888092041,
      "learning_rate": 0.00047291492296576643,
      "loss": 3.1559,
      "step": 8250
    },
    {
      "epoch": 31.64750957854406,
      "grad_norm": 0.499571830034256,
      "learning_rate": 0.0004728460798334823,
      "loss": 3.1796,
      "step": 8260
    },
    {
      "epoch": 31.685823754789272,
      "grad_norm": 0.48899850249290466,
      "learning_rate": 0.00047277715434556033,
      "loss": 3.172,
      "step": 8270
    },
    {
      "epoch": 31.724137931034484,
      "grad_norm": 0.5221883058547974,
      "learning_rate": 0.0004727081465274727,
      "loss": 3.1585,
      "step": 8280
    },
    {
      "epoch": 31.762452107279692,
      "grad_norm": 0.511247456073761,
      "learning_rate": 0.0004726390564047222,
      "loss": 3.1613,
      "step": 8290
    },
    {
      "epoch": 31.800766283524904,
      "grad_norm": 0.5275670289993286,
      "learning_rate": 0.000472569884002842,
      "loss": 3.1677,
      "step": 8300
    },
    {
      "epoch": 31.839080459770116,
      "grad_norm": 0.5241400003433228,
      "learning_rate": 0.0004725006293473956,
      "loss": 3.1686,
      "step": 8310
    },
    {
      "epoch": 31.877394636015325,
      "grad_norm": 0.5129905939102173,
      "learning_rate": 0.00047243129246397696,
      "loss": 3.1534,
      "step": 8320
    },
    {
      "epoch": 31.915708812260537,
      "grad_norm": 0.5093083381652832,
      "learning_rate": 0.00047236187337821044,
      "loss": 3.1509,
      "step": 8330
    },
    {
      "epoch": 31.95402298850575,
      "grad_norm": 0.4953724443912506,
      "learning_rate": 0.0004722923721157508,
      "loss": 3.1596,
      "step": 8340
    },
    {
      "epoch": 31.992337164750957,
      "grad_norm": 0.5075602531433105,
      "learning_rate": 0.00047222278870228296,
      "loss": 3.1512,
      "step": 8350
    },
    {
      "epoch": 32.0,
      "eval_loss": 1.4704104661941528,
      "eval_runtime": 3.8897,
      "eval_samples_per_second": 4399.294,
      "eval_steps_per_second": 8.741,
      "step": 8352
    },
    {
      "epoch": 32.030651340996165,
      "grad_norm": 0.4974568784236908,
      "learning_rate": 0.00047215312316352257,
      "loss": 3.1664,
      "step": 8360
    },
    {
      "epoch": 32.06896551724138,
      "grad_norm": 0.5141851305961609,
      "learning_rate": 0.0004720833755252153,
      "loss": 3.1459,
      "step": 8370
    },
    {
      "epoch": 32.10727969348659,
      "grad_norm": 0.5054855346679688,
      "learning_rate": 0.0004720135458131373,
      "loss": 3.1294,
      "step": 8380
    },
    {
      "epoch": 32.1455938697318,
      "grad_norm": 0.5076364278793335,
      "learning_rate": 0.0004719436340530951,
      "loss": 3.1432,
      "step": 8390
    },
    {
      "epoch": 32.18390804597701,
      "grad_norm": 0.5076902508735657,
      "learning_rate": 0.0004718736402709254,
      "loss": 3.1663,
      "step": 8400
    },
    {
      "epoch": 32.22222222222222,
      "grad_norm": 0.5112895965576172,
      "learning_rate": 0.0004718035644924954,
      "loss": 3.1517,
      "step": 8410
    },
    {
      "epoch": 32.26053639846743,
      "grad_norm": 0.5022846460342407,
      "learning_rate": 0.00047173340674370247,
      "loss": 3.1489,
      "step": 8420
    },
    {
      "epoch": 32.298850574712645,
      "grad_norm": 0.5014413595199585,
      "learning_rate": 0.0004716631670504743,
      "loss": 3.1641,
      "step": 8430
    },
    {
      "epoch": 32.337164750957854,
      "grad_norm": 0.5027888417243958,
      "learning_rate": 0.0004715928454387688,
      "loss": 3.1599,
      "step": 8440
    },
    {
      "epoch": 32.37547892720306,
      "grad_norm": 0.5286840200424194,
      "learning_rate": 0.0004715224419345744,
      "loss": 3.1552,
      "step": 8450
    },
    {
      "epoch": 32.41379310344828,
      "grad_norm": 0.5032575130462646,
      "learning_rate": 0.0004714519565639094,
      "loss": 3.143,
      "step": 8460
    },
    {
      "epoch": 32.452107279693486,
      "grad_norm": 0.49358829855918884,
      "learning_rate": 0.0004713813893528228,
      "loss": 3.164,
      "step": 8470
    },
    {
      "epoch": 32.490421455938694,
      "grad_norm": 0.49448180198669434,
      "learning_rate": 0.00047131074032739353,
      "loss": 3.1374,
      "step": 8480
    },
    {
      "epoch": 32.52873563218391,
      "grad_norm": 0.4908686876296997,
      "learning_rate": 0.0004712400095137308,
      "loss": 3.1348,
      "step": 8490
    },
    {
      "epoch": 32.56704980842912,
      "grad_norm": 0.47286733984947205,
      "learning_rate": 0.00047116919693797414,
      "loss": 3.1512,
      "step": 8500
    },
    {
      "epoch": 32.60536398467433,
      "grad_norm": 0.5182099938392639,
      "learning_rate": 0.00047109830262629315,
      "loss": 3.1448,
      "step": 8510
    },
    {
      "epoch": 32.64367816091954,
      "grad_norm": 0.517780601978302,
      "learning_rate": 0.00047102732660488794,
      "loss": 3.1572,
      "step": 8520
    },
    {
      "epoch": 32.68199233716475,
      "grad_norm": 0.5110996961593628,
      "learning_rate": 0.00047095626889998845,
      "loss": 3.1679,
      "step": 8530
    },
    {
      "epoch": 32.72030651340996,
      "grad_norm": 0.5175467133522034,
      "learning_rate": 0.000470885129537855,
      "loss": 3.1536,
      "step": 8540
    },
    {
      "epoch": 32.758620689655174,
      "grad_norm": 0.49522531032562256,
      "learning_rate": 0.00047081390854477814,
      "loss": 3.15,
      "step": 8550
    },
    {
      "epoch": 32.79693486590038,
      "grad_norm": 0.49960577487945557,
      "learning_rate": 0.0004707426059470784,
      "loss": 3.1515,
      "step": 8560
    },
    {
      "epoch": 32.83524904214559,
      "grad_norm": 0.49136826395988464,
      "learning_rate": 0.00047067122177110656,
      "loss": 3.1535,
      "step": 8570
    },
    {
      "epoch": 32.87356321839081,
      "grad_norm": 0.49209946393966675,
      "learning_rate": 0.0004705997560432437,
      "loss": 3.1483,
      "step": 8580
    },
    {
      "epoch": 32.911877394636015,
      "grad_norm": 0.5297273397445679,
      "learning_rate": 0.0004705282087899008,
      "loss": 3.1561,
      "step": 8590
    },
    {
      "epoch": 32.95019157088122,
      "grad_norm": 0.502891480922699,
      "learning_rate": 0.000470456580037519,
      "loss": 3.1297,
      "step": 8600
    },
    {
      "epoch": 32.98850574712644,
      "grad_norm": 0.5178816914558411,
      "learning_rate": 0.0004703848698125698,
      "loss": 3.1533,
      "step": 8610
    },
    {
      "epoch": 33.0,
      "eval_loss": 1.4677798748016357,
      "eval_runtime": 3.9414,
      "eval_samples_per_second": 4341.589,
      "eval_steps_per_second": 8.626,
      "step": 8613
    },
    {
      "epoch": 33.02681992337165,
      "grad_norm": 0.5264192223548889,
      "learning_rate": 0.0004703130781415545,
      "loss": 3.1322,
      "step": 8620
    },
    {
      "epoch": 33.065134099616856,
      "grad_norm": 0.5193048715591431,
      "learning_rate": 0.00047024120505100464,
      "loss": 3.1533,
      "step": 8630
    },
    {
      "epoch": 33.10344827586207,
      "grad_norm": 0.5040169358253479,
      "learning_rate": 0.00047016925056748196,
      "loss": 3.1119,
      "step": 8640
    },
    {
      "epoch": 33.14176245210728,
      "grad_norm": 0.49041807651519775,
      "learning_rate": 0.000470097214717578,
      "loss": 3.1314,
      "step": 8650
    },
    {
      "epoch": 33.18007662835249,
      "grad_norm": 0.4981425106525421,
      "learning_rate": 0.0004700250975279148,
      "loss": 3.1267,
      "step": 8660
    },
    {
      "epoch": 33.2183908045977,
      "grad_norm": 0.49070990085601807,
      "learning_rate": 0.0004699528990251439,
      "loss": 3.1494,
      "step": 8670
    },
    {
      "epoch": 33.25670498084291,
      "grad_norm": 0.5095452070236206,
      "learning_rate": 0.00046988061923594734,
      "loss": 3.1246,
      "step": 8680
    },
    {
      "epoch": 33.29501915708812,
      "grad_norm": 0.49275991320610046,
      "learning_rate": 0.00046980825818703696,
      "loss": 3.1381,
      "step": 8690
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 0.5256742835044861,
      "learning_rate": 0.0004697358159051549,
      "loss": 3.1351,
      "step": 8700
    },
    {
      "epoch": 33.371647509578544,
      "grad_norm": 0.5057896375656128,
      "learning_rate": 0.00046966329241707297,
      "loss": 3.1344,
      "step": 8710
    },
    {
      "epoch": 33.40996168582375,
      "grad_norm": 0.5204640626907349,
      "learning_rate": 0.00046959068774959315,
      "loss": 3.1384,
      "step": 8720
    },
    {
      "epoch": 33.44827586206897,
      "grad_norm": 0.4938119947910309,
      "learning_rate": 0.00046951800192954755,
      "loss": 3.1519,
      "step": 8730
    },
    {
      "epoch": 33.486590038314176,
      "grad_norm": 0.49296218156814575,
      "learning_rate": 0.00046944523498379797,
      "loss": 3.1555,
      "step": 8740
    },
    {
      "epoch": 33.524904214559385,
      "grad_norm": 0.4933550953865051,
      "learning_rate": 0.0004693723869392366,
      "loss": 3.1388,
      "step": 8750
    },
    {
      "epoch": 33.5632183908046,
      "grad_norm": 0.49034926295280457,
      "learning_rate": 0.00046929945782278517,
      "loss": 3.1159,
      "step": 8760
    },
    {
      "epoch": 33.60153256704981,
      "grad_norm": 0.49537965655326843,
      "learning_rate": 0.00046922644766139574,
      "loss": 3.1426,
      "step": 8770
    },
    {
      "epoch": 33.63984674329502,
      "grad_norm": 0.4878755211830139,
      "learning_rate": 0.00046915335648205003,
      "loss": 3.13,
      "step": 8780
    },
    {
      "epoch": 33.67816091954023,
      "grad_norm": 0.4954482614994049,
      "learning_rate": 0.00046908018431175984,
      "loss": 3.1396,
      "step": 8790
    },
    {
      "epoch": 33.71647509578544,
      "grad_norm": 0.5105640888214111,
      "learning_rate": 0.00046900693117756697,
      "loss": 3.1467,
      "step": 8800
    },
    {
      "epoch": 33.75478927203065,
      "grad_norm": 0.49682193994522095,
      "learning_rate": 0.000468933597106543,
      "loss": 3.151,
      "step": 8810
    },
    {
      "epoch": 33.793103448275865,
      "grad_norm": 0.5029655694961548,
      "learning_rate": 0.00046886018212578953,
      "loss": 3.1505,
      "step": 8820
    },
    {
      "epoch": 33.83141762452107,
      "grad_norm": 0.5156230330467224,
      "learning_rate": 0.00046878668626243794,
      "loss": 3.1646,
      "step": 8830
    },
    {
      "epoch": 33.86973180076628,
      "grad_norm": 0.4947209656238556,
      "learning_rate": 0.00046871310954364954,
      "loss": 3.1449,
      "step": 8840
    },
    {
      "epoch": 33.9080459770115,
      "grad_norm": 0.4985381066799164,
      "learning_rate": 0.0004686394519966157,
      "loss": 3.1316,
      "step": 8850
    },
    {
      "epoch": 33.946360153256705,
      "grad_norm": 0.5023317337036133,
      "learning_rate": 0.0004685657136485574,
      "loss": 3.1295,
      "step": 8860
    },
    {
      "epoch": 33.984674329501914,
      "grad_norm": 0.48907390236854553,
      "learning_rate": 0.0004684918945267256,
      "loss": 3.1459,
      "step": 8870
    },
    {
      "epoch": 34.0,
      "eval_loss": 1.4665335416793823,
      "eval_runtime": 4.0021,
      "eval_samples_per_second": 4275.75,
      "eval_steps_per_second": 8.496,
      "step": 8874
    },
    {
      "epoch": 34.02298850574713,
      "grad_norm": 0.524926483631134,
      "learning_rate": 0.00046841799465840116,
      "loss": 3.1286,
      "step": 8880
    },
    {
      "epoch": 34.06130268199234,
      "grad_norm": 0.5107219815254211,
      "learning_rate": 0.00046834401407089477,
      "loss": 3.1192,
      "step": 8890
    },
    {
      "epoch": 34.099616858237546,
      "grad_norm": 0.5024689435958862,
      "learning_rate": 0.0004682699527915467,
      "loss": 3.1168,
      "step": 8900
    },
    {
      "epoch": 34.13793103448276,
      "grad_norm": 0.5048447847366333,
      "learning_rate": 0.0004681958108477275,
      "loss": 3.1217,
      "step": 8910
    },
    {
      "epoch": 34.17624521072797,
      "grad_norm": 0.5199651718139648,
      "learning_rate": 0.00046812158826683715,
      "loss": 3.1261,
      "step": 8920
    },
    {
      "epoch": 34.21455938697318,
      "grad_norm": 0.5108758807182312,
      "learning_rate": 0.0004680472850763056,
      "loss": 3.1266,
      "step": 8930
    },
    {
      "epoch": 34.252873563218394,
      "grad_norm": 0.4847876727581024,
      "learning_rate": 0.0004679729013035925,
      "loss": 3.1329,
      "step": 8940
    },
    {
      "epoch": 34.2911877394636,
      "grad_norm": 0.5102556943893433,
      "learning_rate": 0.00046789843697618734,
      "loss": 3.1107,
      "step": 8950
    },
    {
      "epoch": 34.32950191570881,
      "grad_norm": 0.49990102648735046,
      "learning_rate": 0.00046782389212160936,
      "loss": 3.1335,
      "step": 8960
    },
    {
      "epoch": 34.367816091954026,
      "grad_norm": 0.5058313608169556,
      "learning_rate": 0.0004677492667674077,
      "loss": 3.1256,
      "step": 8970
    },
    {
      "epoch": 34.406130268199234,
      "grad_norm": 0.5038396120071411,
      "learning_rate": 0.000467674560941161,
      "loss": 3.1336,
      "step": 8980
    },
    {
      "epoch": 34.44444444444444,
      "grad_norm": 0.4985875189304352,
      "learning_rate": 0.00046759977467047774,
      "loss": 3.1485,
      "step": 8990
    },
    {
      "epoch": 34.48275862068966,
      "grad_norm": 0.49845248460769653,
      "learning_rate": 0.00046752490798299617,
      "loss": 3.1276,
      "step": 9000
    },
    {
      "epoch": 34.52107279693487,
      "grad_norm": 0.4967237114906311,
      "learning_rate": 0.00046744996090638435,
      "loss": 3.1321,
      "step": 9010
    },
    {
      "epoch": 34.559386973180075,
      "grad_norm": 0.51105135679245,
      "learning_rate": 0.0004673749334683398,
      "loss": 3.1345,
      "step": 9020
    },
    {
      "epoch": 34.59770114942529,
      "grad_norm": 0.4878047704696655,
      "learning_rate": 0.0004672998256965899,
      "loss": 3.1106,
      "step": 9030
    },
    {
      "epoch": 34.6360153256705,
      "grad_norm": 0.4856381416320801,
      "learning_rate": 0.0004672246376188918,
      "loss": 3.1158,
      "step": 9040
    },
    {
      "epoch": 34.67432950191571,
      "grad_norm": 0.4941536486148834,
      "learning_rate": 0.0004671493692630321,
      "loss": 3.1204,
      "step": 9050
    },
    {
      "epoch": 34.71264367816092,
      "grad_norm": 0.5237762331962585,
      "learning_rate": 0.00046707402065682715,
      "loss": 3.1305,
      "step": 9060
    },
    {
      "epoch": 34.75095785440613,
      "grad_norm": 0.5036439299583435,
      "learning_rate": 0.0004669985918281232,
      "loss": 3.1299,
      "step": 9070
    },
    {
      "epoch": 34.78927203065134,
      "grad_norm": 0.49699777364730835,
      "learning_rate": 0.00046692308280479574,
      "loss": 3.1252,
      "step": 9080
    },
    {
      "epoch": 34.827586206896555,
      "grad_norm": 0.5066977143287659,
      "learning_rate": 0.0004668474936147502,
      "loss": 3.1488,
      "step": 9090
    },
    {
      "epoch": 34.86590038314176,
      "grad_norm": 0.5027129054069519,
      "learning_rate": 0.0004667718242859216,
      "loss": 3.1322,
      "step": 9100
    },
    {
      "epoch": 34.90421455938697,
      "grad_norm": 0.5162356495857239,
      "learning_rate": 0.0004666960748462743,
      "loss": 3.1432,
      "step": 9110
    },
    {
      "epoch": 34.94252873563218,
      "grad_norm": 0.4808424115180969,
      "learning_rate": 0.0004666202453238027,
      "loss": 3.1289,
      "step": 9120
    },
    {
      "epoch": 34.980842911877396,
      "grad_norm": 0.48684874176979065,
      "learning_rate": 0.0004665443357465305,
      "loss": 3.1402,
      "step": 9130
    },
    {
      "epoch": 35.0,
      "eval_loss": 1.4681074619293213,
      "eval_runtime": 4.0548,
      "eval_samples_per_second": 4220.2,
      "eval_steps_per_second": 8.385,
      "step": 9135
    },
    {
      "epoch": 35.019157088122604,
      "grad_norm": 0.49541857838630676,
      "learning_rate": 0.00046646834614251097,
      "loss": 3.1241,
      "step": 9140
    },
    {
      "epoch": 35.05747126436781,
      "grad_norm": 0.508919894695282,
      "learning_rate": 0.00046639227653982727,
      "loss": 3.1078,
      "step": 9150
    },
    {
      "epoch": 35.09578544061303,
      "grad_norm": 0.5159052610397339,
      "learning_rate": 0.0004663161269665917,
      "loss": 3.106,
      "step": 9160
    },
    {
      "epoch": 35.13409961685824,
      "grad_norm": 0.48383116722106934,
      "learning_rate": 0.00046623989745094637,
      "loss": 3.126,
      "step": 9170
    },
    {
      "epoch": 35.172413793103445,
      "grad_norm": 0.5007097721099854,
      "learning_rate": 0.00046616358802106285,
      "loss": 3.1179,
      "step": 9180
    },
    {
      "epoch": 35.21072796934866,
      "grad_norm": 0.4908250868320465,
      "learning_rate": 0.00046608719870514236,
      "loss": 3.1139,
      "step": 9190
    },
    {
      "epoch": 35.24904214559387,
      "grad_norm": 0.5027132630348206,
      "learning_rate": 0.0004660107295314155,
      "loss": 3.116,
      "step": 9200
    },
    {
      "epoch": 35.28735632183908,
      "grad_norm": 0.5084259510040283,
      "learning_rate": 0.00046593418052814244,
      "loss": 3.1232,
      "step": 9210
    },
    {
      "epoch": 35.32567049808429,
      "grad_norm": 0.5306228399276733,
      "learning_rate": 0.0004658575517236129,
      "loss": 3.1095,
      "step": 9220
    },
    {
      "epoch": 35.3639846743295,
      "grad_norm": 0.46782639622688293,
      "learning_rate": 0.00046578084314614595,
      "loss": 3.1091,
      "step": 9230
    },
    {
      "epoch": 35.40229885057471,
      "grad_norm": 0.4826425015926361,
      "learning_rate": 0.0004657040548240903,
      "loss": 3.1147,
      "step": 9240
    },
    {
      "epoch": 35.440613026819925,
      "grad_norm": 0.501743733882904,
      "learning_rate": 0.00046562718678582393,
      "loss": 3.1395,
      "step": 9250
    },
    {
      "epoch": 35.47892720306513,
      "grad_norm": 0.4914288818836212,
      "learning_rate": 0.00046555023905975464,
      "loss": 3.1325,
      "step": 9260
    },
    {
      "epoch": 35.51724137931034,
      "grad_norm": 0.4932383894920349,
      "learning_rate": 0.00046547321167431934,
      "loss": 3.1039,
      "step": 9270
    },
    {
      "epoch": 35.55555555555556,
      "grad_norm": 0.49598750472068787,
      "learning_rate": 0.00046539610465798446,
      "loss": 3.1295,
      "step": 9280
    },
    {
      "epoch": 35.593869731800766,
      "grad_norm": 0.5028868913650513,
      "learning_rate": 0.00046531891803924585,
      "loss": 3.1278,
      "step": 9290
    },
    {
      "epoch": 35.632183908045974,
      "grad_norm": 0.5037199258804321,
      "learning_rate": 0.00046524165184662895,
      "loss": 3.108,
      "step": 9300
    },
    {
      "epoch": 35.67049808429119,
      "grad_norm": 0.4961707890033722,
      "learning_rate": 0.0004651643061086884,
      "loss": 3.1392,
      "step": 9310
    },
    {
      "epoch": 35.7088122605364,
      "grad_norm": 0.5063286423683167,
      "learning_rate": 0.0004650868808540083,
      "loss": 3.1459,
      "step": 9320
    },
    {
      "epoch": 35.747126436781606,
      "grad_norm": 0.5063357949256897,
      "learning_rate": 0.0004650093761112022,
      "loss": 3.1122,
      "step": 9330
    },
    {
      "epoch": 35.78544061302682,
      "grad_norm": 0.5268606543540955,
      "learning_rate": 0.000464931791908913,
      "loss": 3.1127,
      "step": 9340
    },
    {
      "epoch": 35.82375478927203,
      "grad_norm": 0.4866028428077698,
      "learning_rate": 0.0004648541282758129,
      "loss": 3.1393,
      "step": 9350
    },
    {
      "epoch": 35.86206896551724,
      "grad_norm": 0.4858221411705017,
      "learning_rate": 0.0004647763852406034,
      "loss": 3.1165,
      "step": 9360
    },
    {
      "epoch": 35.900383141762454,
      "grad_norm": 0.48971936106681824,
      "learning_rate": 0.0004646985628320155,
      "loss": 3.1331,
      "step": 9370
    },
    {
      "epoch": 35.93869731800766,
      "grad_norm": 0.5383098721504211,
      "learning_rate": 0.00046462066107880965,
      "loss": 3.112,
      "step": 9380
    },
    {
      "epoch": 35.97701149425287,
      "grad_norm": 0.4838809370994568,
      "learning_rate": 0.0004645426800097752,
      "loss": 3.1252,
      "step": 9390
    },
    {
      "epoch": 36.0,
      "eval_loss": 1.467081069946289,
      "eval_runtime": 3.9252,
      "eval_samples_per_second": 4359.537,
      "eval_steps_per_second": 8.662,
      "step": 9396
    },
    {
      "epoch": 36.015325670498086,
      "grad_norm": 0.4842360317707062,
      "learning_rate": 0.0004644646196537311,
      "loss": 3.1298,
      "step": 9400
    },
    {
      "epoch": 36.053639846743295,
      "grad_norm": 0.4826090633869171,
      "learning_rate": 0.0004643864800395257,
      "loss": 3.0996,
      "step": 9410
    },
    {
      "epoch": 36.0919540229885,
      "grad_norm": 0.4988791346549988,
      "learning_rate": 0.0004643082611960364,
      "loss": 3.1036,
      "step": 9420
    },
    {
      "epoch": 36.13026819923372,
      "grad_norm": 0.49139437079429626,
      "learning_rate": 0.00046422996315217003,
      "loss": 3.0925,
      "step": 9430
    },
    {
      "epoch": 36.16858237547893,
      "grad_norm": 0.5115753412246704,
      "learning_rate": 0.0004641515859368625,
      "loss": 3.1224,
      "step": 9440
    },
    {
      "epoch": 36.206896551724135,
      "grad_norm": 0.507626473903656,
      "learning_rate": 0.0004640731295790793,
      "loss": 3.0987,
      "step": 9450
    },
    {
      "epoch": 36.24521072796935,
      "grad_norm": 0.5033636093139648,
      "learning_rate": 0.000463994594107815,
      "loss": 3.106,
      "step": 9460
    },
    {
      "epoch": 36.28352490421456,
      "grad_norm": 0.5125355124473572,
      "learning_rate": 0.00046391597955209316,
      "loss": 3.1006,
      "step": 9470
    },
    {
      "epoch": 36.32183908045977,
      "grad_norm": 0.5088127851486206,
      "learning_rate": 0.00046383728594096696,
      "loss": 3.1028,
      "step": 9480
    },
    {
      "epoch": 36.36015325670498,
      "grad_norm": 0.4853053689002991,
      "learning_rate": 0.00046375851330351865,
      "loss": 3.1002,
      "step": 9490
    },
    {
      "epoch": 36.39846743295019,
      "grad_norm": 0.5101200938224792,
      "learning_rate": 0.00046367966166885967,
      "loss": 3.1144,
      "step": 9500
    },
    {
      "epoch": 36.4367816091954,
      "grad_norm": 0.49060192704200745,
      "learning_rate": 0.00046360073106613064,
      "loss": 3.1189,
      "step": 9510
    },
    {
      "epoch": 36.475095785440615,
      "grad_norm": 0.5140050053596497,
      "learning_rate": 0.00046352172152450133,
      "loss": 3.1003,
      "step": 9520
    },
    {
      "epoch": 36.513409961685824,
      "grad_norm": 0.48559215664863586,
      "learning_rate": 0.0004634426330731708,
      "loss": 3.1139,
      "step": 9530
    },
    {
      "epoch": 36.55172413793103,
      "grad_norm": 0.5027904510498047,
      "learning_rate": 0.00046336346574136724,
      "loss": 3.1306,
      "step": 9540
    },
    {
      "epoch": 36.59003831417625,
      "grad_norm": 0.5254718661308289,
      "learning_rate": 0.00046328421955834774,
      "loss": 3.1286,
      "step": 9550
    },
    {
      "epoch": 36.628352490421456,
      "grad_norm": 0.5041235685348511,
      "learning_rate": 0.00046320489455339907,
      "loss": 3.1159,
      "step": 9560
    },
    {
      "epoch": 36.666666666666664,
      "grad_norm": 0.49401408433914185,
      "learning_rate": 0.0004631254907558365,
      "loss": 3.1045,
      "step": 9570
    },
    {
      "epoch": 36.70498084291188,
      "grad_norm": 0.5232045650482178,
      "learning_rate": 0.00046304600819500506,
      "loss": 3.1264,
      "step": 9580
    },
    {
      "epoch": 36.74329501915709,
      "grad_norm": 0.48918288946151733,
      "learning_rate": 0.0004629664469002782,
      "loss": 3.1113,
      "step": 9590
    },
    {
      "epoch": 36.7816091954023,
      "grad_norm": 0.4981977641582489,
      "learning_rate": 0.00046288680690105907,
      "loss": 3.1076,
      "step": 9600
    },
    {
      "epoch": 36.81992337164751,
      "grad_norm": 0.4947275221347809,
      "learning_rate": 0.00046280708822677954,
      "loss": 3.1251,
      "step": 9610
    },
    {
      "epoch": 36.85823754789272,
      "grad_norm": 0.49952980875968933,
      "learning_rate": 0.0004627272909069007,
      "loss": 3.1116,
      "step": 9620
    },
    {
      "epoch": 36.89655172413793,
      "grad_norm": 0.4927964210510254,
      "learning_rate": 0.00046264741497091267,
      "loss": 3.1142,
      "step": 9630
    },
    {
      "epoch": 36.934865900383144,
      "grad_norm": 0.4795185327529907,
      "learning_rate": 0.0004625674604483346,
      "loss": 3.1033,
      "step": 9640
    },
    {
      "epoch": 36.97318007662835,
      "grad_norm": 0.5200660228729248,
      "learning_rate": 0.00046248742736871483,
      "loss": 3.1274,
      "step": 9650
    },
    {
      "epoch": 37.0,
      "eval_loss": 1.4635900259017944,
      "eval_runtime": 4.1052,
      "eval_samples_per_second": 4168.41,
      "eval_steps_per_second": 8.282,
      "step": 9657
    },
    {
      "epoch": 37.01149425287356,
      "grad_norm": 0.47726234793663025,
      "learning_rate": 0.0004624073157616305,
      "loss": 3.0994,
      "step": 9660
    },
    {
      "epoch": 37.04980842911878,
      "grad_norm": 0.5068309307098389,
      "learning_rate": 0.00046232712565668797,
      "loss": 3.0972,
      "step": 9670
    },
    {
      "epoch": 37.088122605363985,
      "grad_norm": 0.5069278478622437,
      "learning_rate": 0.0004622468570835224,
      "loss": 3.1137,
      "step": 9680
    },
    {
      "epoch": 37.12643678160919,
      "grad_norm": 0.4945053160190582,
      "learning_rate": 0.0004621665100717983,
      "loss": 3.0936,
      "step": 9690
    },
    {
      "epoch": 37.16475095785441,
      "grad_norm": 0.5085230469703674,
      "learning_rate": 0.00046208608465120874,
      "loss": 3.0958,
      "step": 9700
    },
    {
      "epoch": 37.20306513409962,
      "grad_norm": 0.4906866252422333,
      "learning_rate": 0.00046200558085147595,
      "loss": 3.0947,
      "step": 9710
    },
    {
      "epoch": 37.241379310344826,
      "grad_norm": 0.48490652441978455,
      "learning_rate": 0.0004619249987023513,
      "loss": 3.1003,
      "step": 9720
    },
    {
      "epoch": 37.27969348659004,
      "grad_norm": 0.48577386140823364,
      "learning_rate": 0.00046184433823361483,
      "loss": 3.1141,
      "step": 9730
    },
    {
      "epoch": 37.31800766283525,
      "grad_norm": 0.49213558435440063,
      "learning_rate": 0.0004617635994750757,
      "loss": 3.1018,
      "step": 9740
    },
    {
      "epoch": 37.35632183908046,
      "grad_norm": 0.4942778944969177,
      "learning_rate": 0.00046168278245657196,
      "loss": 3.0758,
      "step": 9750
    },
    {
      "epoch": 37.39463601532567,
      "grad_norm": 0.4853217303752899,
      "learning_rate": 0.0004616018872079706,
      "loss": 3.0935,
      "step": 9760
    },
    {
      "epoch": 37.43295019157088,
      "grad_norm": 0.4971369504928589,
      "learning_rate": 0.00046152091375916755,
      "loss": 3.0861,
      "step": 9770
    },
    {
      "epoch": 37.47126436781609,
      "grad_norm": 0.5110716223716736,
      "learning_rate": 0.00046143986214008747,
      "loss": 3.1124,
      "step": 9780
    },
    {
      "epoch": 37.509578544061306,
      "grad_norm": 0.49234500527381897,
      "learning_rate": 0.0004613587323806841,
      "loss": 3.095,
      "step": 9790
    },
    {
      "epoch": 37.547892720306514,
      "grad_norm": 0.496548056602478,
      "learning_rate": 0.00046127752451093995,
      "loss": 3.1074,
      "step": 9800
    },
    {
      "epoch": 37.58620689655172,
      "grad_norm": 0.4999888837337494,
      "learning_rate": 0.0004611962385608665,
      "loss": 3.0955,
      "step": 9810
    },
    {
      "epoch": 37.62452107279694,
      "grad_norm": 0.4959506094455719,
      "learning_rate": 0.0004611148745605039,
      "loss": 3.1107,
      "step": 9820
    },
    {
      "epoch": 37.662835249042146,
      "grad_norm": 0.48741304874420166,
      "learning_rate": 0.0004610334325399215,
      "loss": 3.1165,
      "step": 9830
    },
    {
      "epoch": 37.701149425287355,
      "grad_norm": 0.5436860918998718,
      "learning_rate": 0.000460951912529217,
      "loss": 3.1071,
      "step": 9840
    },
    {
      "epoch": 37.73946360153257,
      "grad_norm": 0.4861955940723419,
      "learning_rate": 0.0004608703145585172,
      "loss": 3.1039,
      "step": 9850
    },
    {
      "epoch": 37.77777777777778,
      "grad_norm": 0.499724417924881,
      "learning_rate": 0.00046078863865797775,
      "loss": 3.0966,
      "step": 9860
    },
    {
      "epoch": 37.81609195402299,
      "grad_norm": 0.4967699348926544,
      "learning_rate": 0.00046070688485778304,
      "loss": 3.0974,
      "step": 9870
    },
    {
      "epoch": 37.8544061302682,
      "grad_norm": 0.529494047164917,
      "learning_rate": 0.0004606250531881463,
      "loss": 3.1123,
      "step": 9880
    },
    {
      "epoch": 37.89272030651341,
      "grad_norm": 0.49587205052375793,
      "learning_rate": 0.0004605431436793093,
      "loss": 3.1109,
      "step": 9890
    },
    {
      "epoch": 37.93103448275862,
      "grad_norm": 0.4671652913093567,
      "learning_rate": 0.00046046115636154286,
      "loss": 3.0775,
      "step": 9900
    },
    {
      "epoch": 37.969348659003835,
      "grad_norm": 0.48620328307151794,
      "learning_rate": 0.00046037909126514645,
      "loss": 3.1027,
      "step": 9910
    },
    {
      "epoch": 38.0,
      "eval_loss": 1.4621849060058594,
      "eval_runtime": 4.1091,
      "eval_samples_per_second": 4164.446,
      "eval_steps_per_second": 8.274,
      "step": 9918
    },
    {
      "epoch": 38.00766283524904,
      "grad_norm": 0.4556311070919037,
      "learning_rate": 0.0004602969484204482,
      "loss": 3.1037,
      "step": 9920
    },
    {
      "epoch": 38.04597701149425,
      "grad_norm": 0.49419522285461426,
      "learning_rate": 0.00046021472785780525,
      "loss": 3.0688,
      "step": 9930
    },
    {
      "epoch": 38.08429118773947,
      "grad_norm": 0.4898952543735504,
      "learning_rate": 0.00046013242960760305,
      "loss": 3.0817,
      "step": 9940
    },
    {
      "epoch": 38.122605363984675,
      "grad_norm": 0.47861558198928833,
      "learning_rate": 0.00046005005370025614,
      "loss": 3.0828,
      "step": 9950
    },
    {
      "epoch": 38.160919540229884,
      "grad_norm": 0.493597149848938,
      "learning_rate": 0.0004599676001662075,
      "loss": 3.095,
      "step": 9960
    },
    {
      "epoch": 38.1992337164751,
      "grad_norm": 0.49843698740005493,
      "learning_rate": 0.00045988506903592893,
      "loss": 3.0911,
      "step": 9970
    },
    {
      "epoch": 38.23754789272031,
      "grad_norm": 0.4861200451850891,
      "learning_rate": 0.00045980246033992085,
      "loss": 3.079,
      "step": 9980
    },
    {
      "epoch": 38.275862068965516,
      "grad_norm": 0.4896606206893921,
      "learning_rate": 0.00045971977410871233,
      "loss": 3.0874,
      "step": 9990
    },
    {
      "epoch": 38.31417624521073,
      "grad_norm": 0.49090898036956787,
      "learning_rate": 0.0004596370103728613,
      "loss": 3.0841,
      "step": 10000
    },
    {
      "epoch": 38.35249042145594,
      "grad_norm": 0.5124427676200867,
      "learning_rate": 0.00045955416916295403,
      "loss": 3.1001,
      "step": 10010
    },
    {
      "epoch": 38.39080459770115,
      "grad_norm": 0.49051931500434875,
      "learning_rate": 0.0004594712505096055,
      "loss": 3.0929,
      "step": 10020
    },
    {
      "epoch": 38.42911877394636,
      "grad_norm": 0.49055832624435425,
      "learning_rate": 0.00045938825444345944,
      "loss": 3.0921,
      "step": 10030
    },
    {
      "epoch": 38.46743295019157,
      "grad_norm": 0.4810200035572052,
      "learning_rate": 0.00045930518099518815,
      "loss": 3.0764,
      "step": 10040
    },
    {
      "epoch": 38.50574712643678,
      "grad_norm": 0.502109706401825,
      "learning_rate": 0.00045922203019549244,
      "loss": 3.0774,
      "step": 10050
    },
    {
      "epoch": 38.54406130268199,
      "grad_norm": 0.5020413398742676,
      "learning_rate": 0.00045913880207510186,
      "loss": 3.0808,
      "step": 10060
    },
    {
      "epoch": 38.582375478927204,
      "grad_norm": 0.5043501853942871,
      "learning_rate": 0.0004590554966647743,
      "loss": 3.1071,
      "step": 10070
    },
    {
      "epoch": 38.62068965517241,
      "grad_norm": 0.4947638511657715,
      "learning_rate": 0.00045897211399529637,
      "loss": 3.0892,
      "step": 10080
    },
    {
      "epoch": 38.65900383141762,
      "grad_norm": 0.49972644448280334,
      "learning_rate": 0.0004588886540974833,
      "loss": 3.0874,
      "step": 10090
    },
    {
      "epoch": 38.69731800766284,
      "grad_norm": 0.4987005889415741,
      "learning_rate": 0.00045880511700217857,
      "loss": 3.1027,
      "step": 10100
    },
    {
      "epoch": 38.735632183908045,
      "grad_norm": 0.4909302890300751,
      "learning_rate": 0.0004587215027402546,
      "loss": 3.1,
      "step": 10110
    },
    {
      "epoch": 38.77394636015325,
      "grad_norm": 0.4780903458595276,
      "learning_rate": 0.00045863781134261204,
      "loss": 3.1152,
      "step": 10120
    },
    {
      "epoch": 38.81226053639847,
      "grad_norm": 0.4915282726287842,
      "learning_rate": 0.00045855404284018015,
      "loss": 3.0815,
      "step": 10130
    },
    {
      "epoch": 38.85057471264368,
      "grad_norm": 0.4881725609302521,
      "learning_rate": 0.00045847019726391657,
      "loss": 3.0996,
      "step": 10140
    },
    {
      "epoch": 38.888888888888886,
      "grad_norm": 0.49733826518058777,
      "learning_rate": 0.00045838627464480763,
      "loss": 3.1234,
      "step": 10150
    },
    {
      "epoch": 38.9272030651341,
      "grad_norm": 0.48460155725479126,
      "learning_rate": 0.00045830227501386797,
      "loss": 3.0966,
      "step": 10160
    },
    {
      "epoch": 38.96551724137931,
      "grad_norm": 0.4875592887401581,
      "learning_rate": 0.0004582181984021407,
      "loss": 3.0977,
      "step": 10170
    },
    {
      "epoch": 39.0,
      "eval_loss": 1.4599801301956177,
      "eval_runtime": 4.2641,
      "eval_samples_per_second": 4013.075,
      "eval_steps_per_second": 7.974,
      "step": 10179
    },
    {
      "epoch": 39.00383141762452,
      "grad_norm": 0.481730580329895,
      "learning_rate": 0.0004581340448406974,
      "loss": 3.1044,
      "step": 10180
    },
    {
      "epoch": 39.04214559386973,
      "grad_norm": 0.4970034062862396,
      "learning_rate": 0.0004580498143606382,
      "loss": 3.0761,
      "step": 10190
    },
    {
      "epoch": 39.08045977011494,
      "grad_norm": 0.4906182885169983,
      "learning_rate": 0.00045796550699309145,
      "loss": 3.0643,
      "step": 10200
    },
    {
      "epoch": 39.11877394636015,
      "grad_norm": 0.5104316473007202,
      "learning_rate": 0.0004578811227692141,
      "loss": 3.0964,
      "step": 10210
    },
    {
      "epoch": 39.157088122605366,
      "grad_norm": 0.5149737000465393,
      "learning_rate": 0.00045779666172019145,
      "loss": 3.0899,
      "step": 10220
    },
    {
      "epoch": 39.195402298850574,
      "grad_norm": 0.5040616393089294,
      "learning_rate": 0.00045771212387723707,
      "loss": 3.0754,
      "step": 10230
    },
    {
      "epoch": 39.23371647509578,
      "grad_norm": 0.4793558418750763,
      "learning_rate": 0.000457627509271593,
      "loss": 3.0768,
      "step": 10240
    },
    {
      "epoch": 39.272030651341,
      "grad_norm": 0.51225745677948,
      "learning_rate": 0.0004575428179345298,
      "loss": 3.0838,
      "step": 10250
    },
    {
      "epoch": 39.310344827586206,
      "grad_norm": 0.4686463177204132,
      "learning_rate": 0.0004574580498973462,
      "loss": 3.1079,
      "step": 10260
    },
    {
      "epoch": 39.348659003831415,
      "grad_norm": 0.49712854623794556,
      "learning_rate": 0.00045737320519136917,
      "loss": 3.0846,
      "step": 10270
    },
    {
      "epoch": 39.38697318007663,
      "grad_norm": 0.4890410006046295,
      "learning_rate": 0.0004572882838479543,
      "loss": 3.0764,
      "step": 10280
    },
    {
      "epoch": 39.42528735632184,
      "grad_norm": 0.5141687989234924,
      "learning_rate": 0.0004572032858984853,
      "loss": 3.0842,
      "step": 10290
    },
    {
      "epoch": 39.46360153256705,
      "grad_norm": 0.5004405379295349,
      "learning_rate": 0.0004571182113743744,
      "loss": 3.0872,
      "step": 10300
    },
    {
      "epoch": 39.50191570881226,
      "grad_norm": 0.4898819625377655,
      "learning_rate": 0.0004570330603070617,
      "loss": 3.0747,
      "step": 10310
    },
    {
      "epoch": 39.54022988505747,
      "grad_norm": 0.48757466673851013,
      "learning_rate": 0.00045694783272801624,
      "loss": 3.0829,
      "step": 10320
    },
    {
      "epoch": 39.57854406130268,
      "grad_norm": 0.4864991307258606,
      "learning_rate": 0.0004568625286687347,
      "loss": 3.1041,
      "step": 10330
    },
    {
      "epoch": 39.616858237547895,
      "grad_norm": 0.5103943943977356,
      "learning_rate": 0.00045677714816074236,
      "loss": 3.0987,
      "step": 10340
    },
    {
      "epoch": 39.6551724137931,
      "grad_norm": 0.5027530789375305,
      "learning_rate": 0.00045669169123559265,
      "loss": 3.101,
      "step": 10350
    },
    {
      "epoch": 39.69348659003831,
      "grad_norm": 0.5045791268348694,
      "learning_rate": 0.0004566061579248674,
      "loss": 3.0848,
      "step": 10360
    },
    {
      "epoch": 39.73180076628353,
      "grad_norm": 0.532825231552124,
      "learning_rate": 0.00045652054826017643,
      "loss": 3.096,
      "step": 10370
    },
    {
      "epoch": 39.770114942528735,
      "grad_norm": 0.5128106474876404,
      "learning_rate": 0.00045643486227315796,
      "loss": 3.0712,
      "step": 10380
    },
    {
      "epoch": 39.808429118773944,
      "grad_norm": 0.5110977292060852,
      "learning_rate": 0.00045634909999547836,
      "loss": 3.0648,
      "step": 10390
    },
    {
      "epoch": 39.84674329501916,
      "grad_norm": 0.47329097986221313,
      "learning_rate": 0.00045626326145883206,
      "loss": 3.091,
      "step": 10400
    },
    {
      "epoch": 39.88505747126437,
      "grad_norm": 0.5045367479324341,
      "learning_rate": 0.00045617734669494195,
      "loss": 3.0934,
      "step": 10410
    },
    {
      "epoch": 39.923371647509576,
      "grad_norm": 0.529341459274292,
      "learning_rate": 0.0004560913557355589,
      "loss": 3.1031,
      "step": 10420
    },
    {
      "epoch": 39.96168582375479,
      "grad_norm": 0.48400744795799255,
      "learning_rate": 0.0004560052886124619,
      "loss": 3.0883,
      "step": 10430
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.4807228446006775,
      "learning_rate": 0.0004559191453574582,
      "loss": 3.091,
      "step": 10440
    },
    {
      "epoch": 40.0,
      "eval_loss": 1.4602302312850952,
      "eval_runtime": 3.8971,
      "eval_samples_per_second": 4390.945,
      "eval_steps_per_second": 8.724,
      "step": 10440
    },
    {
      "epoch": 40.03831417624521,
      "grad_norm": 0.4666390120983124,
      "learning_rate": 0.0004558329260023832,
      "loss": 3.047,
      "step": 10450
    },
    {
      "epoch": 40.076628352490424,
      "grad_norm": 0.49093490839004517,
      "learning_rate": 0.00045574663057910026,
      "loss": 3.0723,
      "step": 10460
    },
    {
      "epoch": 40.11494252873563,
      "grad_norm": 0.4838627576828003,
      "learning_rate": 0.0004556602591195012,
      "loss": 3.088,
      "step": 10470
    },
    {
      "epoch": 40.15325670498084,
      "grad_norm": 0.4822830557823181,
      "learning_rate": 0.00045557381165550536,
      "loss": 3.0786,
      "step": 10480
    },
    {
      "epoch": 40.191570881226056,
      "grad_norm": 0.4736655354499817,
      "learning_rate": 0.00045548728821906084,
      "loss": 3.0642,
      "step": 10490
    },
    {
      "epoch": 40.229885057471265,
      "grad_norm": 0.5204936861991882,
      "learning_rate": 0.0004554006888421433,
      "loss": 3.0669,
      "step": 10500
    },
    {
      "epoch": 40.26819923371647,
      "grad_norm": 0.4985528886318207,
      "learning_rate": 0.0004553140135567566,
      "loss": 3.0794,
      "step": 10510
    },
    {
      "epoch": 40.30651340996169,
      "grad_norm": 0.47252851724624634,
      "learning_rate": 0.00045522726239493293,
      "loss": 3.087,
      "step": 10520
    },
    {
      "epoch": 40.3448275862069,
      "grad_norm": 0.48304030299186707,
      "learning_rate": 0.00045514043538873216,
      "loss": 3.0866,
      "step": 10530
    },
    {
      "epoch": 40.383141762452105,
      "grad_norm": 0.49956974387168884,
      "learning_rate": 0.00045505353257024227,
      "loss": 3.078,
      "step": 10540
    },
    {
      "epoch": 40.42145593869732,
      "grad_norm": 0.5076159238815308,
      "learning_rate": 0.0004549665539715795,
      "loss": 3.0786,
      "step": 10550
    },
    {
      "epoch": 40.45977011494253,
      "grad_norm": 0.49619317054748535,
      "learning_rate": 0.0004548794996248878,
      "loss": 3.0579,
      "step": 10560
    },
    {
      "epoch": 40.49808429118774,
      "grad_norm": 0.5118008255958557,
      "learning_rate": 0.00045479236956233915,
      "loss": 3.0849,
      "step": 10570
    },
    {
      "epoch": 40.53639846743295,
      "grad_norm": 0.496540904045105,
      "learning_rate": 0.0004547051638161337,
      "loss": 3.0718,
      "step": 10580
    },
    {
      "epoch": 40.57471264367816,
      "grad_norm": 0.4976113736629486,
      "learning_rate": 0.0004546178824184994,
      "loss": 3.0802,
      "step": 10590
    },
    {
      "epoch": 40.61302681992337,
      "grad_norm": 0.4688226580619812,
      "learning_rate": 0.0004545305254016923,
      "loss": 3.0808,
      "step": 10600
    },
    {
      "epoch": 40.651340996168585,
      "grad_norm": 0.5225054621696472,
      "learning_rate": 0.0004544430927979962,
      "loss": 3.0699,
      "step": 10610
    },
    {
      "epoch": 40.689655172413794,
      "grad_norm": 0.49127233028411865,
      "learning_rate": 0.00045435558463972296,
      "loss": 3.0443,
      "step": 10620
    },
    {
      "epoch": 40.727969348659,
      "grad_norm": 0.5041837692260742,
      "learning_rate": 0.0004542680009592124,
      "loss": 3.0842,
      "step": 10630
    },
    {
      "epoch": 40.76628352490422,
      "grad_norm": 0.4914102852344513,
      "learning_rate": 0.00045418034178883215,
      "loss": 3.0692,
      "step": 10640
    },
    {
      "epoch": 40.804597701149426,
      "grad_norm": 0.4827280342578888,
      "learning_rate": 0.00045409260716097787,
      "loss": 3.0897,
      "step": 10650
    },
    {
      "epoch": 40.842911877394634,
      "grad_norm": 0.47294679284095764,
      "learning_rate": 0.0004540047971080729,
      "loss": 3.0849,
      "step": 10660
    },
    {
      "epoch": 40.88122605363985,
      "grad_norm": 0.49986276030540466,
      "learning_rate": 0.00045391691166256863,
      "loss": 3.0893,
      "step": 10670
    },
    {
      "epoch": 40.91954022988506,
      "grad_norm": 0.4995509684085846,
      "learning_rate": 0.0004538289508569443,
      "loss": 3.0961,
      "step": 10680
    },
    {
      "epoch": 40.95785440613027,
      "grad_norm": 0.48712319135665894,
      "learning_rate": 0.00045374091472370694,
      "loss": 3.0873,
      "step": 10690
    },
    {
      "epoch": 40.99616858237548,
      "grad_norm": 0.4979122579097748,
      "learning_rate": 0.0004536528032953914,
      "loss": 3.0613,
      "step": 10700
    },
    {
      "epoch": 41.0,
      "eval_loss": 1.4584314823150635,
      "eval_runtime": 3.9345,
      "eval_samples_per_second": 4349.273,
      "eval_steps_per_second": 8.642,
      "step": 10701
    },
    {
      "epoch": 41.03448275862069,
      "grad_norm": 0.4747644066810608,
      "learning_rate": 0.0004535646166045604,
      "loss": 3.0463,
      "step": 10710
    },
    {
      "epoch": 41.0727969348659,
      "grad_norm": 0.48726707696914673,
      "learning_rate": 0.00045347635468380443,
      "loss": 3.0595,
      "step": 10720
    },
    {
      "epoch": 41.111111111111114,
      "grad_norm": 0.49368125200271606,
      "learning_rate": 0.0004533880175657419,
      "loss": 3.0736,
      "step": 10730
    },
    {
      "epoch": 41.14942528735632,
      "grad_norm": 0.5078622698783875,
      "learning_rate": 0.0004532996052830189,
      "loss": 3.0484,
      "step": 10740
    },
    {
      "epoch": 41.18773946360153,
      "grad_norm": 0.4928208589553833,
      "learning_rate": 0.0004532111178683093,
      "loss": 3.0602,
      "step": 10750
    },
    {
      "epoch": 41.22605363984675,
      "grad_norm": 0.46925437450408936,
      "learning_rate": 0.0004531225553543148,
      "loss": 3.0435,
      "step": 10760
    },
    {
      "epoch": 41.264367816091955,
      "grad_norm": 0.49014678597450256,
      "learning_rate": 0.0004530339177737648,
      "loss": 3.0388,
      "step": 10770
    },
    {
      "epoch": 41.30268199233716,
      "grad_norm": 0.46539610624313354,
      "learning_rate": 0.0004529452051594165,
      "loss": 3.0875,
      "step": 10780
    },
    {
      "epoch": 41.34099616858238,
      "grad_norm": 0.4803749620914459,
      "learning_rate": 0.0004528564175440547,
      "loss": 3.0852,
      "step": 10790
    },
    {
      "epoch": 41.37931034482759,
      "grad_norm": 0.4857303500175476,
      "learning_rate": 0.0004527675549604921,
      "loss": 3.06,
      "step": 10800
    },
    {
      "epoch": 41.417624521072796,
      "grad_norm": 0.480001300573349,
      "learning_rate": 0.000452678617441569,
      "loss": 3.0493,
      "step": 10810
    },
    {
      "epoch": 41.45593869731801,
      "grad_norm": 0.47490355372428894,
      "learning_rate": 0.00045258960502015327,
      "loss": 3.0715,
      "step": 10820
    },
    {
      "epoch": 41.49425287356322,
      "grad_norm": 0.4978151023387909,
      "learning_rate": 0.00045250051772914075,
      "loss": 3.0879,
      "step": 10830
    },
    {
      "epoch": 41.53256704980843,
      "grad_norm": 0.48743242025375366,
      "learning_rate": 0.00045241135560145475,
      "loss": 3.075,
      "step": 10840
    },
    {
      "epoch": 41.57088122605364,
      "grad_norm": 0.4738326966762543,
      "learning_rate": 0.00045232211867004624,
      "loss": 3.0766,
      "step": 10850
    },
    {
      "epoch": 41.60919540229885,
      "grad_norm": 0.4713243842124939,
      "learning_rate": 0.00045223280696789404,
      "loss": 3.0777,
      "step": 10860
    },
    {
      "epoch": 41.64750957854406,
      "grad_norm": 0.48026737570762634,
      "learning_rate": 0.0004521434205280042,
      "loss": 3.0892,
      "step": 10870
    },
    {
      "epoch": 41.68582375478927,
      "grad_norm": 0.5068142414093018,
      "learning_rate": 0.00045205395938341077,
      "loss": 3.0716,
      "step": 10880
    },
    {
      "epoch": 41.724137931034484,
      "grad_norm": 0.49937593936920166,
      "learning_rate": 0.0004519644235671752,
      "loss": 3.0564,
      "step": 10890
    },
    {
      "epoch": 41.76245210727969,
      "grad_norm": 0.4938296973705292,
      "learning_rate": 0.0004518748131123867,
      "loss": 3.0737,
      "step": 10900
    },
    {
      "epoch": 41.8007662835249,
      "grad_norm": 0.5317801833152771,
      "learning_rate": 0.0004517851280521619,
      "loss": 3.0893,
      "step": 10910
    },
    {
      "epoch": 41.839080459770116,
      "grad_norm": 0.48991668224334717,
      "learning_rate": 0.0004516953684196451,
      "loss": 3.0594,
      "step": 10920
    },
    {
      "epoch": 41.877394636015325,
      "grad_norm": 0.4862271845340729,
      "learning_rate": 0.0004516055342480081,
      "loss": 3.0774,
      "step": 10930
    },
    {
      "epoch": 41.91570881226053,
      "grad_norm": 0.4988522231578827,
      "learning_rate": 0.00045151562557045025,
      "loss": 3.0744,
      "step": 10940
    },
    {
      "epoch": 41.95402298850575,
      "grad_norm": 0.5004352331161499,
      "learning_rate": 0.0004514256424201985,
      "loss": 3.0869,
      "step": 10950
    },
    {
      "epoch": 41.99233716475096,
      "grad_norm": 0.4765635132789612,
      "learning_rate": 0.0004513355848305073,
      "loss": 3.0795,
      "step": 10960
    },
    {
      "epoch": 42.0,
      "eval_loss": 1.4562145471572876,
      "eval_runtime": 4.0931,
      "eval_samples_per_second": 4180.651,
      "eval_steps_per_second": 8.307,
      "step": 10962
    },
    {
      "epoch": 42.030651340996165,
      "grad_norm": 0.47716024518013,
      "learning_rate": 0.00045124545283465857,
      "loss": 3.0613,
      "step": 10970
    },
    {
      "epoch": 42.06896551724138,
      "grad_norm": 0.4729301929473877,
      "learning_rate": 0.00045115524646596175,
      "loss": 3.0361,
      "step": 10980
    },
    {
      "epoch": 42.10727969348659,
      "grad_norm": 0.4857467710971832,
      "learning_rate": 0.00045106496575775376,
      "loss": 3.073,
      "step": 10990
    },
    {
      "epoch": 42.1455938697318,
      "grad_norm": 0.48894640803337097,
      "learning_rate": 0.000450974610743399,
      "loss": 3.0522,
      "step": 11000
    },
    {
      "epoch": 42.18390804597701,
      "grad_norm": 0.4653294086456299,
      "learning_rate": 0.0004508841814562893,
      "loss": 3.0539,
      "step": 11010
    },
    {
      "epoch": 42.22222222222222,
      "grad_norm": 0.5175238251686096,
      "learning_rate": 0.000450793677929844,
      "loss": 3.0336,
      "step": 11020
    },
    {
      "epoch": 42.26053639846743,
      "grad_norm": 0.47943389415740967,
      "learning_rate": 0.0004507031001975098,
      "loss": 3.0555,
      "step": 11030
    },
    {
      "epoch": 42.298850574712645,
      "grad_norm": 0.49581092596054077,
      "learning_rate": 0.000450612448292761,
      "loss": 3.0682,
      "step": 11040
    },
    {
      "epoch": 42.337164750957854,
      "grad_norm": 0.4935286045074463,
      "learning_rate": 0.000450521722249099,
      "loss": 3.0685,
      "step": 11050
    },
    {
      "epoch": 42.37547892720306,
      "grad_norm": 0.4828867018222809,
      "learning_rate": 0.0004504309221000529,
      "loss": 3.0743,
      "step": 11060
    },
    {
      "epoch": 42.41379310344828,
      "grad_norm": 0.5155743956565857,
      "learning_rate": 0.00045034004787917906,
      "loss": 3.057,
      "step": 11070
    },
    {
      "epoch": 42.452107279693486,
      "grad_norm": 0.4850304424762726,
      "learning_rate": 0.0004502490996200612,
      "loss": 3.0535,
      "step": 11080
    },
    {
      "epoch": 42.490421455938694,
      "grad_norm": 0.49135729670524597,
      "learning_rate": 0.0004501580773563103,
      "loss": 3.0729,
      "step": 11090
    },
    {
      "epoch": 42.52873563218391,
      "grad_norm": 0.48349514603614807,
      "learning_rate": 0.00045006698112156496,
      "loss": 3.0668,
      "step": 11100
    },
    {
      "epoch": 42.56704980842912,
      "grad_norm": 0.4723343849182129,
      "learning_rate": 0.0004499758109494909,
      "loss": 3.0681,
      "step": 11110
    },
    {
      "epoch": 42.60536398467433,
      "grad_norm": 0.4991820752620697,
      "learning_rate": 0.0004498845668737813,
      "loss": 3.0701,
      "step": 11120
    },
    {
      "epoch": 42.64367816091954,
      "grad_norm": 0.47869423031806946,
      "learning_rate": 0.00044979324892815655,
      "loss": 3.0595,
      "step": 11130
    },
    {
      "epoch": 42.68199233716475,
      "grad_norm": 0.47888511419296265,
      "learning_rate": 0.00044970185714636435,
      "loss": 3.0681,
      "step": 11140
    },
    {
      "epoch": 42.72030651340996,
      "grad_norm": 0.46818169951438904,
      "learning_rate": 0.0004496103915621796,
      "loss": 3.0812,
      "step": 11150
    },
    {
      "epoch": 42.758620689655174,
      "grad_norm": 0.4768770635128021,
      "learning_rate": 0.0004495188522094049,
      "loss": 3.041,
      "step": 11160
    },
    {
      "epoch": 42.79693486590038,
      "grad_norm": 0.4890933930873871,
      "learning_rate": 0.00044942723912186946,
      "loss": 3.056,
      "step": 11170
    },
    {
      "epoch": 42.83524904214559,
      "grad_norm": 0.4930141568183899,
      "learning_rate": 0.0004493355523334303,
      "loss": 3.0651,
      "step": 11180
    },
    {
      "epoch": 42.87356321839081,
      "grad_norm": 0.4908832013607025,
      "learning_rate": 0.0004492437918779713,
      "loss": 3.0644,
      "step": 11190
    },
    {
      "epoch": 42.911877394636015,
      "grad_norm": 0.47983503341674805,
      "learning_rate": 0.0004491519577894038,
      "loss": 3.0631,
      "step": 11200
    },
    {
      "epoch": 42.95019157088122,
      "grad_norm": 0.504518985748291,
      "learning_rate": 0.0004490600501016662,
      "loss": 3.081,
      "step": 11210
    },
    {
      "epoch": 42.98850574712644,
      "grad_norm": 0.467965692281723,
      "learning_rate": 0.0004489680688487242,
      "loss": 3.0642,
      "step": 11220
    },
    {
      "epoch": 43.0,
      "eval_loss": 1.4560930728912354,
      "eval_runtime": 3.9099,
      "eval_samples_per_second": 4376.536,
      "eval_steps_per_second": 8.696,
      "step": 11223
    },
    {
      "epoch": 43.02681992337165,
      "grad_norm": 0.4994872212409973,
      "learning_rate": 0.0004488760140645708,
      "loss": 3.0312,
      "step": 11230
    },
    {
      "epoch": 43.065134099616856,
      "grad_norm": 0.47556811571121216,
      "learning_rate": 0.00044878388578322586,
      "loss": 3.0499,
      "step": 11240
    },
    {
      "epoch": 43.10344827586207,
      "grad_norm": 0.4888705313205719,
      "learning_rate": 0.0004486916840387366,
      "loss": 3.0503,
      "step": 11250
    },
    {
      "epoch": 43.14176245210728,
      "grad_norm": 0.4890168607234955,
      "learning_rate": 0.00044859940886517744,
      "loss": 3.0462,
      "step": 11260
    },
    {
      "epoch": 43.18007662835249,
      "grad_norm": 0.5536298155784607,
      "learning_rate": 0.0004485070602966498,
      "loss": 3.0584,
      "step": 11270
    },
    {
      "epoch": 43.2183908045977,
      "grad_norm": 0.4791239798069,
      "learning_rate": 0.0004484146383672823,
      "loss": 3.0537,
      "step": 11280
    },
    {
      "epoch": 43.25670498084291,
      "grad_norm": 0.4718301594257355,
      "learning_rate": 0.0004483221431112305,
      "loss": 3.0534,
      "step": 11290
    },
    {
      "epoch": 43.29501915708812,
      "grad_norm": 0.4786457121372223,
      "learning_rate": 0.00044822957456267746,
      "loss": 3.0479,
      "step": 11300
    },
    {
      "epoch": 43.333333333333336,
      "grad_norm": 0.47771137952804565,
      "learning_rate": 0.0004481369327558329,
      "loss": 3.0408,
      "step": 11310
    },
    {
      "epoch": 43.371647509578544,
      "grad_norm": 0.479359894990921,
      "learning_rate": 0.000448044217724934,
      "loss": 3.0515,
      "step": 11320
    },
    {
      "epoch": 43.40996168582375,
      "grad_norm": 0.5015747547149658,
      "learning_rate": 0.0004479514295042445,
      "loss": 3.0571,
      "step": 11330
    },
    {
      "epoch": 43.44827586206897,
      "grad_norm": 0.722889244556427,
      "learning_rate": 0.0004478585681280557,
      "loss": 3.0695,
      "step": 11340
    },
    {
      "epoch": 43.486590038314176,
      "grad_norm": 0.4821029007434845,
      "learning_rate": 0.00044776563363068566,
      "loss": 3.0326,
      "step": 11350
    },
    {
      "epoch": 43.524904214559385,
      "grad_norm": 0.4817984402179718,
      "learning_rate": 0.00044767262604647956,
      "loss": 3.066,
      "step": 11360
    },
    {
      "epoch": 43.5632183908046,
      "grad_norm": 0.4758632481098175,
      "learning_rate": 0.00044757954540980947,
      "loss": 3.0518,
      "step": 11370
    },
    {
      "epoch": 43.60153256704981,
      "grad_norm": 0.5012981295585632,
      "learning_rate": 0.00044748639175507454,
      "loss": 3.0501,
      "step": 11380
    },
    {
      "epoch": 43.63984674329502,
      "grad_norm": 0.4706147015094757,
      "learning_rate": 0.00044739316511670105,
      "loss": 3.0509,
      "step": 11390
    },
    {
      "epoch": 43.67816091954023,
      "grad_norm": 0.48288458585739136,
      "learning_rate": 0.0004472998655291418,
      "loss": 3.0542,
      "step": 11400
    },
    {
      "epoch": 43.71647509578544,
      "grad_norm": 0.4921993911266327,
      "learning_rate": 0.0004472064930268772,
      "loss": 3.0536,
      "step": 11410
    },
    {
      "epoch": 43.75478927203065,
      "grad_norm": 0.49096059799194336,
      "learning_rate": 0.0004471130476444141,
      "loss": 3.0829,
      "step": 11420
    },
    {
      "epoch": 43.793103448275865,
      "grad_norm": 0.4805893003940582,
      "learning_rate": 0.0004470195294162863,
      "loss": 3.0819,
      "step": 11430
    },
    {
      "epoch": 43.83141762452107,
      "grad_norm": 0.5037803053855896,
      "learning_rate": 0.00044692593837705497,
      "loss": 3.0365,
      "step": 11440
    },
    {
      "epoch": 43.86973180076628,
      "grad_norm": 0.48057541251182556,
      "learning_rate": 0.0004468322745613077,
      "loss": 3.0724,
      "step": 11450
    },
    {
      "epoch": 43.9080459770115,
      "grad_norm": 0.46539363265037537,
      "learning_rate": 0.00044673853800365916,
      "loss": 3.0576,
      "step": 11460
    },
    {
      "epoch": 43.946360153256705,
      "grad_norm": 0.4681975543498993,
      "learning_rate": 0.00044664472873875093,
      "loss": 3.0497,
      "step": 11470
    },
    {
      "epoch": 43.984674329501914,
      "grad_norm": 0.5159205198287964,
      "learning_rate": 0.00044655084680125134,
      "loss": 3.0648,
      "step": 11480
    },
    {
      "epoch": 44.0,
      "eval_loss": 1.4576940536499023,
      "eval_runtime": 4.0801,
      "eval_samples_per_second": 4194.047,
      "eval_steps_per_second": 8.333,
      "step": 11484
    },
    {
      "epoch": 44.02298850574713,
      "grad_norm": 0.4944727122783661,
      "learning_rate": 0.0004464568922258559,
      "loss": 3.0182,
      "step": 11490
    },
    {
      "epoch": 44.06130268199234,
      "grad_norm": 0.48700499534606934,
      "learning_rate": 0.0004463628650472865,
      "loss": 3.0329,
      "step": 11500
    },
    {
      "epoch": 44.099616858237546,
      "grad_norm": 0.4852501153945923,
      "learning_rate": 0.00044626876530029226,
      "loss": 3.0393,
      "step": 11510
    },
    {
      "epoch": 44.13793103448276,
      "grad_norm": 0.48606640100479126,
      "learning_rate": 0.0004461745930196488,
      "loss": 3.0205,
      "step": 11520
    },
    {
      "epoch": 44.17624521072797,
      "grad_norm": 0.482623428106308,
      "learning_rate": 0.00044608034824015883,
      "loss": 3.044,
      "step": 11530
    },
    {
      "epoch": 44.21455938697318,
      "grad_norm": 0.5036938786506653,
      "learning_rate": 0.0004459860309966517,
      "loss": 3.0477,
      "step": 11540
    },
    {
      "epoch": 44.252873563218394,
      "grad_norm": 0.47155773639678955,
      "learning_rate": 0.00044589164132398347,
      "loss": 3.0565,
      "step": 11550
    },
    {
      "epoch": 44.2911877394636,
      "grad_norm": 0.4828896224498749,
      "learning_rate": 0.0004457971792570371,
      "loss": 3.059,
      "step": 11560
    },
    {
      "epoch": 44.32950191570881,
      "grad_norm": 0.4776667058467865,
      "learning_rate": 0.0004457026448307223,
      "loss": 3.0426,
      "step": 11570
    },
    {
      "epoch": 44.367816091954026,
      "grad_norm": 0.4896288812160492,
      "learning_rate": 0.00044560803807997554,
      "loss": 3.0524,
      "step": 11580
    },
    {
      "epoch": 44.406130268199234,
      "grad_norm": 0.48791977763175964,
      "learning_rate": 0.0004455133590397598,
      "loss": 3.0434,
      "step": 11590
    },
    {
      "epoch": 44.44444444444444,
      "grad_norm": 0.4977189302444458,
      "learning_rate": 0.0004454186077450651,
      "loss": 3.0429,
      "step": 11600
    },
    {
      "epoch": 44.48275862068966,
      "grad_norm": 0.507309079170227,
      "learning_rate": 0.00044532378423090793,
      "loss": 3.0596,
      "step": 11610
    },
    {
      "epoch": 44.52107279693487,
      "grad_norm": 0.49927818775177,
      "learning_rate": 0.00044522888853233147,
      "loss": 3.0402,
      "step": 11620
    },
    {
      "epoch": 44.559386973180075,
      "grad_norm": 0.4735530912876129,
      "learning_rate": 0.00044513392068440573,
      "loss": 3.0361,
      "step": 11630
    },
    {
      "epoch": 44.59770114942529,
      "grad_norm": 0.48956459760665894,
      "learning_rate": 0.00044503888072222733,
      "loss": 3.0592,
      "step": 11640
    },
    {
      "epoch": 44.6360153256705,
      "grad_norm": 0.5064771175384521,
      "learning_rate": 0.0004449437686809195,
      "loss": 3.046,
      "step": 11650
    },
    {
      "epoch": 44.67432950191571,
      "grad_norm": 0.5524215698242188,
      "learning_rate": 0.0004448485845956319,
      "loss": 3.0428,
      "step": 11660
    },
    {
      "epoch": 44.71264367816092,
      "grad_norm": 0.4685022234916687,
      "learning_rate": 0.00044475332850154144,
      "loss": 3.0455,
      "step": 11670
    },
    {
      "epoch": 44.75095785440613,
      "grad_norm": 0.47174271941185,
      "learning_rate": 0.0004446580004338509,
      "loss": 3.0654,
      "step": 11680
    },
    {
      "epoch": 44.78927203065134,
      "grad_norm": 0.48736175894737244,
      "learning_rate": 0.0004445626004277902,
      "loss": 3.024,
      "step": 11690
    },
    {
      "epoch": 44.827586206896555,
      "grad_norm": 0.49249014258384705,
      "learning_rate": 0.0004444671285186155,
      "loss": 3.0447,
      "step": 11700
    },
    {
      "epoch": 44.86590038314176,
      "grad_norm": 0.5130833387374878,
      "learning_rate": 0.00044437158474160967,
      "loss": 3.0502,
      "step": 11710
    },
    {
      "epoch": 44.90421455938697,
      "grad_norm": 0.5032146573066711,
      "learning_rate": 0.00044427596913208225,
      "loss": 3.0616,
      "step": 11720
    },
    {
      "epoch": 44.94252873563218,
      "grad_norm": 0.4894515573978424,
      "learning_rate": 0.0004441802817253692,
      "loss": 3.0745,
      "step": 11730
    },
    {
      "epoch": 44.980842911877396,
      "grad_norm": 0.513245701789856,
      "learning_rate": 0.00044408452255683295,
      "loss": 3.0396,
      "step": 11740
    },
    {
      "epoch": 45.0,
      "eval_loss": 1.454192042350769,
      "eval_runtime": 3.9041,
      "eval_samples_per_second": 4383.087,
      "eval_steps_per_second": 8.709,
      "step": 11745
    },
    {
      "epoch": 45.019157088122604,
      "grad_norm": 0.46949708461761475,
      "learning_rate": 0.00044398869166186264,
      "loss": 3.0437,
      "step": 11750
    },
    {
      "epoch": 45.05747126436781,
      "grad_norm": 0.48378607630729675,
      "learning_rate": 0.0004438927890758737,
      "loss": 3.0454,
      "step": 11760
    },
    {
      "epoch": 45.09578544061303,
      "grad_norm": 0.4868142604827881,
      "learning_rate": 0.0004437968148343083,
      "loss": 3.029,
      "step": 11770
    },
    {
      "epoch": 45.13409961685824,
      "grad_norm": 0.5181694030761719,
      "learning_rate": 0.00044370076897263486,
      "loss": 3.0399,
      "step": 11780
    },
    {
      "epoch": 45.172413793103445,
      "grad_norm": 0.492363840341568,
      "learning_rate": 0.00044360465152634834,
      "loss": 3.0272,
      "step": 11790
    },
    {
      "epoch": 45.21072796934866,
      "grad_norm": 0.47457775473594666,
      "learning_rate": 0.00044350846253097024,
      "loss": 3.0295,
      "step": 11800
    },
    {
      "epoch": 45.24904214559387,
      "grad_norm": 0.4923609793186188,
      "learning_rate": 0.0004434122020220484,
      "loss": 3.0378,
      "step": 11810
    },
    {
      "epoch": 45.28735632183908,
      "grad_norm": 0.4795927107334137,
      "learning_rate": 0.00044331587003515715,
      "loss": 3.0369,
      "step": 11820
    },
    {
      "epoch": 45.32567049808429,
      "grad_norm": 0.4645582139492035,
      "learning_rate": 0.00044321946660589727,
      "loss": 3.0232,
      "step": 11830
    },
    {
      "epoch": 45.3639846743295,
      "grad_norm": 0.49858829379081726,
      "learning_rate": 0.0004431229917698959,
      "loss": 3.0317,
      "step": 11840
    },
    {
      "epoch": 45.40229885057471,
      "grad_norm": 0.48673295974731445,
      "learning_rate": 0.00044302644556280633,
      "loss": 3.0377,
      "step": 11850
    },
    {
      "epoch": 45.440613026819925,
      "grad_norm": 0.509699821472168,
      "learning_rate": 0.0004429298280203088,
      "loss": 3.0458,
      "step": 11860
    },
    {
      "epoch": 45.47892720306513,
      "grad_norm": 0.4752320647239685,
      "learning_rate": 0.00044283313917810926,
      "loss": 3.0415,
      "step": 11870
    },
    {
      "epoch": 45.51724137931034,
      "grad_norm": 0.4847220480442047,
      "learning_rate": 0.00044273637907194056,
      "loss": 3.0459,
      "step": 11880
    },
    {
      "epoch": 45.55555555555556,
      "grad_norm": 0.48303714394569397,
      "learning_rate": 0.00044263954773756146,
      "loss": 3.0333,
      "step": 11890
    },
    {
      "epoch": 45.593869731800766,
      "grad_norm": 0.49903979897499084,
      "learning_rate": 0.0004425426452107572,
      "loss": 3.0573,
      "step": 11900
    },
    {
      "epoch": 45.632183908045974,
      "grad_norm": 0.48711079359054565,
      "learning_rate": 0.00044244567152733956,
      "loss": 3.0616,
      "step": 11910
    },
    {
      "epoch": 45.67049808429119,
      "grad_norm": 0.47881975769996643,
      "learning_rate": 0.0004423486267231463,
      "loss": 3.0415,
      "step": 11920
    },
    {
      "epoch": 45.7088122605364,
      "grad_norm": 0.4883398413658142,
      "learning_rate": 0.0004422515108340416,
      "loss": 3.0514,
      "step": 11930
    },
    {
      "epoch": 45.747126436781606,
      "grad_norm": 0.4857936501502991,
      "learning_rate": 0.00044215432389591583,
      "loss": 3.0503,
      "step": 11940
    },
    {
      "epoch": 45.78544061302682,
      "grad_norm": 0.45655304193496704,
      "learning_rate": 0.0004420570659446856,
      "loss": 3.0258,
      "step": 11950
    },
    {
      "epoch": 45.82375478927203,
      "grad_norm": 0.4879145920276642,
      "learning_rate": 0.00044195973701629415,
      "loss": 3.0442,
      "step": 11960
    },
    {
      "epoch": 45.86206896551724,
      "grad_norm": 0.5011438727378845,
      "learning_rate": 0.00044186233714671035,
      "loss": 3.0383,
      "step": 11970
    },
    {
      "epoch": 45.900383141762454,
      "grad_norm": 0.48584604263305664,
      "learning_rate": 0.0004417648663719297,
      "loss": 3.0459,
      "step": 11980
    },
    {
      "epoch": 45.93869731800766,
      "grad_norm": 0.4830171763896942,
      "learning_rate": 0.00044166732472797365,
      "loss": 3.0506,
      "step": 11990
    },
    {
      "epoch": 45.97701149425287,
      "grad_norm": 0.4647495150566101,
      "learning_rate": 0.00044156971225089016,
      "loss": 3.0343,
      "step": 12000
    },
    {
      "epoch": 46.0,
      "eval_loss": 1.454347848892212,
      "eval_runtime": 4.1666,
      "eval_samples_per_second": 4106.898,
      "eval_steps_per_second": 8.16,
      "step": 12006
    },
    {
      "epoch": 46.015325670498086,
      "grad_norm": 0.49283692240715027,
      "learning_rate": 0.0004414720289767531,
      "loss": 3.0485,
      "step": 12010
    },
    {
      "epoch": 46.053639846743295,
      "grad_norm": 0.49257126450538635,
      "learning_rate": 0.0004413742749416625,
      "loss": 3.0341,
      "step": 12020
    },
    {
      "epoch": 46.0919540229885,
      "grad_norm": 0.478304922580719,
      "learning_rate": 0.0004412764501817447,
      "loss": 3.0165,
      "step": 12030
    },
    {
      "epoch": 46.13026819923372,
      "grad_norm": 0.49394500255584717,
      "learning_rate": 0.0004411785547331521,
      "loss": 2.9973,
      "step": 12040
    },
    {
      "epoch": 46.16858237547893,
      "grad_norm": 0.49183011054992676,
      "learning_rate": 0.0004410805886320632,
      "loss": 3.0407,
      "step": 12050
    },
    {
      "epoch": 46.206896551724135,
      "grad_norm": 0.4784674346446991,
      "learning_rate": 0.0004409825519146826,
      "loss": 3.0523,
      "step": 12060
    },
    {
      "epoch": 46.24521072796935,
      "grad_norm": 0.4960241913795471,
      "learning_rate": 0.00044088444461724114,
      "loss": 3.0293,
      "step": 12070
    },
    {
      "epoch": 46.28352490421456,
      "grad_norm": 0.4522716701030731,
      "learning_rate": 0.00044078626677599553,
      "loss": 3.0107,
      "step": 12080
    },
    {
      "epoch": 46.32183908045977,
      "grad_norm": 0.4854159951210022,
      "learning_rate": 0.0004406880184272287,
      "loss": 3.0423,
      "step": 12090
    },
    {
      "epoch": 46.36015325670498,
      "grad_norm": 0.48962080478668213,
      "learning_rate": 0.0004405896996072496,
      "loss": 3.0408,
      "step": 12100
    },
    {
      "epoch": 46.39846743295019,
      "grad_norm": 0.4845530390739441,
      "learning_rate": 0.0004404913103523932,
      "loss": 3.0383,
      "step": 12110
    },
    {
      "epoch": 46.4367816091954,
      "grad_norm": 0.47553354501724243,
      "learning_rate": 0.00044039285069902057,
      "loss": 3.0417,
      "step": 12120
    },
    {
      "epoch": 46.475095785440615,
      "grad_norm": 0.49679437279701233,
      "learning_rate": 0.0004402943206835186,
      "loss": 3.0332,
      "step": 12130
    },
    {
      "epoch": 46.513409961685824,
      "grad_norm": 0.49273011088371277,
      "learning_rate": 0.0004401957203423005,
      "loss": 3.0092,
      "step": 12140
    },
    {
      "epoch": 46.55172413793103,
      "grad_norm": 0.4867888391017914,
      "learning_rate": 0.0004400970497118051,
      "loss": 3.0501,
      "step": 12150
    },
    {
      "epoch": 46.59003831417625,
      "grad_norm": 0.4821525812149048,
      "learning_rate": 0.00043999830882849764,
      "loss": 3.0518,
      "step": 12160
    },
    {
      "epoch": 46.628352490421456,
      "grad_norm": 0.45573297142982483,
      "learning_rate": 0.00043989949772886886,
      "loss": 3.022,
      "step": 12170
    },
    {
      "epoch": 46.666666666666664,
      "grad_norm": 0.4853936433792114,
      "learning_rate": 0.00043980061644943583,
      "loss": 3.0418,
      "step": 12180
    },
    {
      "epoch": 46.70498084291188,
      "grad_norm": 0.4566109776496887,
      "learning_rate": 0.00043970166502674133,
      "loss": 3.0531,
      "step": 12190
    },
    {
      "epoch": 46.74329501915709,
      "grad_norm": 0.4720090329647064,
      "learning_rate": 0.0004396026434973541,
      "loss": 3.0194,
      "step": 12200
    },
    {
      "epoch": 46.7816091954023,
      "grad_norm": 0.4767932891845703,
      "learning_rate": 0.00043950355189786893,
      "loss": 3.0642,
      "step": 12210
    },
    {
      "epoch": 46.81992337164751,
      "grad_norm": 0.47653961181640625,
      "learning_rate": 0.0004394043902649063,
      "loss": 3.0434,
      "step": 12220
    },
    {
      "epoch": 46.85823754789272,
      "grad_norm": 0.48375219106674194,
      "learning_rate": 0.0004393051586351127,
      "loss": 3.022,
      "step": 12230
    },
    {
      "epoch": 46.89655172413793,
      "grad_norm": 0.4789520502090454,
      "learning_rate": 0.00043920585704516045,
      "loss": 3.0311,
      "step": 12240
    },
    {
      "epoch": 46.934865900383144,
      "grad_norm": 0.4790440499782562,
      "learning_rate": 0.0004391064855317478,
      "loss": 3.0472,
      "step": 12250
    },
    {
      "epoch": 46.97318007662835,
      "grad_norm": 0.4980763792991638,
      "learning_rate": 0.00043900704413159865,
      "loss": 3.0375,
      "step": 12260
    },
    {
      "epoch": 47.0,
      "eval_loss": 1.4537400007247925,
      "eval_runtime": 4.727,
      "eval_samples_per_second": 3620.033,
      "eval_steps_per_second": 7.193,
      "step": 12267
    },
    {
      "epoch": 47.01149425287356,
      "grad_norm": 0.4726601541042328,
      "learning_rate": 0.0004389075328814629,
      "loss": 3.0323,
      "step": 12270
    },
    {
      "epoch": 47.04980842911878,
      "grad_norm": 0.46916118264198303,
      "learning_rate": 0.0004388079518181163,
      "loss": 2.9936,
      "step": 12280
    },
    {
      "epoch": 47.088122605363985,
      "grad_norm": 0.49130043387413025,
      "learning_rate": 0.00043870830097836013,
      "loss": 2.997,
      "step": 12290
    },
    {
      "epoch": 47.12643678160919,
      "grad_norm": 0.49792197346687317,
      "learning_rate": 0.00043860858039902187,
      "loss": 3.0272,
      "step": 12300
    },
    {
      "epoch": 47.16475095785441,
      "grad_norm": 0.4780653417110443,
      "learning_rate": 0.00043850879011695434,
      "loss": 3.0176,
      "step": 12310
    },
    {
      "epoch": 47.20306513409962,
      "grad_norm": 0.4871237874031067,
      "learning_rate": 0.00043840893016903635,
      "loss": 3.0048,
      "step": 12320
    },
    {
      "epoch": 47.241379310344826,
      "grad_norm": 0.4830506443977356,
      "learning_rate": 0.0004383090005921726,
      "loss": 3.0309,
      "step": 12330
    },
    {
      "epoch": 47.27969348659004,
      "grad_norm": 0.4811670780181885,
      "learning_rate": 0.00043820900142329324,
      "loss": 3.0228,
      "step": 12340
    },
    {
      "epoch": 47.31800766283525,
      "grad_norm": 0.4755518138408661,
      "learning_rate": 0.00043810893269935415,
      "loss": 3.036,
      "step": 12350
    },
    {
      "epoch": 47.35632183908046,
      "grad_norm": 0.4951093792915344,
      "learning_rate": 0.0004380087944573372,
      "loss": 3.0187,
      "step": 12360
    },
    {
      "epoch": 47.39463601532567,
      "grad_norm": 0.4841137230396271,
      "learning_rate": 0.0004379085867342496,
      "loss": 3.0355,
      "step": 12370
    },
    {
      "epoch": 47.43295019157088,
      "grad_norm": 0.46679332852363586,
      "learning_rate": 0.00043780830956712457,
      "loss": 3.0088,
      "step": 12380
    },
    {
      "epoch": 47.47126436781609,
      "grad_norm": 0.511269211769104,
      "learning_rate": 0.0004377079629930207,
      "loss": 3.0353,
      "step": 12390
    },
    {
      "epoch": 47.509578544061306,
      "grad_norm": 0.4786599278450012,
      "learning_rate": 0.0004376075470490224,
      "loss": 2.9955,
      "step": 12400
    },
    {
      "epoch": 47.547892720306514,
      "grad_norm": 0.49497032165527344,
      "learning_rate": 0.0004375070617722396,
      "loss": 3.0228,
      "step": 12410
    },
    {
      "epoch": 47.58620689655172,
      "grad_norm": 0.4708401560783386,
      "learning_rate": 0.00043740650719980813,
      "loss": 3.041,
      "step": 12420
    },
    {
      "epoch": 47.62452107279694,
      "grad_norm": 0.48418867588043213,
      "learning_rate": 0.000437305883368889,
      "loss": 3.0261,
      "step": 12430
    },
    {
      "epoch": 47.662835249042146,
      "grad_norm": 0.46410706639289856,
      "learning_rate": 0.00043720519031666924,
      "loss": 3.0339,
      "step": 12440
    },
    {
      "epoch": 47.701149425287355,
      "grad_norm": 0.46133315563201904,
      "learning_rate": 0.0004371044280803611,
      "loss": 3.0298,
      "step": 12450
    },
    {
      "epoch": 47.73946360153257,
      "grad_norm": 0.47163891792297363,
      "learning_rate": 0.00043700359669720265,
      "loss": 3.0515,
      "step": 12460
    },
    {
      "epoch": 47.77777777777778,
      "grad_norm": 0.5035664439201355,
      "learning_rate": 0.0004369026962044574,
      "loss": 3.0491,
      "step": 12470
    },
    {
      "epoch": 47.81609195402299,
      "grad_norm": 0.4772981107234955,
      "learning_rate": 0.00043680172663941445,
      "loss": 3.0474,
      "step": 12480
    },
    {
      "epoch": 47.8544061302682,
      "grad_norm": 0.494637668132782,
      "learning_rate": 0.00043670068803938837,
      "loss": 3.0374,
      "step": 12490
    },
    {
      "epoch": 47.89272030651341,
      "grad_norm": 0.47947224974632263,
      "learning_rate": 0.00043659958044171924,
      "loss": 3.0224,
      "step": 12500
    },
    {
      "epoch": 47.93103448275862,
      "grad_norm": 0.47967463731765747,
      "learning_rate": 0.00043649840388377273,
      "loss": 3.0322,
      "step": 12510
    },
    {
      "epoch": 47.969348659003835,
      "grad_norm": 0.5019855499267578,
      "learning_rate": 0.00043639715840293995,
      "loss": 3.0406,
      "step": 12520
    },
    {
      "epoch": 48.0,
      "eval_loss": 1.4538490772247314,
      "eval_runtime": 4.1841,
      "eval_samples_per_second": 4089.733,
      "eval_steps_per_second": 8.126,
      "step": 12528
    },
    {
      "epoch": 48.00766283524904,
      "grad_norm": 0.46857884526252747,
      "learning_rate": 0.00043629584403663747,
      "loss": 3.0397,
      "step": 12530
    },
    {
      "epoch": 48.04597701149425,
      "grad_norm": 0.48138314485549927,
      "learning_rate": 0.0004361944608223073,
      "loss": 2.9951,
      "step": 12540
    },
    {
      "epoch": 48.08429118773947,
      "grad_norm": 0.4834848642349243,
      "learning_rate": 0.00043609300879741687,
      "loss": 3.015,
      "step": 12550
    },
    {
      "epoch": 48.122605363984675,
      "grad_norm": 0.5042800903320312,
      "learning_rate": 0.0004359914879994592,
      "loss": 2.9898,
      "step": 12560
    },
    {
      "epoch": 48.160919540229884,
      "grad_norm": 0.4785517454147339,
      "learning_rate": 0.00043588989846595253,
      "loss": 3.0246,
      "step": 12570
    },
    {
      "epoch": 48.1992337164751,
      "grad_norm": 0.48594847321510315,
      "learning_rate": 0.00043578824023444046,
      "loss": 3.0315,
      "step": 12580
    },
    {
      "epoch": 48.23754789272031,
      "grad_norm": 0.49164628982543945,
      "learning_rate": 0.00043568651334249234,
      "loss": 3.0316,
      "step": 12590
    },
    {
      "epoch": 48.275862068965516,
      "grad_norm": 0.4953506588935852,
      "learning_rate": 0.0004355847178277025,
      "loss": 3.0296,
      "step": 12600
    },
    {
      "epoch": 48.31417624521073,
      "grad_norm": 0.47767502069473267,
      "learning_rate": 0.0004354828537276908,
      "loss": 3.0061,
      "step": 12610
    },
    {
      "epoch": 48.35249042145594,
      "grad_norm": 0.4823176860809326,
      "learning_rate": 0.00043538092108010246,
      "loss": 3.0283,
      "step": 12620
    },
    {
      "epoch": 48.39080459770115,
      "grad_norm": 0.4875190854072571,
      "learning_rate": 0.00043527891992260797,
      "loss": 3.036,
      "step": 12630
    },
    {
      "epoch": 48.42911877394636,
      "grad_norm": 0.5024794936180115,
      "learning_rate": 0.00043517685029290326,
      "loss": 3.0326,
      "step": 12640
    },
    {
      "epoch": 48.46743295019157,
      "grad_norm": 0.49718907475471497,
      "learning_rate": 0.00043507471222870936,
      "loss": 3.0223,
      "step": 12650
    },
    {
      "epoch": 48.50574712643678,
      "grad_norm": 0.48376184701919556,
      "learning_rate": 0.00043497250576777276,
      "loss": 3.0342,
      "step": 12660
    },
    {
      "epoch": 48.54406130268199,
      "grad_norm": 0.47588855028152466,
      "learning_rate": 0.0004348702309478652,
      "loss": 3.006,
      "step": 12670
    },
    {
      "epoch": 48.582375478927204,
      "grad_norm": 0.46421289443969727,
      "learning_rate": 0.00043476788780678357,
      "loss": 3.0048,
      "step": 12680
    },
    {
      "epoch": 48.62068965517241,
      "grad_norm": 0.4916709363460541,
      "learning_rate": 0.0004346654763823501,
      "loss": 3.0342,
      "step": 12690
    },
    {
      "epoch": 48.65900383141762,
      "grad_norm": 0.4796141982078552,
      "learning_rate": 0.0004345629967124124,
      "loss": 3.0335,
      "step": 12700
    },
    {
      "epoch": 48.69731800766284,
      "grad_norm": 0.503361701965332,
      "learning_rate": 0.000434460448834843,
      "loss": 3.0207,
      "step": 12710
    },
    {
      "epoch": 48.735632183908045,
      "grad_norm": 0.49730128049850464,
      "learning_rate": 0.00043435783278753984,
      "loss": 3.0077,
      "step": 12720
    },
    {
      "epoch": 48.77394636015325,
      "grad_norm": 0.47554031014442444,
      "learning_rate": 0.00043425514860842595,
      "loss": 3.0223,
      "step": 12730
    },
    {
      "epoch": 48.81226053639847,
      "grad_norm": 0.47064968943595886,
      "learning_rate": 0.00043415239633544974,
      "loss": 3.0288,
      "step": 12740
    },
    {
      "epoch": 48.85057471264368,
      "grad_norm": 0.4588174521923065,
      "learning_rate": 0.0004340495760065845,
      "loss": 3.018,
      "step": 12750
    },
    {
      "epoch": 48.888888888888886,
      "grad_norm": 0.4998205602169037,
      "learning_rate": 0.0004339466876598288,
      "loss": 3.0403,
      "step": 12760
    },
    {
      "epoch": 48.9272030651341,
      "grad_norm": 0.4814964532852173,
      "learning_rate": 0.0004338437313332065,
      "loss": 3.0233,
      "step": 12770
    },
    {
      "epoch": 48.96551724137931,
      "grad_norm": 0.48305752873420715,
      "learning_rate": 0.0004337407070647662,
      "loss": 3.0137,
      "step": 12780
    },
    {
      "epoch": 49.0,
      "eval_loss": 1.4524495601654053,
      "eval_runtime": 3.9202,
      "eval_samples_per_second": 4365.079,
      "eval_steps_per_second": 8.673,
      "step": 12789
    },
    {
      "epoch": 49.00383141762452,
      "grad_norm": 0.49143239855766296,
      "learning_rate": 0.000433637614892582,
      "loss": 3.0184,
      "step": 12790
    },
    {
      "epoch": 49.04214559386973,
      "grad_norm": 0.46008220314979553,
      "learning_rate": 0.000433534454854753,
      "loss": 2.9887,
      "step": 12800
    },
    {
      "epoch": 49.08045977011494,
      "grad_norm": 0.4720810353755951,
      "learning_rate": 0.0004334312269894032,
      "loss": 3.011,
      "step": 12810
    },
    {
      "epoch": 49.11877394636015,
      "grad_norm": 0.4982871413230896,
      "learning_rate": 0.00043332793133468185,
      "loss": 3.0144,
      "step": 12820
    },
    {
      "epoch": 49.157088122605366,
      "grad_norm": 0.4868525564670563,
      "learning_rate": 0.00043322456792876306,
      "loss": 2.9986,
      "step": 12830
    },
    {
      "epoch": 49.195402298850574,
      "grad_norm": 0.4757811725139618,
      "learning_rate": 0.00043312113680984635,
      "loss": 3.0054,
      "step": 12840
    },
    {
      "epoch": 49.23371647509578,
      "grad_norm": 0.48789262771606445,
      "learning_rate": 0.0004330176380161558,
      "loss": 2.9998,
      "step": 12850
    },
    {
      "epoch": 49.272030651341,
      "grad_norm": 0.4643022418022156,
      "learning_rate": 0.00043291407158594074,
      "loss": 2.992,
      "step": 12860
    },
    {
      "epoch": 49.310344827586206,
      "grad_norm": 0.4754021465778351,
      "learning_rate": 0.0004328104375574756,
      "loss": 3.0025,
      "step": 12870
    },
    {
      "epoch": 49.348659003831415,
      "grad_norm": 0.48621344566345215,
      "learning_rate": 0.0004327067359690596,
      "loss": 3.0015,
      "step": 12880
    },
    {
      "epoch": 49.38697318007663,
      "grad_norm": 0.4908672273159027,
      "learning_rate": 0.00043260296685901703,
      "loss": 3.0055,
      "step": 12890
    },
    {
      "epoch": 49.42528735632184,
      "grad_norm": 0.4561474621295929,
      "learning_rate": 0.0004324991302656971,
      "loss": 2.9937,
      "step": 12900
    },
    {
      "epoch": 49.46360153256705,
      "grad_norm": 0.5017191767692566,
      "learning_rate": 0.0004323952262274739,
      "loss": 3.0192,
      "step": 12910
    },
    {
      "epoch": 49.50191570881226,
      "grad_norm": 0.45648834109306335,
      "learning_rate": 0.00043229125478274656,
      "loss": 3.0269,
      "step": 12920
    },
    {
      "epoch": 49.54022988505747,
      "grad_norm": 0.48459097743034363,
      "learning_rate": 0.00043218721596993904,
      "loss": 3.0237,
      "step": 12930
    },
    {
      "epoch": 49.57854406130268,
      "grad_norm": 0.4853278696537018,
      "learning_rate": 0.0004320831098275002,
      "loss": 3.0249,
      "step": 12940
    },
    {
      "epoch": 49.616858237547895,
      "grad_norm": 0.49223288893699646,
      "learning_rate": 0.00043197893639390384,
      "loss": 2.9874,
      "step": 12950
    },
    {
      "epoch": 49.6551724137931,
      "grad_norm": 0.4891969561576843,
      "learning_rate": 0.0004318746957076486,
      "loss": 3.0253,
      "step": 12960
    },
    {
      "epoch": 49.69348659003831,
      "grad_norm": 0.4781923294067383,
      "learning_rate": 0.000431770387807258,
      "loss": 3.02,
      "step": 12970
    },
    {
      "epoch": 49.73180076628353,
      "grad_norm": 0.4864003658294678,
      "learning_rate": 0.0004316660127312803,
      "loss": 3.0236,
      "step": 12980
    },
    {
      "epoch": 49.770114942528735,
      "grad_norm": 0.4862186014652252,
      "learning_rate": 0.0004315615705182887,
      "loss": 3.0206,
      "step": 12990
    },
    {
      "epoch": 49.808429118773944,
      "grad_norm": 0.4897310733795166,
      "learning_rate": 0.0004314570612068811,
      "loss": 3.0229,
      "step": 13000
    },
    {
      "epoch": 49.84674329501916,
      "grad_norm": 0.5118080377578735,
      "learning_rate": 0.0004313524848356803,
      "loss": 3.0127,
      "step": 13010
    },
    {
      "epoch": 49.88505747126437,
      "grad_norm": 0.4701251983642578,
      "learning_rate": 0.00043124784144333385,
      "loss": 3.0482,
      "step": 13020
    },
    {
      "epoch": 49.923371647509576,
      "grad_norm": 0.4813195466995239,
      "learning_rate": 0.0004311431310685141,
      "loss": 3.0358,
      "step": 13030
    },
    {
      "epoch": 49.96168582375479,
      "grad_norm": 0.49098658561706543,
      "learning_rate": 0.00043103835374991805,
      "loss": 3.0033,
      "step": 13040
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.47400522232055664,
      "learning_rate": 0.0004309335095262675,
      "loss": 3.039,
      "step": 13050
    },
    {
      "epoch": 50.0,
      "eval_loss": 1.4537460803985596,
      "eval_runtime": 4.167,
      "eval_samples_per_second": 4106.519,
      "eval_steps_per_second": 8.159,
      "step": 13050
    },
    {
      "epoch": 50.03831417624521,
      "grad_norm": 0.5068673491477966,
      "learning_rate": 0.0004308285984363091,
      "loss": 3.0028,
      "step": 13060
    },
    {
      "epoch": 50.076628352490424,
      "grad_norm": 0.4740736782550812,
      "learning_rate": 0.000430723620518814,
      "loss": 3.0094,
      "step": 13070
    },
    {
      "epoch": 50.11494252873563,
      "grad_norm": 0.4855150580406189,
      "learning_rate": 0.00043061857581257814,
      "loss": 2.9942,
      "step": 13080
    },
    {
      "epoch": 50.15325670498084,
      "grad_norm": 0.4853774607181549,
      "learning_rate": 0.0004305134643564221,
      "loss": 3.0073,
      "step": 13090
    },
    {
      "epoch": 50.191570881226056,
      "grad_norm": 0.47534140944480896,
      "learning_rate": 0.0004304082861891913,
      "loss": 2.9914,
      "step": 13100
    },
    {
      "epoch": 50.229885057471265,
      "grad_norm": 0.4786510169506073,
      "learning_rate": 0.00043030304134975564,
      "loss": 3.0276,
      "step": 13110
    },
    {
      "epoch": 50.26819923371647,
      "grad_norm": 0.47732362151145935,
      "learning_rate": 0.0004301977298770096,
      "loss": 3.0144,
      "step": 13120
    },
    {
      "epoch": 50.30651340996169,
      "grad_norm": 0.49563202261924744,
      "learning_rate": 0.0004300923518098725,
      "loss": 3.0147,
      "step": 13130
    },
    {
      "epoch": 50.3448275862069,
      "grad_norm": 0.48350396752357483,
      "learning_rate": 0.00042998690718728817,
      "loss": 3.0187,
      "step": 13140
    },
    {
      "epoch": 50.383141762452105,
      "grad_norm": 0.501193106174469,
      "learning_rate": 0.0004298813960482249,
      "loss": 2.9877,
      "step": 13150
    },
    {
      "epoch": 50.42145593869732,
      "grad_norm": 0.4931327700614929,
      "learning_rate": 0.00042977581843167586,
      "loss": 3.0169,
      "step": 13160
    },
    {
      "epoch": 50.45977011494253,
      "grad_norm": 0.4903852641582489,
      "learning_rate": 0.0004296701743766585,
      "loss": 3.002,
      "step": 13170
    },
    {
      "epoch": 50.49808429118774,
      "grad_norm": 0.48516765236854553,
      "learning_rate": 0.000429564463922215,
      "loss": 3.0057,
      "step": 13180
    },
    {
      "epoch": 50.53639846743295,
      "grad_norm": 0.4909551441669464,
      "learning_rate": 0.000429458687107412,
      "loss": 2.9935,
      "step": 13190
    },
    {
      "epoch": 50.57471264367816,
      "grad_norm": 0.481803297996521,
      "learning_rate": 0.00042935284397134065,
      "loss": 2.998,
      "step": 13200
    },
    {
      "epoch": 50.61302681992337,
      "grad_norm": 0.47013261914253235,
      "learning_rate": 0.0004292469345531167,
      "loss": 3.0017,
      "step": 13210
    },
    {
      "epoch": 50.651340996168585,
      "grad_norm": 0.4917968809604645,
      "learning_rate": 0.00042914095889188034,
      "loss": 3.0127,
      "step": 13220
    },
    {
      "epoch": 50.689655172413794,
      "grad_norm": 0.469512939453125,
      "learning_rate": 0.0004290349170267961,
      "loss": 3.0101,
      "step": 13230
    },
    {
      "epoch": 50.727969348659,
      "grad_norm": 0.477455198764801,
      "learning_rate": 0.00042892880899705335,
      "loss": 3.01,
      "step": 13240
    },
    {
      "epoch": 50.76628352490422,
      "grad_norm": 0.5007646679878235,
      "learning_rate": 0.0004288226348418656,
      "loss": 3.0205,
      "step": 13250
    },
    {
      "epoch": 50.804597701149426,
      "grad_norm": 0.5010150074958801,
      "learning_rate": 0.0004287163946004707,
      "loss": 3.0067,
      "step": 13260
    },
    {
      "epoch": 50.842911877394634,
      "grad_norm": 0.4801812469959259,
      "learning_rate": 0.00042861008831213127,
      "loss": 3.0073,
      "step": 13270
    },
    {
      "epoch": 50.88122605363985,
      "grad_norm": 0.5070015788078308,
      "learning_rate": 0.00042850371601613413,
      "loss": 3.0061,
      "step": 13280
    },
    {
      "epoch": 50.91954022988506,
      "grad_norm": 0.4949154257774353,
      "learning_rate": 0.0004283972777517906,
      "loss": 3.0182,
      "step": 13290
    },
    {
      "epoch": 50.95785440613027,
      "grad_norm": 0.488504022359848,
      "learning_rate": 0.00042829077355843617,
      "loss": 2.9987,
      "step": 13300
    },
    {
      "epoch": 50.99616858237548,
      "grad_norm": 0.48914289474487305,
      "learning_rate": 0.00042818420347543097,
      "loss": 3.0021,
      "step": 13310
    },
    {
      "epoch": 51.0,
      "eval_loss": 1.4524884223937988,
      "eval_runtime": 4.1115,
      "eval_samples_per_second": 4161.942,
      "eval_steps_per_second": 8.269,
      "step": 13311
    },
    {
      "epoch": 51.03448275862069,
      "grad_norm": 0.5055219531059265,
      "learning_rate": 0.0004280775675421593,
      "loss": 3.0014,
      "step": 13320
    },
    {
      "epoch": 51.0727969348659,
      "grad_norm": 0.48001718521118164,
      "learning_rate": 0.00042797086579802974,
      "loss": 2.985,
      "step": 13330
    },
    {
      "epoch": 51.111111111111114,
      "grad_norm": 0.47677046060562134,
      "learning_rate": 0.0004278640982824754,
      "loss": 2.9746,
      "step": 13340
    },
    {
      "epoch": 51.14942528735632,
      "grad_norm": 0.5071274638175964,
      "learning_rate": 0.0004277572650349536,
      "loss": 3.004,
      "step": 13350
    },
    {
      "epoch": 51.18773946360153,
      "grad_norm": 0.4748132526874542,
      "learning_rate": 0.00042765036609494593,
      "loss": 3.0066,
      "step": 13360
    },
    {
      "epoch": 51.22605363984675,
      "grad_norm": 0.47997453808784485,
      "learning_rate": 0.00042754340150195826,
      "loss": 3.0148,
      "step": 13370
    },
    {
      "epoch": 51.264367816091955,
      "grad_norm": 0.4798702597618103,
      "learning_rate": 0.0004274363712955207,
      "loss": 2.9934,
      "step": 13380
    },
    {
      "epoch": 51.30268199233716,
      "grad_norm": 0.4959869384765625,
      "learning_rate": 0.00042732927551518765,
      "loss": 3.0331,
      "step": 13390
    },
    {
      "epoch": 51.34099616858238,
      "grad_norm": 0.494902640581131,
      "learning_rate": 0.0004272221142005378,
      "loss": 2.9962,
      "step": 13400
    },
    {
      "epoch": 51.37931034482759,
      "grad_norm": 0.48960375785827637,
      "learning_rate": 0.00042711488739117387,
      "loss": 2.9868,
      "step": 13410
    },
    {
      "epoch": 51.417624521072796,
      "grad_norm": 0.48472729325294495,
      "learning_rate": 0.00042700759512672305,
      "loss": 2.9796,
      "step": 13420
    },
    {
      "epoch": 51.45593869731801,
      "grad_norm": 0.5184029936790466,
      "learning_rate": 0.00042690023744683647,
      "loss": 3.0084,
      "step": 13430
    },
    {
      "epoch": 51.49425287356322,
      "grad_norm": 0.4712388515472412,
      "learning_rate": 0.0004267928143911897,
      "loss": 2.997,
      "step": 13440
    },
    {
      "epoch": 51.53256704980843,
      "grad_norm": 0.48251989483833313,
      "learning_rate": 0.0004266853259994821,
      "loss": 2.9996,
      "step": 13450
    },
    {
      "epoch": 51.57088122605364,
      "grad_norm": 0.4981166422367096,
      "learning_rate": 0.0004265777723114374,
      "loss": 2.9899,
      "step": 13460
    },
    {
      "epoch": 51.60919540229885,
      "grad_norm": 0.46598973870277405,
      "learning_rate": 0.0004264701533668036,
      "loss": 3.0136,
      "step": 13470
    },
    {
      "epoch": 51.64750957854406,
      "grad_norm": 0.47629687190055847,
      "learning_rate": 0.00042636246920535264,
      "loss": 3.0076,
      "step": 13480
    },
    {
      "epoch": 51.68582375478927,
      "grad_norm": 0.475033700466156,
      "learning_rate": 0.00042625471986688056,
      "loss": 2.9912,
      "step": 13490
    },
    {
      "epoch": 51.724137931034484,
      "grad_norm": 0.4735074043273926,
      "learning_rate": 0.0004261469053912075,
      "loss": 3.0176,
      "step": 13500
    },
    {
      "epoch": 51.76245210727969,
      "grad_norm": 0.48880162835121155,
      "learning_rate": 0.00042603902581817764,
      "loss": 3.0033,
      "step": 13510
    },
    {
      "epoch": 51.8007662835249,
      "grad_norm": 0.4664541780948639,
      "learning_rate": 0.00042593108118765935,
      "loss": 2.9978,
      "step": 13520
    },
    {
      "epoch": 51.839080459770116,
      "grad_norm": 0.4839923679828644,
      "learning_rate": 0.00042582307153954494,
      "loss": 3.0193,
      "step": 13530
    },
    {
      "epoch": 51.877394636015325,
      "grad_norm": 0.4989109933376312,
      "learning_rate": 0.0004257149969137507,
      "loss": 3.002,
      "step": 13540
    },
    {
      "epoch": 51.91570881226053,
      "grad_norm": 0.5059329867362976,
      "learning_rate": 0.00042560685735021716,
      "loss": 3.0134,
      "step": 13550
    },
    {
      "epoch": 51.95402298850575,
      "grad_norm": 0.47835537791252136,
      "learning_rate": 0.0004254986528889085,
      "loss": 2.9876,
      "step": 13560
    },
    {
      "epoch": 51.99233716475096,
      "grad_norm": 0.4828948378562927,
      "learning_rate": 0.0004253903835698131,
      "loss": 3.0011,
      "step": 13570
    },
    {
      "epoch": 52.0,
      "eval_loss": 1.4508142471313477,
      "eval_runtime": 4.0724,
      "eval_samples_per_second": 4201.991,
      "eval_steps_per_second": 8.349,
      "step": 13572
    },
    {
      "epoch": 52.030651340996165,
      "grad_norm": 0.4599575102329254,
      "learning_rate": 0.00042528204943294344,
      "loss": 2.999,
      "step": 13580
    },
    {
      "epoch": 52.06896551724138,
      "grad_norm": 0.4729829430580139,
      "learning_rate": 0.00042517365051833564,
      "loss": 2.985,
      "step": 13590
    },
    {
      "epoch": 52.10727969348659,
      "grad_norm": 0.4818941056728363,
      "learning_rate": 0.00042506518686605,
      "loss": 2.9961,
      "step": 13600
    },
    {
      "epoch": 52.1455938697318,
      "grad_norm": 0.5615375638008118,
      "learning_rate": 0.00042495665851617056,
      "loss": 2.99,
      "step": 13610
    },
    {
      "epoch": 52.18390804597701,
      "grad_norm": 0.4887631833553314,
      "learning_rate": 0.00042484806550880543,
      "loss": 2.983,
      "step": 13620
    },
    {
      "epoch": 52.22222222222222,
      "grad_norm": 0.4860580563545227,
      "learning_rate": 0.00042473940788408663,
      "loss": 2.9784,
      "step": 13630
    },
    {
      "epoch": 52.26053639846743,
      "grad_norm": 0.46515703201293945,
      "learning_rate": 0.0004246306856821698,
      "loss": 2.9946,
      "step": 13640
    },
    {
      "epoch": 52.298850574712645,
      "grad_norm": 0.4774560332298279,
      "learning_rate": 0.00042452189894323476,
      "loss": 3.0158,
      "step": 13650
    },
    {
      "epoch": 52.337164750957854,
      "grad_norm": 0.4936988353729248,
      "learning_rate": 0.0004244130477074851,
      "loss": 2.9799,
      "step": 13660
    },
    {
      "epoch": 52.37547892720306,
      "grad_norm": 0.4931752383708954,
      "learning_rate": 0.00042430413201514813,
      "loss": 3.0295,
      "step": 13670
    },
    {
      "epoch": 52.41379310344828,
      "grad_norm": 0.5021185874938965,
      "learning_rate": 0.00042419515190647496,
      "loss": 2.9875,
      "step": 13680
    },
    {
      "epoch": 52.452107279693486,
      "grad_norm": 0.491744726896286,
      "learning_rate": 0.00042408610742174067,
      "loss": 3.005,
      "step": 13690
    },
    {
      "epoch": 52.490421455938694,
      "grad_norm": 0.46867635846138,
      "learning_rate": 0.0004239769986012442,
      "loss": 2.9742,
      "step": 13700
    },
    {
      "epoch": 52.52873563218391,
      "grad_norm": 0.4963719844818115,
      "learning_rate": 0.00042386782548530785,
      "loss": 3.004,
      "step": 13710
    },
    {
      "epoch": 52.56704980842912,
      "grad_norm": 0.4674939811229706,
      "learning_rate": 0.000423758588114278,
      "loss": 2.9928,
      "step": 13720
    },
    {
      "epoch": 52.60536398467433,
      "grad_norm": 0.49145686626434326,
      "learning_rate": 0.000423649286528525,
      "loss": 2.9968,
      "step": 13730
    },
    {
      "epoch": 52.64367816091954,
      "grad_norm": 0.5118900537490845,
      "learning_rate": 0.0004235399207684423,
      "loss": 2.9964,
      "step": 13740
    },
    {
      "epoch": 52.68199233716475,
      "grad_norm": 0.4740794003009796,
      "learning_rate": 0.0004234304908744476,
      "loss": 2.9789,
      "step": 13750
    },
    {
      "epoch": 52.72030651340996,
      "grad_norm": 0.4611814022064209,
      "learning_rate": 0.00042332099688698223,
      "loss": 2.9783,
      "step": 13760
    },
    {
      "epoch": 52.758620689655174,
      "grad_norm": 0.46326667070388794,
      "learning_rate": 0.00042321143884651094,
      "loss": 2.9993,
      "step": 13770
    },
    {
      "epoch": 52.79693486590038,
      "grad_norm": 0.4779576063156128,
      "learning_rate": 0.00042310181679352225,
      "loss": 3.0029,
      "step": 13780
    },
    {
      "epoch": 52.83524904214559,
      "grad_norm": 0.49415794014930725,
      "learning_rate": 0.0004229921307685286,
      "loss": 3.0227,
      "step": 13790
    },
    {
      "epoch": 52.87356321839081,
      "grad_norm": 0.4797985851764679,
      "learning_rate": 0.00042288238081206577,
      "loss": 3.0114,
      "step": 13800
    },
    {
      "epoch": 52.911877394636015,
      "grad_norm": 0.5030474066734314,
      "learning_rate": 0.0004227725669646933,
      "loss": 2.9873,
      "step": 13810
    },
    {
      "epoch": 52.95019157088122,
      "grad_norm": 0.4606779217720032,
      "learning_rate": 0.00042266268926699426,
      "loss": 3.0055,
      "step": 13820
    },
    {
      "epoch": 52.98850574712644,
      "grad_norm": 0.45901763439178467,
      "learning_rate": 0.0004225527477595754,
      "loss": 3.0109,
      "step": 13830
    },
    {
      "epoch": 53.0,
      "eval_loss": 1.450954556465149,
      "eval_runtime": 4.0388,
      "eval_samples_per_second": 4236.927,
      "eval_steps_per_second": 8.418,
      "step": 13833
    },
    {
      "epoch": 53.02681992337165,
      "grad_norm": 0.466410756111145,
      "learning_rate": 0.00042244274248306703,
      "loss": 2.976,
      "step": 13840
    },
    {
      "epoch": 53.065134099616856,
      "grad_norm": 0.4724635183811188,
      "learning_rate": 0.00042233267347812297,
      "loss": 2.9864,
      "step": 13850
    },
    {
      "epoch": 53.10344827586207,
      "grad_norm": 1.3803743124008179,
      "learning_rate": 0.00042222254078542077,
      "loss": 2.9761,
      "step": 13860
    },
    {
      "epoch": 53.14176245210728,
      "grad_norm": 0.4828786551952362,
      "learning_rate": 0.00042211234444566127,
      "loss": 2.9671,
      "step": 13870
    },
    {
      "epoch": 53.18007662835249,
      "grad_norm": 0.49543482065200806,
      "learning_rate": 0.0004220020844995689,
      "loss": 2.9934,
      "step": 13880
    },
    {
      "epoch": 53.2183908045977,
      "grad_norm": 0.49550727009773254,
      "learning_rate": 0.0004218917609878917,
      "loss": 2.9936,
      "step": 13890
    },
    {
      "epoch": 53.25670498084291,
      "grad_norm": 0.4779115617275238,
      "learning_rate": 0.0004217813739514012,
      "loss": 2.991,
      "step": 13900
    },
    {
      "epoch": 53.29501915708812,
      "grad_norm": 0.48571842908859253,
      "learning_rate": 0.0004216709234308923,
      "loss": 2.9702,
      "step": 13910
    },
    {
      "epoch": 53.333333333333336,
      "grad_norm": 0.4835660755634308,
      "learning_rate": 0.00042156040946718344,
      "loss": 3.0031,
      "step": 13920
    },
    {
      "epoch": 53.371647509578544,
      "grad_norm": 0.500789225101471,
      "learning_rate": 0.00042144983210111643,
      "loss": 3.0011,
      "step": 13930
    },
    {
      "epoch": 53.40996168582375,
      "grad_norm": 0.4845542311668396,
      "learning_rate": 0.00042133919137355655,
      "loss": 2.9984,
      "step": 13940
    },
    {
      "epoch": 53.44827586206897,
      "grad_norm": 0.483594685792923,
      "learning_rate": 0.00042122848732539255,
      "loss": 2.9899,
      "step": 13950
    },
    {
      "epoch": 53.486590038314176,
      "grad_norm": 0.48279738426208496,
      "learning_rate": 0.0004211177199975365,
      "loss": 3.0061,
      "step": 13960
    },
    {
      "epoch": 53.524904214559385,
      "grad_norm": 0.4667781591415405,
      "learning_rate": 0.00042100688943092396,
      "loss": 2.9868,
      "step": 13970
    },
    {
      "epoch": 53.5632183908046,
      "grad_norm": 0.47377535700798035,
      "learning_rate": 0.0004208959956665137,
      "loss": 2.9652,
      "step": 13980
    },
    {
      "epoch": 53.60153256704981,
      "grad_norm": 0.4694085121154785,
      "learning_rate": 0.0004207850387452881,
      "loss": 2.9798,
      "step": 13990
    },
    {
      "epoch": 53.63984674329502,
      "grad_norm": 0.4863240122795105,
      "learning_rate": 0.00042067401870825245,
      "loss": 2.9872,
      "step": 14000
    },
    {
      "epoch": 53.67816091954023,
      "grad_norm": 0.47476720809936523,
      "learning_rate": 0.00042056293559643586,
      "loss": 3.0166,
      "step": 14010
    },
    {
      "epoch": 53.71647509578544,
      "grad_norm": 0.4607677161693573,
      "learning_rate": 0.00042045178945089047,
      "loss": 2.9982,
      "step": 14020
    },
    {
      "epoch": 53.75478927203065,
      "grad_norm": 0.4969530403614044,
      "learning_rate": 0.00042034058031269173,
      "loss": 3.0275,
      "step": 14030
    },
    {
      "epoch": 53.793103448275865,
      "grad_norm": 0.4745502471923828,
      "learning_rate": 0.0004202293082229385,
      "loss": 2.9776,
      "step": 14040
    },
    {
      "epoch": 53.83141762452107,
      "grad_norm": 0.4807031452655792,
      "learning_rate": 0.0004201179732227526,
      "loss": 2.9899,
      "step": 14050
    },
    {
      "epoch": 53.86973180076628,
      "grad_norm": 0.47507500648498535,
      "learning_rate": 0.00042000657535327956,
      "loss": 2.9922,
      "step": 14060
    },
    {
      "epoch": 53.9080459770115,
      "grad_norm": 0.5036388039588928,
      "learning_rate": 0.0004198951146556878,
      "loss": 2.9856,
      "step": 14070
    },
    {
      "epoch": 53.946360153256705,
      "grad_norm": 0.4741889238357544,
      "learning_rate": 0.00041978359117116905,
      "loss": 3.0029,
      "step": 14080
    },
    {
      "epoch": 53.984674329501914,
      "grad_norm": 0.47680923342704773,
      "learning_rate": 0.00041967200494093823,
      "loss": 3.0091,
      "step": 14090
    },
    {
      "epoch": 54.0,
      "eval_loss": 1.449590802192688,
      "eval_runtime": 3.909,
      "eval_samples_per_second": 4377.589,
      "eval_steps_per_second": 8.698,
      "step": 14094
    },
    {
      "epoch": 54.02298850574713,
      "grad_norm": 0.4835977256298065,
      "learning_rate": 0.0004195603560062334,
      "loss": 2.9722,
      "step": 14100
    },
    {
      "epoch": 54.06130268199234,
      "grad_norm": 0.48464155197143555,
      "learning_rate": 0.0004194486444083161,
      "loss": 2.9905,
      "step": 14110
    },
    {
      "epoch": 54.099616858237546,
      "grad_norm": 0.4651314616203308,
      "learning_rate": 0.00041933687018847057,
      "loss": 2.9593,
      "step": 14120
    },
    {
      "epoch": 54.13793103448276,
      "grad_norm": 0.49153172969818115,
      "learning_rate": 0.0004192250333880045,
      "loss": 2.978,
      "step": 14130
    },
    {
      "epoch": 54.17624521072797,
      "grad_norm": 0.4897738993167877,
      "learning_rate": 0.00041911313404824856,
      "loss": 2.9685,
      "step": 14140
    },
    {
      "epoch": 54.21455938697318,
      "grad_norm": 0.47951868176460266,
      "learning_rate": 0.0004190011722105567,
      "loss": 2.9848,
      "step": 14150
    },
    {
      "epoch": 54.252873563218394,
      "grad_norm": 0.49292677640914917,
      "learning_rate": 0.0004188891479163057,
      "loss": 2.9717,
      "step": 14160
    },
    {
      "epoch": 54.2911877394636,
      "grad_norm": 0.464504599571228,
      "learning_rate": 0.00041877706120689573,
      "loss": 2.9694,
      "step": 14170
    },
    {
      "epoch": 54.32950191570881,
      "grad_norm": 0.48204344511032104,
      "learning_rate": 0.0004186649121237498,
      "loss": 2.9737,
      "step": 14180
    },
    {
      "epoch": 54.367816091954026,
      "grad_norm": 0.5044743418693542,
      "learning_rate": 0.00041855270070831406,
      "loss": 2.9885,
      "step": 14190
    },
    {
      "epoch": 54.406130268199234,
      "grad_norm": 0.47364136576652527,
      "learning_rate": 0.00041844042700205764,
      "loss": 3.0061,
      "step": 14200
    },
    {
      "epoch": 54.44444444444444,
      "grad_norm": 0.4701411724090576,
      "learning_rate": 0.00041832809104647286,
      "loss": 2.9836,
      "step": 14210
    },
    {
      "epoch": 54.48275862068966,
      "grad_norm": 0.4903891384601593,
      "learning_rate": 0.0004182156928830748,
      "loss": 2.9632,
      "step": 14220
    },
    {
      "epoch": 54.52107279693487,
      "grad_norm": 0.4875530004501343,
      "learning_rate": 0.0004181032325534018,
      "loss": 2.9737,
      "step": 14230
    },
    {
      "epoch": 54.559386973180075,
      "grad_norm": 0.4826752543449402,
      "learning_rate": 0.0004179907100990148,
      "loss": 2.9804,
      "step": 14240
    },
    {
      "epoch": 54.59770114942529,
      "grad_norm": 0.4806857705116272,
      "learning_rate": 0.000417878125561498,
      "loss": 2.9806,
      "step": 14250
    },
    {
      "epoch": 54.6360153256705,
      "grad_norm": 0.4871588349342346,
      "learning_rate": 0.0004177654789824585,
      "loss": 2.9865,
      "step": 14260
    },
    {
      "epoch": 54.67432950191571,
      "grad_norm": 0.46434420347213745,
      "learning_rate": 0.0004176527704035264,
      "loss": 2.9802,
      "step": 14270
    },
    {
      "epoch": 54.71264367816092,
      "grad_norm": 0.530825138092041,
      "learning_rate": 0.0004175399998663545,
      "loss": 2.991,
      "step": 14280
    },
    {
      "epoch": 54.75095785440613,
      "grad_norm": 0.4681365489959717,
      "learning_rate": 0.0004174271674126185,
      "loss": 2.9686,
      "step": 14290
    },
    {
      "epoch": 54.78927203065134,
      "grad_norm": 0.4698611795902252,
      "learning_rate": 0.0004173142730840172,
      "loss": 2.9854,
      "step": 14300
    },
    {
      "epoch": 54.827586206896555,
      "grad_norm": 0.4818688631057739,
      "learning_rate": 0.00041720131692227214,
      "loss": 2.9882,
      "step": 14310
    },
    {
      "epoch": 54.86590038314176,
      "grad_norm": 0.5143542885780334,
      "learning_rate": 0.0004170882989691278,
      "loss": 3.0221,
      "step": 14320
    },
    {
      "epoch": 54.90421455938697,
      "grad_norm": 0.49386024475097656,
      "learning_rate": 0.0004169752192663513,
      "loss": 2.9917,
      "step": 14330
    },
    {
      "epoch": 54.94252873563218,
      "grad_norm": 0.45923569798469543,
      "learning_rate": 0.0004168620778557327,
      "loss": 2.9981,
      "step": 14340
    },
    {
      "epoch": 54.980842911877396,
      "grad_norm": 0.4918632507324219,
      "learning_rate": 0.000416748874779085,
      "loss": 2.999,
      "step": 14350
    },
    {
      "epoch": 55.0,
      "eval_loss": 1.4497671127319336,
      "eval_runtime": 4.0366,
      "eval_samples_per_second": 4239.174,
      "eval_steps_per_second": 8.423,
      "step": 14355
    },
    {
      "epoch": 55.019157088122604,
      "grad_norm": 0.47382980585098267,
      "learning_rate": 0.00041663561007824367,
      "loss": 2.9796,
      "step": 14360
    },
    {
      "epoch": 55.05747126436781,
      "grad_norm": 0.46432602405548096,
      "learning_rate": 0.0004165222837950673,
      "loss": 2.9596,
      "step": 14370
    },
    {
      "epoch": 55.09578544061303,
      "grad_norm": 0.4801586866378784,
      "learning_rate": 0.00041640889597143707,
      "loss": 2.993,
      "step": 14380
    },
    {
      "epoch": 55.13409961685824,
      "grad_norm": 0.4848974943161011,
      "learning_rate": 0.0004162954466492567,
      "loss": 2.972,
      "step": 14390
    },
    {
      "epoch": 55.172413793103445,
      "grad_norm": 0.465109646320343,
      "learning_rate": 0.0004161819358704531,
      "loss": 2.9769,
      "step": 14400
    },
    {
      "epoch": 55.21072796934866,
      "grad_norm": 0.4593299925327301,
      "learning_rate": 0.0004160683636769755,
      "loss": 2.9733,
      "step": 14410
    },
    {
      "epoch": 55.24904214559387,
      "grad_norm": 0.46874767541885376,
      "learning_rate": 0.000415954730110796,
      "loss": 2.9832,
      "step": 14420
    },
    {
      "epoch": 55.28735632183908,
      "grad_norm": 0.46637746691703796,
      "learning_rate": 0.00041584103521390934,
      "loss": 2.978,
      "step": 14430
    },
    {
      "epoch": 55.32567049808429,
      "grad_norm": 0.4712178409099579,
      "learning_rate": 0.00041572727902833295,
      "loss": 2.9658,
      "step": 14440
    },
    {
      "epoch": 55.3639846743295,
      "grad_norm": 0.8026063442230225,
      "learning_rate": 0.00041561346159610694,
      "loss": 2.9668,
      "step": 14450
    },
    {
      "epoch": 55.40229885057471,
      "grad_norm": 0.5182448625564575,
      "learning_rate": 0.0004154995829592939,
      "loss": 2.9635,
      "step": 14460
    },
    {
      "epoch": 55.440613026819925,
      "grad_norm": 0.48172566294670105,
      "learning_rate": 0.00041538564315997915,
      "loss": 2.9801,
      "step": 14470
    },
    {
      "epoch": 55.47892720306513,
      "grad_norm": 0.4778823256492615,
      "learning_rate": 0.0004152716422402708,
      "loss": 2.9718,
      "step": 14480
    },
    {
      "epoch": 55.51724137931034,
      "grad_norm": 0.47700658440589905,
      "learning_rate": 0.0004151575802422991,
      "loss": 2.9699,
      "step": 14490
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 0.5035185217857361,
      "learning_rate": 0.0004150434572082173,
      "loss": 2.951,
      "step": 14500
    },
    {
      "epoch": 55.593869731800766,
      "grad_norm": 0.48695096373558044,
      "learning_rate": 0.0004149292731802009,
      "loss": 3.0009,
      "step": 14510
    },
    {
      "epoch": 55.632183908045974,
      "grad_norm": 0.4829077124595642,
      "learning_rate": 0.00041481502820044823,
      "loss": 2.9803,
      "step": 14520
    },
    {
      "epoch": 55.67049808429119,
      "grad_norm": 0.48083430528640747,
      "learning_rate": 0.00041470072231117994,
      "loss": 2.9793,
      "step": 14530
    },
    {
      "epoch": 55.7088122605364,
      "grad_norm": 0.49325937032699585,
      "learning_rate": 0.0004145863555546393,
      "loss": 2.9874,
      "step": 14540
    },
    {
      "epoch": 55.747126436781606,
      "grad_norm": 0.49483722448349,
      "learning_rate": 0.0004144719279730918,
      "loss": 3.0014,
      "step": 14550
    },
    {
      "epoch": 55.78544061302682,
      "grad_norm": 0.4703589677810669,
      "learning_rate": 0.0004143574396088258,
      "loss": 2.9902,
      "step": 14560
    },
    {
      "epoch": 55.82375478927203,
      "grad_norm": 0.49159952998161316,
      "learning_rate": 0.00041424289050415197,
      "loss": 2.9822,
      "step": 14570
    },
    {
      "epoch": 55.86206896551724,
      "grad_norm": 0.4691711962223053,
      "learning_rate": 0.00041412828070140335,
      "loss": 2.9948,
      "step": 14580
    },
    {
      "epoch": 55.900383141762454,
      "grad_norm": 0.5030290484428406,
      "learning_rate": 0.00041401361024293547,
      "loss": 2.9876,
      "step": 14590
    },
    {
      "epoch": 55.93869731800766,
      "grad_norm": 0.4859595000743866,
      "learning_rate": 0.0004138988791711263,
      "loss": 2.9786,
      "step": 14600
    },
    {
      "epoch": 55.97701149425287,
      "grad_norm": 0.4675387740135193,
      "learning_rate": 0.00041378408752837614,
      "loss": 2.9723,
      "step": 14610
    },
    {
      "epoch": 56.0,
      "eval_loss": 1.4494366645812988,
      "eval_runtime": 4.3307,
      "eval_samples_per_second": 3951.312,
      "eval_steps_per_second": 7.851,
      "step": 14616
    },
    {
      "epoch": 56.015325670498086,
      "grad_norm": 0.4681605398654938,
      "learning_rate": 0.0004136692353571077,
      "loss": 2.9908,
      "step": 14620
    },
    {
      "epoch": 56.053639846743295,
      "grad_norm": 0.4708280563354492,
      "learning_rate": 0.00041355432269976616,
      "loss": 2.9758,
      "step": 14630
    },
    {
      "epoch": 56.0919540229885,
      "grad_norm": 0.4808584451675415,
      "learning_rate": 0.00041343934959881895,
      "loss": 2.9418,
      "step": 14640
    },
    {
      "epoch": 56.13026819923372,
      "grad_norm": 0.46759912371635437,
      "learning_rate": 0.0004133243160967559,
      "loss": 2.9428,
      "step": 14650
    },
    {
      "epoch": 56.16858237547893,
      "grad_norm": 0.48587822914123535,
      "learning_rate": 0.0004132092222360889,
      "loss": 2.9893,
      "step": 14660
    },
    {
      "epoch": 56.206896551724135,
      "grad_norm": 0.4891560971736908,
      "learning_rate": 0.00041309406805935267,
      "loss": 2.979,
      "step": 14670
    },
    {
      "epoch": 56.24521072796935,
      "grad_norm": 0.46482083201408386,
      "learning_rate": 0.0004129788536091037,
      "loss": 2.9695,
      "step": 14680
    },
    {
      "epoch": 56.28352490421456,
      "grad_norm": 0.48455187678337097,
      "learning_rate": 0.00041286357892792114,
      "loss": 2.9614,
      "step": 14690
    },
    {
      "epoch": 56.32183908045977,
      "grad_norm": 0.50592440366745,
      "learning_rate": 0.00041274824405840613,
      "loss": 2.999,
      "step": 14700
    },
    {
      "epoch": 56.36015325670498,
      "grad_norm": 0.4942980706691742,
      "learning_rate": 0.0004126328490431821,
      "loss": 2.9878,
      "step": 14710
    },
    {
      "epoch": 56.39846743295019,
      "grad_norm": 0.477091908454895,
      "learning_rate": 0.000412517393924895,
      "loss": 2.9625,
      "step": 14720
    },
    {
      "epoch": 56.4367816091954,
      "grad_norm": 0.4776420295238495,
      "learning_rate": 0.00041240187874621236,
      "loss": 2.9694,
      "step": 14730
    },
    {
      "epoch": 56.475095785440615,
      "grad_norm": 0.49641501903533936,
      "learning_rate": 0.00041228630354982464,
      "loss": 2.974,
      "step": 14740
    },
    {
      "epoch": 56.513409961685824,
      "grad_norm": 0.4903419315814972,
      "learning_rate": 0.00041217066837844407,
      "loss": 2.9958,
      "step": 14750
    },
    {
      "epoch": 56.55172413793103,
      "grad_norm": 0.4888482391834259,
      "learning_rate": 0.0004120549732748051,
      "loss": 2.9724,
      "step": 14760
    },
    {
      "epoch": 56.59003831417625,
      "grad_norm": 0.49001097679138184,
      "learning_rate": 0.00041193921828166423,
      "loss": 2.9836,
      "step": 14770
    },
    {
      "epoch": 56.628352490421456,
      "grad_norm": 0.47245100140571594,
      "learning_rate": 0.00041182340344180044,
      "loss": 2.9763,
      "step": 14780
    },
    {
      "epoch": 56.666666666666664,
      "grad_norm": 0.4727122485637665,
      "learning_rate": 0.00041170752879801436,
      "loss": 2.9772,
      "step": 14790
    },
    {
      "epoch": 56.70498084291188,
      "grad_norm": 0.5116272568702698,
      "learning_rate": 0.0004115915943931291,
      "loss": 2.9945,
      "step": 14800
    },
    {
      "epoch": 56.74329501915709,
      "grad_norm": 0.4748995900154114,
      "learning_rate": 0.00041147560026998965,
      "loss": 2.9712,
      "step": 14810
    },
    {
      "epoch": 56.7816091954023,
      "grad_norm": 0.4882517158985138,
      "learning_rate": 0.00041135954647146316,
      "loss": 2.9714,
      "step": 14820
    },
    {
      "epoch": 56.81992337164751,
      "grad_norm": 0.47992682456970215,
      "learning_rate": 0.00041124343304043884,
      "loss": 2.9863,
      "step": 14830
    },
    {
      "epoch": 56.85823754789272,
      "grad_norm": 0.481975257396698,
      "learning_rate": 0.00041112726001982784,
      "loss": 2.9909,
      "step": 14840
    },
    {
      "epoch": 56.89655172413793,
      "grad_norm": 0.4685801863670349,
      "learning_rate": 0.00041101102745256336,
      "loss": 2.9662,
      "step": 14850
    },
    {
      "epoch": 56.934865900383144,
      "grad_norm": 0.5730029344558716,
      "learning_rate": 0.0004108947353816007,
      "loss": 2.9622,
      "step": 14860
    },
    {
      "epoch": 56.97318007662835,
      "grad_norm": 0.45908692479133606,
      "learning_rate": 0.0004107783838499172,
      "loss": 2.9739,
      "step": 14870
    },
    {
      "epoch": 57.0,
      "eval_loss": 1.4495744705200195,
      "eval_runtime": 3.9505,
      "eval_samples_per_second": 4331.626,
      "eval_steps_per_second": 8.607,
      "step": 14877
    },
    {
      "epoch": 57.01149425287356,
      "grad_norm": 0.48447126150131226,
      "learning_rate": 0.00041066197290051177,
      "loss": 2.9778,
      "step": 14880
    },
    {
      "epoch": 57.04980842911878,
      "grad_norm": 0.49957817792892456,
      "learning_rate": 0.0004105455025764059,
      "loss": 2.9546,
      "step": 14890
    },
    {
      "epoch": 57.088122605363985,
      "grad_norm": 0.475125789642334,
      "learning_rate": 0.0004104289729206425,
      "loss": 2.9553,
      "step": 14900
    },
    {
      "epoch": 57.12643678160919,
      "grad_norm": 0.501803994178772,
      "learning_rate": 0.0004103123839762867,
      "loss": 2.9795,
      "step": 14910
    },
    {
      "epoch": 57.16475095785441,
      "grad_norm": 0.47843435406684875,
      "learning_rate": 0.00041019573578642547,
      "loss": 2.9394,
      "step": 14920
    },
    {
      "epoch": 57.20306513409962,
      "grad_norm": 0.4856318235397339,
      "learning_rate": 0.00041007902839416743,
      "loss": 2.9819,
      "step": 14930
    },
    {
      "epoch": 57.241379310344826,
      "grad_norm": 0.4610205888748169,
      "learning_rate": 0.0004099622618426436,
      "loss": 2.9747,
      "step": 14940
    },
    {
      "epoch": 57.27969348659004,
      "grad_norm": 0.4934169054031372,
      "learning_rate": 0.0004098454361750063,
      "loss": 2.9582,
      "step": 14950
    },
    {
      "epoch": 57.31800766283525,
      "grad_norm": 0.47655993700027466,
      "learning_rate": 0.00040972855143443023,
      "loss": 2.9693,
      "step": 14960
    },
    {
      "epoch": 57.35632183908046,
      "grad_norm": 0.4762021005153656,
      "learning_rate": 0.0004096116076641113,
      "loss": 2.9644,
      "step": 14970
    },
    {
      "epoch": 57.39463601532567,
      "grad_norm": 0.5029250979423523,
      "learning_rate": 0.000409494604907268,
      "loss": 2.9793,
      "step": 14980
    },
    {
      "epoch": 57.43295019157088,
      "grad_norm": 0.4705179035663605,
      "learning_rate": 0.0004093775432071398,
      "loss": 2.9847,
      "step": 14990
    },
    {
      "epoch": 57.47126436781609,
      "grad_norm": 0.5061257481575012,
      "learning_rate": 0.0004092604226069887,
      "loss": 2.9787,
      "step": 15000
    },
    {
      "epoch": 57.509578544061306,
      "grad_norm": 0.5078808069229126,
      "learning_rate": 0.0004091432431500979,
      "loss": 2.9657,
      "step": 15010
    },
    {
      "epoch": 57.547892720306514,
      "grad_norm": 0.502374529838562,
      "learning_rate": 0.0004090260048797727,
      "loss": 2.9566,
      "step": 15020
    },
    {
      "epoch": 57.58620689655172,
      "grad_norm": 0.5050685405731201,
      "learning_rate": 0.00040890870783933997,
      "loss": 2.9717,
      "step": 15030
    },
    {
      "epoch": 57.62452107279694,
      "grad_norm": 0.49028337001800537,
      "learning_rate": 0.0004087913520721482,
      "loss": 2.9713,
      "step": 15040
    },
    {
      "epoch": 57.662835249042146,
      "grad_norm": 0.5051403045654297,
      "learning_rate": 0.00040867393762156803,
      "loss": 2.973,
      "step": 15050
    },
    {
      "epoch": 57.701149425287355,
      "grad_norm": 0.4879763722419739,
      "learning_rate": 0.0004085564645309913,
      "loss": 2.9499,
      "step": 15060
    },
    {
      "epoch": 57.73946360153257,
      "grad_norm": 0.4886232614517212,
      "learning_rate": 0.00040843893284383165,
      "loss": 2.9784,
      "step": 15070
    },
    {
      "epoch": 57.77777777777778,
      "grad_norm": 0.47523045539855957,
      "learning_rate": 0.0004083213426035245,
      "loss": 2.984,
      "step": 15080
    },
    {
      "epoch": 57.81609195402299,
      "grad_norm": 0.49285727739334106,
      "learning_rate": 0.0004082036938535268,
      "loss": 2.9826,
      "step": 15090
    },
    {
      "epoch": 57.8544061302682,
      "grad_norm": 0.49414241313934326,
      "learning_rate": 0.00040808598663731734,
      "loss": 2.9824,
      "step": 15100
    },
    {
      "epoch": 57.89272030651341,
      "grad_norm": 0.47019752860069275,
      "learning_rate": 0.00040796822099839605,
      "loss": 2.9692,
      "step": 15110
    },
    {
      "epoch": 57.93103448275862,
      "grad_norm": 0.4767693877220154,
      "learning_rate": 0.00040785039698028493,
      "loss": 2.9717,
      "step": 15120
    },
    {
      "epoch": 57.969348659003835,
      "grad_norm": 0.47821247577667236,
      "learning_rate": 0.00040773251462652727,
      "loss": 2.969,
      "step": 15130
    },
    {
      "epoch": 58.0,
      "eval_loss": 1.4499273300170898,
      "eval_runtime": 4.0676,
      "eval_samples_per_second": 4206.864,
      "eval_steps_per_second": 8.359,
      "step": 15138
    },
    {
      "epoch": 58.00766283524904,
      "grad_norm": 0.4907020926475525,
      "learning_rate": 0.0004076145739806881,
      "loss": 2.968,
      "step": 15140
    },
    {
      "epoch": 58.04597701149425,
      "grad_norm": 0.4790942668914795,
      "learning_rate": 0.00040749657508635384,
      "loss": 2.9542,
      "step": 15150
    },
    {
      "epoch": 58.08429118773947,
      "grad_norm": 0.4925198554992676,
      "learning_rate": 0.00040737851798713253,
      "loss": 2.9477,
      "step": 15160
    },
    {
      "epoch": 58.122605363984675,
      "grad_norm": 0.4771466851234436,
      "learning_rate": 0.0004072604027266537,
      "loss": 2.955,
      "step": 15170
    },
    {
      "epoch": 58.160919540229884,
      "grad_norm": 0.489437997341156,
      "learning_rate": 0.0004071422293485683,
      "loss": 2.9623,
      "step": 15180
    },
    {
      "epoch": 58.1992337164751,
      "grad_norm": 0.4936770796775818,
      "learning_rate": 0.0004070239978965489,
      "loss": 2.9636,
      "step": 15190
    },
    {
      "epoch": 58.23754789272031,
      "grad_norm": 0.49411702156066895,
      "learning_rate": 0.0004069057084142893,
      "loss": 2.9572,
      "step": 15200
    },
    {
      "epoch": 58.275862068965516,
      "grad_norm": 0.486356258392334,
      "learning_rate": 0.0004067873609455052,
      "loss": 2.9635,
      "step": 15210
    },
    {
      "epoch": 58.31417624521073,
      "grad_norm": 0.4720582365989685,
      "learning_rate": 0.0004066689555339331,
      "loss": 2.9638,
      "step": 15220
    },
    {
      "epoch": 58.35249042145594,
      "grad_norm": 0.47544482350349426,
      "learning_rate": 0.00040655049222333153,
      "loss": 2.9728,
      "step": 15230
    },
    {
      "epoch": 58.39080459770115,
      "grad_norm": 0.4900279939174652,
      "learning_rate": 0.00040643197105747994,
      "loss": 2.963,
      "step": 15240
    },
    {
      "epoch": 58.42911877394636,
      "grad_norm": 0.48086509108543396,
      "learning_rate": 0.00040631339208017936,
      "loss": 2.96,
      "step": 15250
    },
    {
      "epoch": 58.46743295019157,
      "grad_norm": 0.46860718727111816,
      "learning_rate": 0.0004061947553352523,
      "loss": 2.9677,
      "step": 15260
    },
    {
      "epoch": 58.50574712643678,
      "grad_norm": 0.4794977307319641,
      "learning_rate": 0.0004060760608665425,
      "loss": 2.9733,
      "step": 15270
    },
    {
      "epoch": 58.54406130268199,
      "grad_norm": 0.4810510277748108,
      "learning_rate": 0.0004059573087179148,
      "loss": 2.9916,
      "step": 15280
    },
    {
      "epoch": 58.582375478927204,
      "grad_norm": 0.5060144066810608,
      "learning_rate": 0.0004058384989332559,
      "loss": 2.9571,
      "step": 15290
    },
    {
      "epoch": 58.62068965517241,
      "grad_norm": 0.48399555683135986,
      "learning_rate": 0.00040571963155647316,
      "loss": 2.9577,
      "step": 15300
    },
    {
      "epoch": 58.65900383141762,
      "grad_norm": 0.4904211461544037,
      "learning_rate": 0.0004056007066314957,
      "loss": 2.9603,
      "step": 15310
    },
    {
      "epoch": 58.69731800766284,
      "grad_norm": 0.46907493472099304,
      "learning_rate": 0.0004054817242022739,
      "loss": 2.9664,
      "step": 15320
    },
    {
      "epoch": 58.735632183908045,
      "grad_norm": 0.48868080973625183,
      "learning_rate": 0.0004053626843127791,
      "loss": 2.9581,
      "step": 15330
    },
    {
      "epoch": 58.77394636015325,
      "grad_norm": 0.46926552057266235,
      "learning_rate": 0.00040524358700700405,
      "loss": 2.9818,
      "step": 15340
    },
    {
      "epoch": 58.81226053639847,
      "grad_norm": 0.4665415585041046,
      "learning_rate": 0.0004051244323289627,
      "loss": 2.9729,
      "step": 15350
    },
    {
      "epoch": 58.85057471264368,
      "grad_norm": 0.471779465675354,
      "learning_rate": 0.00040500522032269016,
      "loss": 2.9612,
      "step": 15360
    },
    {
      "epoch": 58.888888888888886,
      "grad_norm": 0.47147446870803833,
      "learning_rate": 0.0004048859510322427,
      "loss": 2.9562,
      "step": 15370
    },
    {
      "epoch": 58.9272030651341,
      "grad_norm": 0.474832683801651,
      "learning_rate": 0.0004047666245016981,
      "loss": 2.9572,
      "step": 15380
    },
    {
      "epoch": 58.96551724137931,
      "grad_norm": 0.46831968426704407,
      "learning_rate": 0.0004046472407751547,
      "loss": 2.9559,
      "step": 15390
    },
    {
      "epoch": 59.0,
      "eval_loss": 1.448550820350647,
      "eval_runtime": 4.3047,
      "eval_samples_per_second": 3975.17,
      "eval_steps_per_second": 7.898,
      "step": 15399
    },
    {
      "epoch": 59.00383141762452,
      "grad_norm": 0.4711752235889435,
      "learning_rate": 0.00040452779989673236,
      "loss": 2.9713,
      "step": 15400
    },
    {
      "epoch": 59.04214559386973,
      "grad_norm": 0.4642803966999054,
      "learning_rate": 0.00040440830191057215,
      "loss": 2.9492,
      "step": 15410
    },
    {
      "epoch": 59.08045977011494,
      "grad_norm": 0.4895194470882416,
      "learning_rate": 0.00040428874686083597,
      "loss": 2.9357,
      "step": 15420
    },
    {
      "epoch": 59.11877394636015,
      "grad_norm": 0.48601245880126953,
      "learning_rate": 0.00040416913479170694,
      "loss": 2.9485,
      "step": 15430
    },
    {
      "epoch": 59.157088122605366,
      "grad_norm": 0.47150811553001404,
      "learning_rate": 0.0004040494657473892,
      "loss": 2.9525,
      "step": 15440
    },
    {
      "epoch": 59.195402298850574,
      "grad_norm": 0.4860350489616394,
      "learning_rate": 0.00040392973977210813,
      "loss": 2.9544,
      "step": 15450
    },
    {
      "epoch": 59.23371647509578,
      "grad_norm": 0.490342915058136,
      "learning_rate": 0.0004038099569101098,
      "loss": 2.9615,
      "step": 15460
    },
    {
      "epoch": 59.272030651341,
      "grad_norm": 0.49644720554351807,
      "learning_rate": 0.00040369011720566174,
      "loss": 2.9641,
      "step": 15470
    },
    {
      "epoch": 59.310344827586206,
      "grad_norm": 0.47269776463508606,
      "learning_rate": 0.00040357022070305204,
      "loss": 2.9497,
      "step": 15480
    },
    {
      "epoch": 59.348659003831415,
      "grad_norm": 0.49231964349746704,
      "learning_rate": 0.0004034502674465901,
      "loss": 2.9663,
      "step": 15490
    },
    {
      "epoch": 59.38697318007663,
      "grad_norm": 0.49814143776893616,
      "learning_rate": 0.00040333025748060626,
      "loss": 2.9736,
      "step": 15500
    },
    {
      "epoch": 59.42528735632184,
      "grad_norm": 0.5012304186820984,
      "learning_rate": 0.0004032101908494516,
      "loss": 2.9451,
      "step": 15510
    },
    {
      "epoch": 59.46360153256705,
      "grad_norm": 0.4876822531223297,
      "learning_rate": 0.00040309006759749834,
      "loss": 2.9536,
      "step": 15520
    },
    {
      "epoch": 59.50191570881226,
      "grad_norm": 0.4879160225391388,
      "learning_rate": 0.0004029698877691396,
      "loss": 2.929,
      "step": 15530
    },
    {
      "epoch": 59.54022988505747,
      "grad_norm": 0.4906235635280609,
      "learning_rate": 0.0004028496514087895,
      "loss": 2.9713,
      "step": 15540
    },
    {
      "epoch": 59.57854406130268,
      "grad_norm": 0.49296674132347107,
      "learning_rate": 0.00040272935856088264,
      "loss": 2.9709,
      "step": 15550
    },
    {
      "epoch": 59.616858237547895,
      "grad_norm": 0.4723140597343445,
      "learning_rate": 0.000402609009269875,
      "loss": 2.9707,
      "step": 15560
    },
    {
      "epoch": 59.6551724137931,
      "grad_norm": 0.491748571395874,
      "learning_rate": 0.0004024886035802432,
      "loss": 2.9762,
      "step": 15570
    },
    {
      "epoch": 59.69348659003831,
      "grad_norm": 0.49418413639068604,
      "learning_rate": 0.0004023681415364847,
      "loss": 3.0005,
      "step": 15580
    },
    {
      "epoch": 59.73180076628353,
      "grad_norm": 0.470033198595047,
      "learning_rate": 0.0004022476231831178,
      "loss": 2.9579,
      "step": 15590
    },
    {
      "epoch": 59.770114942528735,
      "grad_norm": 0.48761171102523804,
      "learning_rate": 0.0004021270485646816,
      "loss": 2.9371,
      "step": 15600
    },
    {
      "epoch": 59.808429118773944,
      "grad_norm": 0.46986308693885803,
      "learning_rate": 0.0004020064177257359,
      "loss": 2.9608,
      "step": 15610
    },
    {
      "epoch": 59.84674329501916,
      "grad_norm": 0.4833946228027344,
      "learning_rate": 0.00040188573071086155,
      "loss": 2.957,
      "step": 15620
    },
    {
      "epoch": 59.88505747126437,
      "grad_norm": 0.4815317988395691,
      "learning_rate": 0.00040176498756465986,
      "loss": 2.9646,
      "step": 15630
    },
    {
      "epoch": 59.923371647509576,
      "grad_norm": 0.49154749512672424,
      "learning_rate": 0.0004016441883317531,
      "loss": 2.984,
      "step": 15640
    },
    {
      "epoch": 59.96168582375479,
      "grad_norm": 0.4942568242549896,
      "learning_rate": 0.00040152333305678423,
      "loss": 2.9514,
      "step": 15650
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.4754132330417633,
      "learning_rate": 0.00040140242178441667,
      "loss": 2.9402,
      "step": 15660
    },
    {
      "epoch": 60.0,
      "eval_loss": 1.4504791498184204,
      "eval_runtime": 4.0576,
      "eval_samples_per_second": 4217.319,
      "eval_steps_per_second": 8.379,
      "step": 15660
    },
    {
      "epoch": 60.03831417624521,
      "grad_norm": 0.48153311014175415,
      "learning_rate": 0.00040128145455933497,
      "loss": 2.9537,
      "step": 15670
    },
    {
      "epoch": 60.076628352490424,
      "grad_norm": 0.4872553050518036,
      "learning_rate": 0.0004011604314262439,
      "loss": 2.9345,
      "step": 15680
    },
    {
      "epoch": 60.11494252873563,
      "grad_norm": 0.47176769375801086,
      "learning_rate": 0.0004010393524298693,
      "loss": 2.949,
      "step": 15690
    },
    {
      "epoch": 60.15325670498084,
      "grad_norm": 0.49325937032699585,
      "learning_rate": 0.0004009182176149574,
      "loss": 2.9229,
      "step": 15700
    },
    {
      "epoch": 60.191570881226056,
      "grad_norm": 0.47326403856277466,
      "learning_rate": 0.00040079702702627514,
      "loss": 2.968,
      "step": 15710
    },
    {
      "epoch": 60.229885057471265,
      "grad_norm": 0.49830153584480286,
      "learning_rate": 0.00040067578070861,
      "loss": 2.9494,
      "step": 15720
    },
    {
      "epoch": 60.26819923371647,
      "grad_norm": 0.4997774064540863,
      "learning_rate": 0.00040055447870677026,
      "loss": 2.9456,
      "step": 15730
    },
    {
      "epoch": 60.30651340996169,
      "grad_norm": 0.4923149645328522,
      "learning_rate": 0.0004004331210655845,
      "loss": 2.9545,
      "step": 15740
    },
    {
      "epoch": 60.3448275862069,
      "grad_norm": 0.5084024667739868,
      "learning_rate": 0.0004003117078299021,
      "loss": 2.9548,
      "step": 15750
    },
    {
      "epoch": 60.383141762452105,
      "grad_norm": 0.47281888127326965,
      "learning_rate": 0.0004001902390445928,
      "loss": 2.9522,
      "step": 15760
    },
    {
      "epoch": 60.42145593869732,
      "grad_norm": 0.4895137846469879,
      "learning_rate": 0.000400068714754547,
      "loss": 2.9437,
      "step": 15770
    },
    {
      "epoch": 60.45977011494253,
      "grad_norm": 0.4708593487739563,
      "learning_rate": 0.0003999471350046755,
      "loss": 2.9659,
      "step": 15780
    },
    {
      "epoch": 60.49808429118774,
      "grad_norm": 0.49537044763565063,
      "learning_rate": 0.0003998254998399098,
      "loss": 2.9381,
      "step": 15790
    },
    {
      "epoch": 60.53639846743295,
      "grad_norm": 0.4746299982070923,
      "learning_rate": 0.00039970380930520164,
      "loss": 2.951,
      "step": 15800
    },
    {
      "epoch": 60.57471264367816,
      "grad_norm": 0.4648134112358093,
      "learning_rate": 0.0003995820634455233,
      "loss": 2.9508,
      "step": 15810
    },
    {
      "epoch": 60.61302681992337,
      "grad_norm": 0.4751119613647461,
      "learning_rate": 0.0003994602623058676,
      "loss": 2.962,
      "step": 15820
    },
    {
      "epoch": 60.651340996168585,
      "grad_norm": 0.49082204699516296,
      "learning_rate": 0.00039933840593124766,
      "loss": 2.9467,
      "step": 15830
    },
    {
      "epoch": 60.689655172413794,
      "grad_norm": 0.5090520977973938,
      "learning_rate": 0.00039921649436669714,
      "loss": 2.9538,
      "step": 15840
    },
    {
      "epoch": 60.727969348659,
      "grad_norm": 0.5277681350708008,
      "learning_rate": 0.00039909452765726994,
      "loss": 2.9764,
      "step": 15850
    },
    {
      "epoch": 60.76628352490422,
      "grad_norm": 0.46788039803504944,
      "learning_rate": 0.0003989725058480406,
      "loss": 2.9697,
      "step": 15860
    },
    {
      "epoch": 60.804597701149426,
      "grad_norm": 0.4840967059135437,
      "learning_rate": 0.00039885042898410363,
      "loss": 2.9702,
      "step": 15870
    },
    {
      "epoch": 60.842911877394634,
      "grad_norm": 0.4781110882759094,
      "learning_rate": 0.0003987282971105742,
      "loss": 2.9739,
      "step": 15880
    },
    {
      "epoch": 60.88122605363985,
      "grad_norm": 0.47625353932380676,
      "learning_rate": 0.00039860611027258774,
      "loss": 2.953,
      "step": 15890
    },
    {
      "epoch": 60.91954022988506,
      "grad_norm": 0.5019318461418152,
      "learning_rate": 0.0003984838685153,
      "loss": 2.966,
      "step": 15900
    },
    {
      "epoch": 60.95785440613027,
      "grad_norm": 0.49159136414527893,
      "learning_rate": 0.0003983615718838869,
      "loss": 2.944,
      "step": 15910
    },
    {
      "epoch": 60.99616858237548,
      "grad_norm": 0.47554588317871094,
      "learning_rate": 0.00039823922042354475,
      "loss": 2.96,
      "step": 15920
    },
    {
      "epoch": 61.0,
      "eval_loss": 1.4498883485794067,
      "eval_runtime": 4.0427,
      "eval_samples_per_second": 4232.782,
      "eval_steps_per_second": 8.41,
      "step": 15921
    },
    {
      "epoch": 61.03448275862069,
      "grad_norm": 0.48012620210647583,
      "learning_rate": 0.0003981168141794902,
      "loss": 2.9384,
      "step": 15930
    },
    {
      "epoch": 61.0727969348659,
      "grad_norm": 0.486042857170105,
      "learning_rate": 0.00039799435319696,
      "loss": 2.9338,
      "step": 15940
    },
    {
      "epoch": 61.111111111111114,
      "grad_norm": 0.5115165114402771,
      "learning_rate": 0.00039787183752121113,
      "loss": 2.9327,
      "step": 15950
    },
    {
      "epoch": 61.14942528735632,
      "grad_norm": 0.47418031096458435,
      "learning_rate": 0.0003977492671975209,
      "loss": 2.9349,
      "step": 15960
    },
    {
      "epoch": 61.18773946360153,
      "grad_norm": 0.4874533712863922,
      "learning_rate": 0.00039762664227118675,
      "loss": 2.9351,
      "step": 15970
    },
    {
      "epoch": 61.22605363984675,
      "grad_norm": 0.5098086595535278,
      "learning_rate": 0.0003975039627875262,
      "loss": 2.9618,
      "step": 15980
    },
    {
      "epoch": 61.264367816091955,
      "grad_norm": 0.4701441824436188,
      "learning_rate": 0.0003973812287918772,
      "loss": 2.9413,
      "step": 15990
    },
    {
      "epoch": 61.30268199233716,
      "grad_norm": 0.48356398940086365,
      "learning_rate": 0.0003972584403295976,
      "loss": 2.9453,
      "step": 16000
    },
    {
      "epoch": 61.34099616858238,
      "grad_norm": 0.5022403001785278,
      "learning_rate": 0.0003971355974460654,
      "loss": 2.9455,
      "step": 16010
    },
    {
      "epoch": 61.37931034482759,
      "grad_norm": 0.46946200728416443,
      "learning_rate": 0.0003970127001866789,
      "loss": 2.9531,
      "step": 16020
    },
    {
      "epoch": 61.417624521072796,
      "grad_norm": 0.47001996636390686,
      "learning_rate": 0.0003968897485968562,
      "loss": 2.9679,
      "step": 16030
    },
    {
      "epoch": 61.45593869731801,
      "grad_norm": 0.4698232114315033,
      "learning_rate": 0.00039676674272203573,
      "loss": 2.9443,
      "step": 16040
    },
    {
      "epoch": 61.49425287356322,
      "grad_norm": 0.483869343996048,
      "learning_rate": 0.0003966436826076759,
      "loss": 2.9524,
      "step": 16050
    },
    {
      "epoch": 61.53256704980843,
      "grad_norm": 0.47977951169013977,
      "learning_rate": 0.0003965205682992552,
      "loss": 2.9436,
      "step": 16060
    },
    {
      "epoch": 61.57088122605364,
      "grad_norm": 0.4725092351436615,
      "learning_rate": 0.0003963973998422721,
      "loss": 2.9352,
      "step": 16070
    },
    {
      "epoch": 61.60919540229885,
      "grad_norm": 0.5080975890159607,
      "learning_rate": 0.0003962741772822449,
      "loss": 2.9623,
      "step": 16080
    },
    {
      "epoch": 61.64750957854406,
      "grad_norm": 0.48630818724632263,
      "learning_rate": 0.0003961509006647123,
      "loss": 2.9502,
      "step": 16090
    },
    {
      "epoch": 61.68582375478927,
      "grad_norm": 0.46614396572113037,
      "learning_rate": 0.0003960275700352327,
      "loss": 2.9562,
      "step": 16100
    },
    {
      "epoch": 61.724137931034484,
      "grad_norm": 0.49313029646873474,
      "learning_rate": 0.00039590418543938453,
      "loss": 2.9369,
      "step": 16110
    },
    {
      "epoch": 61.76245210727969,
      "grad_norm": 0.499589204788208,
      "learning_rate": 0.0003957807469227662,
      "loss": 2.9652,
      "step": 16120
    },
    {
      "epoch": 61.8007662835249,
      "grad_norm": 0.46399497985839844,
      "learning_rate": 0.00039565725453099583,
      "loss": 2.9405,
      "step": 16130
    },
    {
      "epoch": 61.839080459770116,
      "grad_norm": 0.47607743740081787,
      "learning_rate": 0.00039553370830971177,
      "loss": 2.9548,
      "step": 16140
    },
    {
      "epoch": 61.877394636015325,
      "grad_norm": 0.46640336513519287,
      "learning_rate": 0.0003954101083045721,
      "loss": 2.9555,
      "step": 16150
    },
    {
      "epoch": 61.91570881226053,
      "grad_norm": 0.4940558969974518,
      "learning_rate": 0.0003952864545612548,
      "loss": 2.9538,
      "step": 16160
    },
    {
      "epoch": 61.95402298850575,
      "grad_norm": 0.482607364654541,
      "learning_rate": 0.00039516274712545765,
      "loss": 2.9395,
      "step": 16170
    },
    {
      "epoch": 61.99233716475096,
      "grad_norm": 0.4575265645980835,
      "learning_rate": 0.00039503898604289833,
      "loss": 2.9643,
      "step": 16180
    },
    {
      "epoch": 62.0,
      "eval_loss": 1.4505436420440674,
      "eval_runtime": 3.9818,
      "eval_samples_per_second": 4297.511,
      "eval_steps_per_second": 8.539,
      "step": 16182
    },
    {
      "epoch": 62.030651340996165,
      "grad_norm": 0.4704483449459076,
      "learning_rate": 0.0003949151713593144,
      "loss": 2.9449,
      "step": 16190
    },
    {
      "epoch": 62.06896551724138,
      "grad_norm": 0.49772530794143677,
      "learning_rate": 0.00039479130312046305,
      "loss": 2.9563,
      "step": 16200
    },
    {
      "epoch": 62.10727969348659,
      "grad_norm": 0.4854792356491089,
      "learning_rate": 0.0003946673813721216,
      "loss": 2.9302,
      "step": 16210
    },
    {
      "epoch": 62.1455938697318,
      "grad_norm": 0.4816496968269348,
      "learning_rate": 0.00039454340616008676,
      "loss": 2.9509,
      "step": 16220
    },
    {
      "epoch": 62.18390804597701,
      "grad_norm": 0.4692234694957733,
      "learning_rate": 0.0003944193775301752,
      "loss": 2.9342,
      "step": 16230
    },
    {
      "epoch": 62.22222222222222,
      "grad_norm": 0.49796316027641296,
      "learning_rate": 0.0003942952955282233,
      "loss": 2.9239,
      "step": 16240
    },
    {
      "epoch": 62.26053639846743,
      "grad_norm": 0.4702972173690796,
      "learning_rate": 0.00039417116020008715,
      "loss": 2.9455,
      "step": 16250
    },
    {
      "epoch": 62.298850574712645,
      "grad_norm": 0.49334779381752014,
      "learning_rate": 0.0003940469715916426,
      "loss": 2.93,
      "step": 16260
    },
    {
      "epoch": 62.337164750957854,
      "grad_norm": 0.4798605442047119,
      "learning_rate": 0.00039392272974878505,
      "loss": 2.9455,
      "step": 16270
    },
    {
      "epoch": 62.37547892720306,
      "grad_norm": 0.5058832168579102,
      "learning_rate": 0.0003937984347174298,
      "loss": 2.9471,
      "step": 16280
    },
    {
      "epoch": 62.41379310344828,
      "grad_norm": 0.49425652623176575,
      "learning_rate": 0.0003936740865435116,
      "loss": 2.9459,
      "step": 16290
    },
    {
      "epoch": 62.452107279693486,
      "grad_norm": 0.4694110155105591,
      "learning_rate": 0.00039354968527298493,
      "loss": 2.9573,
      "step": 16300
    },
    {
      "epoch": 62.490421455938694,
      "grad_norm": 0.46785736083984375,
      "learning_rate": 0.00039342523095182394,
      "loss": 2.9287,
      "step": 16310
    },
    {
      "epoch": 62.52873563218391,
      "grad_norm": 0.4822242259979248,
      "learning_rate": 0.0003933007236260223,
      "loss": 2.9273,
      "step": 16320
    },
    {
      "epoch": 62.56704980842912,
      "grad_norm": 0.508546769618988,
      "learning_rate": 0.00039317616334159325,
      "loss": 2.9404,
      "step": 16330
    },
    {
      "epoch": 62.60536398467433,
      "grad_norm": 0.4743902087211609,
      "learning_rate": 0.0003930515501445696,
      "loss": 2.9578,
      "step": 16340
    },
    {
      "epoch": 62.64367816091954,
      "grad_norm": 0.47447341680526733,
      "learning_rate": 0.00039292688408100386,
      "loss": 2.9401,
      "step": 16350
    },
    {
      "epoch": 62.68199233716475,
      "grad_norm": 0.5054757595062256,
      "learning_rate": 0.0003928021651969679,
      "loss": 2.9658,
      "step": 16360
    },
    {
      "epoch": 62.72030651340996,
      "grad_norm": 0.4829448461532593,
      "learning_rate": 0.0003926773935385534,
      "loss": 2.9161,
      "step": 16370
    },
    {
      "epoch": 62.758620689655174,
      "grad_norm": 0.46620985865592957,
      "learning_rate": 0.0003925525691518711,
      "loss": 2.939,
      "step": 16380
    },
    {
      "epoch": 62.79693486590038,
      "grad_norm": 0.49826404452323914,
      "learning_rate": 0.00039242769208305145,
      "loss": 2.9476,
      "step": 16390
    },
    {
      "epoch": 62.83524904214559,
      "grad_norm": 0.4788656234741211,
      "learning_rate": 0.0003923027623782445,
      "loss": 2.9378,
      "step": 16400
    },
    {
      "epoch": 62.87356321839081,
      "grad_norm": 0.4980309307575226,
      "learning_rate": 0.0003921777800836196,
      "loss": 2.9477,
      "step": 16410
    },
    {
      "epoch": 62.911877394636015,
      "grad_norm": 0.4761233329772949,
      "learning_rate": 0.00039205274524536563,
      "loss": 2.9746,
      "step": 16420
    },
    {
      "epoch": 62.95019157088122,
      "grad_norm": 0.4893054962158203,
      "learning_rate": 0.0003919276579096906,
      "loss": 2.9469,
      "step": 16430
    },
    {
      "epoch": 62.98850574712644,
      "grad_norm": 0.4820120334625244,
      "learning_rate": 0.00039180251812282244,
      "loss": 2.9713,
      "step": 16440
    },
    {
      "epoch": 63.0,
      "eval_loss": 1.4493536949157715,
      "eval_runtime": 4.1112,
      "eval_samples_per_second": 4162.311,
      "eval_steps_per_second": 8.27,
      "step": 16443
    },
    {
      "epoch": 63.02681992337165,
      "grad_norm": 0.4592854678630829,
      "learning_rate": 0.000391677325931008,
      "loss": 2.9136,
      "step": 16450
    },
    {
      "epoch": 63.065134099616856,
      "grad_norm": 0.4863201677799225,
      "learning_rate": 0.00039155208138051364,
      "loss": 2.9226,
      "step": 16460
    },
    {
      "epoch": 63.10344827586207,
      "grad_norm": 0.47843435406684875,
      "learning_rate": 0.00039142678451762513,
      "loss": 2.9048,
      "step": 16470
    },
    {
      "epoch": 63.14176245210728,
      "grad_norm": 0.48488718271255493,
      "learning_rate": 0.0003913014353886476,
      "loss": 2.9227,
      "step": 16480
    },
    {
      "epoch": 63.18007662835249,
      "grad_norm": 0.47051024436950684,
      "learning_rate": 0.0003911760340399054,
      "loss": 2.949,
      "step": 16490
    },
    {
      "epoch": 63.2183908045977,
      "grad_norm": 0.47269418835639954,
      "learning_rate": 0.0003910505805177422,
      "loss": 2.9548,
      "step": 16500
    },
    {
      "epoch": 63.25670498084291,
      "grad_norm": 0.48957836627960205,
      "learning_rate": 0.00039092507486852097,
      "loss": 2.9342,
      "step": 16510
    },
    {
      "epoch": 63.29501915708812,
      "grad_norm": 0.4747925102710724,
      "learning_rate": 0.0003907995171386239,
      "loss": 2.9294,
      "step": 16520
    },
    {
      "epoch": 63.333333333333336,
      "grad_norm": 0.5049697160720825,
      "learning_rate": 0.00039067390737445256,
      "loss": 2.9218,
      "step": 16530
    },
    {
      "epoch": 63.371647509578544,
      "grad_norm": 0.48446935415267944,
      "learning_rate": 0.00039054824562242756,
      "loss": 2.9343,
      "step": 16540
    },
    {
      "epoch": 63.40996168582375,
      "grad_norm": 0.4941818416118622,
      "learning_rate": 0.0003904225319289889,
      "loss": 2.9335,
      "step": 16550
    },
    {
      "epoch": 63.44827586206897,
      "grad_norm": 0.4772859215736389,
      "learning_rate": 0.00039029676634059563,
      "loss": 2.9415,
      "step": 16560
    },
    {
      "epoch": 63.486590038314176,
      "grad_norm": 0.47336331009864807,
      "learning_rate": 0.00039017094890372606,
      "loss": 2.954,
      "step": 16570
    },
    {
      "epoch": 63.524904214559385,
      "grad_norm": 0.480241984128952,
      "learning_rate": 0.00039004507966487766,
      "loss": 2.9415,
      "step": 16580
    },
    {
      "epoch": 63.5632183908046,
      "grad_norm": 0.4830726385116577,
      "learning_rate": 0.00038991915867056694,
      "loss": 2.942,
      "step": 16590
    },
    {
      "epoch": 63.60153256704981,
      "grad_norm": 0.47621840238571167,
      "learning_rate": 0.0003897931859673298,
      "loss": 2.9344,
      "step": 16600
    },
    {
      "epoch": 63.63984674329502,
      "grad_norm": 0.4640594720840454,
      "learning_rate": 0.0003896671616017209,
      "loss": 2.9542,
      "step": 16610
    },
    {
      "epoch": 63.67816091954023,
      "grad_norm": 0.47306501865386963,
      "learning_rate": 0.00038954108562031425,
      "loss": 2.9275,
      "step": 16620
    },
    {
      "epoch": 63.71647509578544,
      "grad_norm": 0.47963008284568787,
      "learning_rate": 0.00038941495806970287,
      "loss": 2.9478,
      "step": 16630
    },
    {
      "epoch": 63.75478927203065,
      "grad_norm": 0.48165014386177063,
      "learning_rate": 0.0003892887789964988,
      "loss": 2.9356,
      "step": 16640
    },
    {
      "epoch": 63.793103448275865,
      "grad_norm": 0.47013622522354126,
      "learning_rate": 0.00038916254844733315,
      "loss": 2.9266,
      "step": 16650
    },
    {
      "epoch": 63.83141762452107,
      "grad_norm": 0.48942843079566956,
      "learning_rate": 0.000389036266468856,
      "loss": 2.934,
      "step": 16660
    },
    {
      "epoch": 63.86973180076628,
      "grad_norm": 0.48709097504615784,
      "learning_rate": 0.0003889099331077365,
      "loss": 2.9311,
      "step": 16670
    },
    {
      "epoch": 63.9080459770115,
      "grad_norm": 0.4836779534816742,
      "learning_rate": 0.00038878354841066277,
      "loss": 2.949,
      "step": 16680
    },
    {
      "epoch": 63.946360153256705,
      "grad_norm": 0.48393377661705017,
      "learning_rate": 0.00038865711242434196,
      "loss": 2.9489,
      "step": 16690
    },
    {
      "epoch": 63.984674329501914,
      "grad_norm": 0.49708396196365356,
      "learning_rate": 0.0003885306251955,
      "loss": 2.9527,
      "step": 16700
    },
    {
      "epoch": 64.0,
      "eval_loss": 1.4496597051620483,
      "eval_runtime": 4.0568,
      "eval_samples_per_second": 4218.106,
      "eval_steps_per_second": 8.381,
      "step": 16704
    },
    {
      "epoch": 64.02298850574712,
      "grad_norm": 0.4791461229324341,
      "learning_rate": 0.000388404086770882,
      "loss": 2.9397,
      "step": 16710
    },
    {
      "epoch": 64.06130268199233,
      "grad_norm": 0.4953204095363617,
      "learning_rate": 0.0003882774971972518,
      "loss": 2.9258,
      "step": 16720
    },
    {
      "epoch": 64.09961685823755,
      "grad_norm": 0.4823141098022461,
      "learning_rate": 0.00038815085652139214,
      "loss": 2.905,
      "step": 16730
    },
    {
      "epoch": 64.13793103448276,
      "grad_norm": 0.4878960847854614,
      "learning_rate": 0.00038802416479010495,
      "loss": 2.916,
      "step": 16740
    },
    {
      "epoch": 64.17624521072797,
      "grad_norm": 0.4814237952232361,
      "learning_rate": 0.0003878974220502105,
      "loss": 2.9317,
      "step": 16750
    },
    {
      "epoch": 64.21455938697318,
      "grad_norm": 0.46175718307495117,
      "learning_rate": 0.00038777062834854833,
      "loss": 2.9322,
      "step": 16760
    },
    {
      "epoch": 64.25287356321839,
      "grad_norm": 0.4818803071975708,
      "learning_rate": 0.0003876437837319767,
      "loss": 2.9123,
      "step": 16770
    },
    {
      "epoch": 64.2911877394636,
      "grad_norm": 0.4759681820869446,
      "learning_rate": 0.00038751688824737265,
      "loss": 2.9171,
      "step": 16780
    },
    {
      "epoch": 64.32950191570882,
      "grad_norm": 0.48064157366752625,
      "learning_rate": 0.00038738994194163203,
      "loss": 2.9425,
      "step": 16790
    },
    {
      "epoch": 64.36781609195403,
      "grad_norm": 0.4727669358253479,
      "learning_rate": 0.00038726294486166957,
      "loss": 2.9172,
      "step": 16800
    },
    {
      "epoch": 64.40613026819923,
      "grad_norm": 0.49809908866882324,
      "learning_rate": 0.0003871358970544185,
      "loss": 2.9358,
      "step": 16810
    },
    {
      "epoch": 64.44444444444444,
      "grad_norm": 0.4723859429359436,
      "learning_rate": 0.00038700879856683114,
      "loss": 2.9343,
      "step": 16820
    },
    {
      "epoch": 64.48275862068965,
      "grad_norm": 0.4773610234260559,
      "learning_rate": 0.0003868816494458782,
      "loss": 2.9349,
      "step": 16830
    },
    {
      "epoch": 64.52107279693486,
      "grad_norm": 0.4824680685997009,
      "learning_rate": 0.0003867544497385496,
      "loss": 2.9494,
      "step": 16840
    },
    {
      "epoch": 64.55938697318008,
      "grad_norm": 0.4886968731880188,
      "learning_rate": 0.0003866271994918532,
      "loss": 2.9396,
      "step": 16850
    },
    {
      "epoch": 64.59770114942529,
      "grad_norm": 0.47754377126693726,
      "learning_rate": 0.00038649989875281626,
      "loss": 2.934,
      "step": 16860
    },
    {
      "epoch": 64.6360153256705,
      "grad_norm": 0.4719487428665161,
      "learning_rate": 0.0003863725475684843,
      "loss": 2.9384,
      "step": 16870
    },
    {
      "epoch": 64.67432950191571,
      "grad_norm": 0.48295971751213074,
      "learning_rate": 0.00038624514598592166,
      "loss": 2.9362,
      "step": 16880
    },
    {
      "epoch": 64.71264367816092,
      "grad_norm": 0.49104899168014526,
      "learning_rate": 0.0003861176940522111,
      "loss": 2.9201,
      "step": 16890
    },
    {
      "epoch": 64.75095785440612,
      "grad_norm": 0.48708948493003845,
      "learning_rate": 0.0003859901918144543,
      "loss": 2.9374,
      "step": 16900
    },
    {
      "epoch": 64.78927203065135,
      "grad_norm": 0.4811170697212219,
      "learning_rate": 0.0003858626393197713,
      "loss": 2.949,
      "step": 16910
    },
    {
      "epoch": 64.82758620689656,
      "grad_norm": 0.47985732555389404,
      "learning_rate": 0.00038573503661530064,
      "loss": 2.9382,
      "step": 16920
    },
    {
      "epoch": 64.86590038314176,
      "grad_norm": 0.48971882462501526,
      "learning_rate": 0.00038560738374819966,
      "loss": 2.9278,
      "step": 16930
    },
    {
      "epoch": 64.90421455938697,
      "grad_norm": 0.5123701691627502,
      "learning_rate": 0.0003854796807656441,
      "loss": 2.9437,
      "step": 16940
    },
    {
      "epoch": 64.94252873563218,
      "grad_norm": 0.48060110211372375,
      "learning_rate": 0.0003853519277148283,
      "loss": 2.9351,
      "step": 16950
    },
    {
      "epoch": 64.98084291187739,
      "grad_norm": 0.4762153625488281,
      "learning_rate": 0.0003852241246429649,
      "loss": 2.9612,
      "step": 16960
    },
    {
      "epoch": 65.0,
      "eval_loss": 1.4485869407653809,
      "eval_runtime": 4.1055,
      "eval_samples_per_second": 4168.028,
      "eval_steps_per_second": 8.281,
      "step": 16965
    },
    {
      "epoch": 65.01915708812261,
      "grad_norm": 0.46979236602783203,
      "learning_rate": 0.0003850962715972852,
      "loss": 2.9308,
      "step": 16970
    },
    {
      "epoch": 65.05747126436782,
      "grad_norm": 0.5025481581687927,
      "learning_rate": 0.0003849683686250391,
      "loss": 2.9315,
      "step": 16980
    },
    {
      "epoch": 65.09578544061303,
      "grad_norm": 0.4797862768173218,
      "learning_rate": 0.0003848404157734946,
      "loss": 2.8987,
      "step": 16990
    },
    {
      "epoch": 65.13409961685824,
      "grad_norm": 0.4922809898853302,
      "learning_rate": 0.00038471241308993856,
      "loss": 2.9261,
      "step": 17000
    },
    {
      "epoch": 65.17241379310344,
      "grad_norm": 0.484823614358902,
      "learning_rate": 0.00038458436062167574,
      "loss": 2.9397,
      "step": 17010
    },
    {
      "epoch": 65.21072796934865,
      "grad_norm": 0.480254203081131,
      "learning_rate": 0.0003844562584160297,
      "loss": 2.9088,
      "step": 17020
    },
    {
      "epoch": 65.24904214559388,
      "grad_norm": 0.4673929214477539,
      "learning_rate": 0.0003843281065203422,
      "loss": 2.9276,
      "step": 17030
    },
    {
      "epoch": 65.28735632183908,
      "grad_norm": 0.4809364080429077,
      "learning_rate": 0.0003841999049819736,
      "loss": 2.9263,
      "step": 17040
    },
    {
      "epoch": 65.32567049808429,
      "grad_norm": 0.4560840427875519,
      "learning_rate": 0.0003840716538483023,
      "loss": 2.919,
      "step": 17050
    },
    {
      "epoch": 65.3639846743295,
      "grad_norm": 0.505381166934967,
      "learning_rate": 0.0003839433531667251,
      "loss": 2.9304,
      "step": 17060
    },
    {
      "epoch": 65.40229885057471,
      "grad_norm": 0.4873339533805847,
      "learning_rate": 0.0003838150029846573,
      "loss": 2.9404,
      "step": 17070
    },
    {
      "epoch": 65.44061302681992,
      "grad_norm": 0.48083364963531494,
      "learning_rate": 0.0003836866033495324,
      "loss": 2.932,
      "step": 17080
    },
    {
      "epoch": 65.47892720306514,
      "grad_norm": 0.500195324420929,
      "learning_rate": 0.00038355815430880195,
      "loss": 2.922,
      "step": 17090
    },
    {
      "epoch": 65.51724137931035,
      "grad_norm": 0.4812147617340088,
      "learning_rate": 0.000383429655909936,
      "loss": 2.9132,
      "step": 17100
    },
    {
      "epoch": 65.55555555555556,
      "grad_norm": 0.4698314964771271,
      "learning_rate": 0.00038330110820042286,
      "loss": 2.9406,
      "step": 17110
    },
    {
      "epoch": 65.59386973180077,
      "grad_norm": 0.488330215215683,
      "learning_rate": 0.0003831725112277689,
      "loss": 2.9241,
      "step": 17120
    },
    {
      "epoch": 65.63218390804597,
      "grad_norm": 0.4910716116428375,
      "learning_rate": 0.0003830438650394989,
      "loss": 2.9199,
      "step": 17130
    },
    {
      "epoch": 65.67049808429118,
      "grad_norm": 0.4818420708179474,
      "learning_rate": 0.00038291516968315566,
      "loss": 2.9421,
      "step": 17140
    },
    {
      "epoch": 65.7088122605364,
      "grad_norm": 0.4889225363731384,
      "learning_rate": 0.0003827864252063002,
      "loss": 2.9398,
      "step": 17150
    },
    {
      "epoch": 65.74712643678161,
      "grad_norm": 0.4797860085964203,
      "learning_rate": 0.0003826576316565117,
      "loss": 2.9525,
      "step": 17160
    },
    {
      "epoch": 65.78544061302682,
      "grad_norm": 0.48182716965675354,
      "learning_rate": 0.0003825287890813875,
      "loss": 2.9361,
      "step": 17170
    },
    {
      "epoch": 65.82375478927203,
      "grad_norm": 0.4879437983036041,
      "learning_rate": 0.000382399897528543,
      "loss": 2.9236,
      "step": 17180
    },
    {
      "epoch": 65.86206896551724,
      "grad_norm": 0.46301209926605225,
      "learning_rate": 0.0003822709570456117,
      "loss": 2.9377,
      "step": 17190
    },
    {
      "epoch": 65.90038314176245,
      "grad_norm": 0.4796096682548523,
      "learning_rate": 0.0003821419676802453,
      "loss": 2.9231,
      "step": 17200
    },
    {
      "epoch": 65.93869731800767,
      "grad_norm": 0.4830133020877838,
      "learning_rate": 0.00038201292948011347,
      "loss": 2.9375,
      "step": 17210
    },
    {
      "epoch": 65.97701149425288,
      "grad_norm": 0.4787744879722595,
      "learning_rate": 0.00038188384249290385,
      "loss": 2.9232,
      "step": 17220
    },
    {
      "epoch": 66.0,
      "eval_loss": 1.4497264623641968,
      "eval_runtime": 4.0828,
      "eval_samples_per_second": 4191.271,
      "eval_steps_per_second": 8.328,
      "step": 17226
    },
    {
      "epoch": 66.01532567049809,
      "grad_norm": 0.4903891682624817,
      "learning_rate": 0.00038175470676632227,
      "loss": 2.9363,
      "step": 17230
    },
    {
      "epoch": 66.0536398467433,
      "grad_norm": 0.48449626564979553,
      "learning_rate": 0.0003816255223480925,
      "loss": 2.8969,
      "step": 17240
    },
    {
      "epoch": 66.0919540229885,
      "grad_norm": 0.46086809039115906,
      "learning_rate": 0.00038149628928595624,
      "loss": 2.9134,
      "step": 17250
    },
    {
      "epoch": 66.13026819923371,
      "grad_norm": 0.48939812183380127,
      "learning_rate": 0.0003813670076276734,
      "loss": 2.9169,
      "step": 17260
    },
    {
      "epoch": 66.16858237547893,
      "grad_norm": 0.4718664288520813,
      "learning_rate": 0.00038123767742102144,
      "loss": 2.9089,
      "step": 17270
    },
    {
      "epoch": 66.20689655172414,
      "grad_norm": 0.4795789122581482,
      "learning_rate": 0.0003811082987137962,
      "loss": 2.9184,
      "step": 17280
    },
    {
      "epoch": 66.24521072796935,
      "grad_norm": 0.4899213910102844,
      "learning_rate": 0.0003809788715538112,
      "loss": 2.912,
      "step": 17290
    },
    {
      "epoch": 66.28352490421456,
      "grad_norm": 0.4856022596359253,
      "learning_rate": 0.00038084939598889787,
      "loss": 2.9124,
      "step": 17300
    },
    {
      "epoch": 66.32183908045977,
      "grad_norm": 0.4898528754711151,
      "learning_rate": 0.0003807198720669055,
      "loss": 2.9326,
      "step": 17310
    },
    {
      "epoch": 66.36015325670498,
      "grad_norm": 0.46801674365997314,
      "learning_rate": 0.00038059029983570156,
      "loss": 2.9122,
      "step": 17320
    },
    {
      "epoch": 66.3984674329502,
      "grad_norm": 0.4820460379123688,
      "learning_rate": 0.00038046067934317086,
      "loss": 2.9138,
      "step": 17330
    },
    {
      "epoch": 66.4367816091954,
      "grad_norm": 0.484989196062088,
      "learning_rate": 0.0003803310106372165,
      "loss": 2.913,
      "step": 17340
    },
    {
      "epoch": 66.47509578544062,
      "grad_norm": 0.49403631687164307,
      "learning_rate": 0.0003802012937657591,
      "loss": 2.9305,
      "step": 17350
    },
    {
      "epoch": 66.51340996168582,
      "grad_norm": 0.5025214552879333,
      "learning_rate": 0.00038007152877673735,
      "loss": 2.9153,
      "step": 17360
    },
    {
      "epoch": 66.55172413793103,
      "grad_norm": 0.4907546043395996,
      "learning_rate": 0.0003799417157181075,
      "loss": 2.9352,
      "step": 17370
    },
    {
      "epoch": 66.59003831417624,
      "grad_norm": 0.46867018938064575,
      "learning_rate": 0.00037981185463784365,
      "loss": 2.9429,
      "step": 17380
    },
    {
      "epoch": 66.62835249042146,
      "grad_norm": 0.4820750951766968,
      "learning_rate": 0.0003796819455839376,
      "loss": 2.9472,
      "step": 17390
    },
    {
      "epoch": 66.66666666666667,
      "grad_norm": 0.48447316884994507,
      "learning_rate": 0.00037955198860439886,
      "loss": 2.932,
      "step": 17400
    },
    {
      "epoch": 66.70498084291188,
      "grad_norm": 0.48422443866729736,
      "learning_rate": 0.00037942198374725484,
      "loss": 2.9403,
      "step": 17410
    },
    {
      "epoch": 66.74329501915709,
      "grad_norm": 0.5002766847610474,
      "learning_rate": 0.0003792919310605505,
      "loss": 2.9357,
      "step": 17420
    },
    {
      "epoch": 66.7816091954023,
      "grad_norm": 0.47618091106414795,
      "learning_rate": 0.0003791618305923483,
      "loss": 2.9202,
      "step": 17430
    },
    {
      "epoch": 66.8199233716475,
      "grad_norm": 0.47689419984817505,
      "learning_rate": 0.0003790316823907287,
      "loss": 2.9287,
      "step": 17440
    },
    {
      "epoch": 66.85823754789271,
      "grad_norm": 0.4619329273700714,
      "learning_rate": 0.00037890148650378963,
      "loss": 2.9284,
      "step": 17450
    },
    {
      "epoch": 66.89655172413794,
      "grad_norm": 0.4807334840297699,
      "learning_rate": 0.0003787712429796467,
      "loss": 2.9168,
      "step": 17460
    },
    {
      "epoch": 66.93486590038314,
      "grad_norm": 0.49551814794540405,
      "learning_rate": 0.0003786409518664329,
      "loss": 2.9324,
      "step": 17470
    },
    {
      "epoch": 66.97318007662835,
      "grad_norm": 0.4845808446407318,
      "learning_rate": 0.00037851061321229917,
      "loss": 2.9244,
      "step": 17480
    },
    {
      "epoch": 67.0,
      "eval_loss": 1.4486454725265503,
      "eval_runtime": 3.9787,
      "eval_samples_per_second": 4300.875,
      "eval_steps_per_second": 8.545,
      "step": 17487
    },
    {
      "epoch": 67.01149425287356,
      "grad_norm": 0.4669046998023987,
      "learning_rate": 0.00037838022706541377,
      "loss": 2.9294,
      "step": 17490
    },
    {
      "epoch": 67.04980842911877,
      "grad_norm": 0.47396209836006165,
      "learning_rate": 0.0003782497934739626,
      "loss": 2.9174,
      "step": 17500
    },
    {
      "epoch": 67.08812260536398,
      "grad_norm": 0.46510207653045654,
      "learning_rate": 0.00037811931248614905,
      "loss": 2.9109,
      "step": 17510
    },
    {
      "epoch": 67.1264367816092,
      "grad_norm": 0.504923939704895,
      "learning_rate": 0.0003779887841501941,
      "loss": 2.9107,
      "step": 17520
    },
    {
      "epoch": 67.16475095785441,
      "grad_norm": 0.46845388412475586,
      "learning_rate": 0.0003778582085143361,
      "loss": 2.9088,
      "step": 17530
    },
    {
      "epoch": 67.20306513409962,
      "grad_norm": 0.4685605764389038,
      "learning_rate": 0.00037772758562683095,
      "loss": 2.9289,
      "step": 17540
    },
    {
      "epoch": 67.24137931034483,
      "grad_norm": 0.48948878049850464,
      "learning_rate": 0.0003775969155359521,
      "loss": 2.9176,
      "step": 17550
    },
    {
      "epoch": 67.27969348659003,
      "grad_norm": 0.48138609528541565,
      "learning_rate": 0.0003774661982899903,
      "loss": 2.9129,
      "step": 17560
    },
    {
      "epoch": 67.31800766283524,
      "grad_norm": 0.4696434736251831,
      "learning_rate": 0.0003773354339372538,
      "loss": 2.905,
      "step": 17570
    },
    {
      "epoch": 67.35632183908046,
      "grad_norm": 0.4828183054924011,
      "learning_rate": 0.00037720462252606836,
      "loss": 2.926,
      "step": 17580
    },
    {
      "epoch": 67.39463601532567,
      "grad_norm": 0.4852254390716553,
      "learning_rate": 0.0003770737641047769,
      "loss": 2.9226,
      "step": 17590
    },
    {
      "epoch": 67.43295019157088,
      "grad_norm": 0.4890185296535492,
      "learning_rate": 0.00037694285872173983,
      "loss": 2.9266,
      "step": 17600
    },
    {
      "epoch": 67.47126436781609,
      "grad_norm": 0.478050172328949,
      "learning_rate": 0.000376811906425335,
      "loss": 2.9155,
      "step": 17610
    },
    {
      "epoch": 67.5095785440613,
      "grad_norm": 0.4960328936576843,
      "learning_rate": 0.00037668090726395743,
      "loss": 2.9198,
      "step": 17620
    },
    {
      "epoch": 67.5478927203065,
      "grad_norm": 0.47391825914382935,
      "learning_rate": 0.00037654986128601967,
      "loss": 2.9176,
      "step": 17630
    },
    {
      "epoch": 67.58620689655173,
      "grad_norm": 0.5041559934616089,
      "learning_rate": 0.00037641876853995124,
      "loss": 2.9233,
      "step": 17640
    },
    {
      "epoch": 67.62452107279694,
      "grad_norm": 0.5044448971748352,
      "learning_rate": 0.0003762876290741993,
      "loss": 2.9269,
      "step": 17650
    },
    {
      "epoch": 67.66283524904215,
      "grad_norm": 0.4874146580696106,
      "learning_rate": 0.0003761564429372281,
      "loss": 2.9328,
      "step": 17660
    },
    {
      "epoch": 67.70114942528735,
      "grad_norm": 0.4953829050064087,
      "learning_rate": 0.0003760252101775192,
      "loss": 2.9066,
      "step": 17670
    },
    {
      "epoch": 67.73946360153256,
      "grad_norm": 0.4781108498573303,
      "learning_rate": 0.00037589393084357136,
      "loss": 2.9266,
      "step": 17680
    },
    {
      "epoch": 67.77777777777777,
      "grad_norm": 0.4775203764438629,
      "learning_rate": 0.0003757626049839005,
      "loss": 2.916,
      "step": 17690
    },
    {
      "epoch": 67.816091954023,
      "grad_norm": 0.5023650527000427,
      "learning_rate": 0.0003756312326470398,
      "loss": 2.9091,
      "step": 17700
    },
    {
      "epoch": 67.8544061302682,
      "grad_norm": 0.49355295300483704,
      "learning_rate": 0.00037549981388153953,
      "loss": 2.9363,
      "step": 17710
    },
    {
      "epoch": 67.89272030651341,
      "grad_norm": 0.47132065892219543,
      "learning_rate": 0.0003753683487359673,
      "loss": 2.9357,
      "step": 17720
    },
    {
      "epoch": 67.93103448275862,
      "grad_norm": 0.4730718433856964,
      "learning_rate": 0.0003752368372589078,
      "loss": 2.9014,
      "step": 17730
    },
    {
      "epoch": 67.96934865900383,
      "grad_norm": 0.5005579590797424,
      "learning_rate": 0.0003751052794989626,
      "loss": 2.9282,
      "step": 17740
    },
    {
      "epoch": 68.0,
      "eval_loss": 1.4491556882858276,
      "eval_runtime": 3.8331,
      "eval_samples_per_second": 4464.281,
      "eval_steps_per_second": 8.87,
      "step": 17748
    },
    {
      "epoch": 68.00766283524904,
      "grad_norm": 0.4860665798187256,
      "learning_rate": 0.0003749736755047505,
      "loss": 2.9092,
      "step": 17750
    },
    {
      "epoch": 68.04597701149426,
      "grad_norm": 0.4885008931159973,
      "learning_rate": 0.0003748420253249079,
      "loss": 2.894,
      "step": 17760
    },
    {
      "epoch": 68.08429118773947,
      "grad_norm": 0.473856121301651,
      "learning_rate": 0.00037471032900808726,
      "loss": 2.898,
      "step": 17770
    },
    {
      "epoch": 68.12260536398468,
      "grad_norm": 0.47391027212142944,
      "learning_rate": 0.00037457858660295906,
      "loss": 2.9088,
      "step": 17780
    },
    {
      "epoch": 68.16091954022988,
      "grad_norm": 0.4919392168521881,
      "learning_rate": 0.0003744467981582103,
      "loss": 2.9078,
      "step": 17790
    },
    {
      "epoch": 68.19923371647509,
      "grad_norm": 0.48468273878097534,
      "learning_rate": 0.00037431496372254485,
      "loss": 2.9062,
      "step": 17800
    },
    {
      "epoch": 68.2375478927203,
      "grad_norm": 0.498770147562027,
      "learning_rate": 0.0003741830833446842,
      "loss": 2.9077,
      "step": 17810
    },
    {
      "epoch": 68.27586206896552,
      "grad_norm": 0.48488885164260864,
      "learning_rate": 0.0003740511570733661,
      "loss": 2.9331,
      "step": 17820
    },
    {
      "epoch": 68.31417624521073,
      "grad_norm": 0.5073511600494385,
      "learning_rate": 0.00037391918495734594,
      "loss": 2.8934,
      "step": 17830
    },
    {
      "epoch": 68.35249042145594,
      "grad_norm": 0.5041377544403076,
      "learning_rate": 0.0003737871670453955,
      "loss": 2.9165,
      "step": 17840
    },
    {
      "epoch": 68.39080459770115,
      "grad_norm": 0.47787991166114807,
      "learning_rate": 0.0003736551033863037,
      "loss": 2.9207,
      "step": 17850
    },
    {
      "epoch": 68.42911877394636,
      "grad_norm": 0.4858301281929016,
      "learning_rate": 0.00037352299402887646,
      "loss": 2.9047,
      "step": 17860
    },
    {
      "epoch": 68.46743295019157,
      "grad_norm": 0.4907964766025543,
      "learning_rate": 0.0003733908390219364,
      "loss": 2.9244,
      "step": 17870
    },
    {
      "epoch": 68.50574712643679,
      "grad_norm": 0.4950515329837799,
      "learning_rate": 0.0003732586384143231,
      "loss": 2.9086,
      "step": 17880
    },
    {
      "epoch": 68.544061302682,
      "grad_norm": 0.4877249300479889,
      "learning_rate": 0.00037312639225489316,
      "loss": 2.9017,
      "step": 17890
    },
    {
      "epoch": 68.5823754789272,
      "grad_norm": 0.4644007384777069,
      "learning_rate": 0.00037299410059251964,
      "loss": 2.9325,
      "step": 17900
    },
    {
      "epoch": 68.62068965517241,
      "grad_norm": 0.4791659116744995,
      "learning_rate": 0.0003728617634760928,
      "loss": 2.9002,
      "step": 17910
    },
    {
      "epoch": 68.65900383141762,
      "grad_norm": 0.48414790630340576,
      "learning_rate": 0.00037272938095451937,
      "loss": 2.9256,
      "step": 17920
    },
    {
      "epoch": 68.69731800766283,
      "grad_norm": 0.5087858438491821,
      "learning_rate": 0.000372596953076723,
      "loss": 2.9084,
      "step": 17930
    },
    {
      "epoch": 68.73563218390805,
      "grad_norm": 0.49394577741622925,
      "learning_rate": 0.0003724644798916444,
      "loss": 2.9347,
      "step": 17940
    },
    {
      "epoch": 68.77394636015326,
      "grad_norm": 0.48531046509742737,
      "learning_rate": 0.00037233196144824046,
      "loss": 2.9153,
      "step": 17950
    },
    {
      "epoch": 68.81226053639847,
      "grad_norm": 0.493801474571228,
      "learning_rate": 0.00037219939779548517,
      "loss": 2.9101,
      "step": 17960
    },
    {
      "epoch": 68.85057471264368,
      "grad_norm": 0.4889964163303375,
      "learning_rate": 0.00037206678898236913,
      "loss": 2.9145,
      "step": 17970
    },
    {
      "epoch": 68.88888888888889,
      "grad_norm": 0.47370389103889465,
      "learning_rate": 0.00037193413505789963,
      "loss": 2.923,
      "step": 17980
    },
    {
      "epoch": 68.9272030651341,
      "grad_norm": 0.4771389067173004,
      "learning_rate": 0.0003718014360711006,
      "loss": 2.9228,
      "step": 17990
    },
    {
      "epoch": 68.96551724137932,
      "grad_norm": 0.4845947325229645,
      "learning_rate": 0.00037166869207101277,
      "loss": 2.9406,
      "step": 18000
    },
    {
      "epoch": 69.0,
      "eval_loss": 1.4512490034103394,
      "eval_runtime": 3.9149,
      "eval_samples_per_second": 4370.964,
      "eval_steps_per_second": 8.685,
      "step": 18009
    },
    {
      "epoch": 69.0,
      "step": 18009,
      "total_flos": 4.372632532156416e+16,
      "train_loss": 3.433396296469651,
      "train_runtime": 7889.9249,
      "train_samples_per_second": 6772.435,
      "train_steps_per_second": 6.616
    }
  ],
  "logging_steps": 10,
  "max_steps": 52200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 200,
  "save_steps": 1000,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 10,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 10
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.372632532156416e+16,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
